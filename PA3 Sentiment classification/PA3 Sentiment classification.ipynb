{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725e508-a50f-4703-bca4-5b3dc6c9469f",
   "metadata": {},
   "source": [
    "PA3\n",
    "Author: Zhuangzhuang Gong, Richard Hua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cdfe2-8f4d-4675-91f3-50556c334522",
   "metadata": {},
   "source": [
    "## 1. Reflection on the annotation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1895300-3d8c-4df0-9748-33ca439dac75",
   "metadata": {},
   "source": [
    "**Challenges**  \n",
    "One of the key challenges is that we found it hard to understand the tweets with the culture references and context outside of the tweet itself.  \n",
    "In this case, we had to make judgement calls,\n",
    "which made us more aware of the subjectivity involved in annotation. Another takeaway is dealing\n",
    "with mixed-sentiment tweets. For example, some tweets consist of both positive and negative emotions, making it hard to label them definitively.\n",
    "\n",
    "**Ideals of improvement**  \n",
    "To improve the process of annotation, refine and expand the guidelines of annotation by providing clearer annotation instructions, especially for\n",
    "mixed-sentiment tweets. This will reduce the subjectivity in the annotation.\n",
    "Another way to improve is to introduce InterAnnotator Agreement by having multiple annotators and comparing their results. This not only improve the quality of annotation but also give the insight how subjective the sentiment label is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120244-b511-4b33-aff2-b8d35816747a",
   "metadata": {},
   "source": [
    "## 2. Exploring the crowdsourced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201243-36ac-4aa9-bf86-e353fe3a80cb",
   "metadata": {},
   "source": [
    "**Read and observe the data**  \n",
    "As we can see, the sentiment annotations are not expected. There are many inconsistent label formats, such as 'neutral' and 'Neutral' or typos like 'Nutral'. Therefore, we need to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b76b17c-fdd0-4130-8e2e-5ab127e1f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentiment                                               text\n",
      "0      Positive  There's so much misconception on Islam rn so s...\n",
      "1      Positive  @Mr_Rondeau You should try Iron Maiden at abou...\n",
      "2      Negative  Going to #FantasticFour tomorrow. Half expecti...\n",
      "3       Neutral  @cfelan hey hey, just checkng to see if you or...\n",
      "4      Positive  does anyone just get drunk and watch twilight ...\n",
      "...         ...                                                ...\n",
      "10671  Positive  Glad to hear there may be a bigger more public...\n",
      "10672  Positive  Great stand by the Wolves on 3rd and long. Cur...\n",
      "10673   Neutral  Ayyye I just purchased my Ed Sheeran tickets f...\n",
      "10674  Negative  The anti-semitism, the misogyny, and the suppo...\n",
      "10675  Positive  And yet, I have yet to see the whole series of...\n",
      "\n",
      "[10676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crowd_train_data = pd.read_csv('crowdsourced_train.csv',sep='\\t')\n",
    "gold_train_data = pd.read_csv('gold_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv('test.csv', sep='\\t')\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(crowd_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de0c6-ab70-4f1f-9000-2750f79331dc",
   "metadata": {},
   "source": [
    "**Check the agreement before cleaning the data**  \n",
    "If we check the inner annotation agreement now by comparing the sentiment labels from the crowdsourced annotator and the gold annotator across 10,675 tweets. The overall accuracy is only 35.63%, and Cohen's Kappa is only 0.19, which indicates a low level of agreement.\n",
    "This suggests there's either inconsistency or subjective interpretation of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8592c8d4-f5b8-4107-a862-868c915f0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.19\n",
      "Accuracy: 35.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02d7ae-0dc5-45f5-ab08-dc7dbaa492d1",
   "metadata": {},
   "source": [
    "**Clean and re-classify labels**  \n",
    "We easily fixed some inconsistent label formats by converting all labels to lowercase and stripping the blank spaces.  \n",
    "Furthermore, we found several spelling variations and formatting issues in the label column (e.g., 'nuetral', 'positve', 'nedative', etc.). These were normalised using a mapping dictionary, and all labels were successfully consolidated into the three standard sentiment categories: positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95691e3-f3e7-4030-8b39-71986cc963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning\n",
      "sentiment\n",
      "neutral           5046\n",
      "positive          3213\n",
      "negative          2375\n",
      "netural             20\n",
      "nuetral              3\n",
      "postive              2\n",
      "postitive            1\n",
      "neutal               1\n",
      "npositive            1\n",
      "neutra l             1\n",
      "positie              1\n",
      "negayive             1\n",
      "nutral               1\n",
      "neugral              1\n",
      "negtaive             1\n",
      "neutrla              1\n",
      "neutrall             1\n",
      "neural               1\n",
      "netutral             1\n",
      "_x0008_neutral       1\n",
      "nedative             1\n",
      "neutral?             1\n",
      "positve              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert labels to lowercase\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].str.strip().str.lower()\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(\"before cleaning\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a8d299-c7f4-4e7a-82bd-9a1b1d50b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning\n",
      "sentiment\n",
      "neutral     5079\n",
      "positive    3219\n",
      "negative    2378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "label_map = {\n",
    "    # positive \n",
    "    'positve': 'positive',\n",
    "    'postive': 'positive',\n",
    "    'postitive': 'positive',\n",
    "    'positie': 'positive',\n",
    "    'npositive': 'positive',\n",
    "\n",
    "    # neutral \n",
    "    'neutral?': 'neutral',\n",
    "    'nuetral': 'neutral',\n",
    "    '_x0008_neutral': 'neutral',\n",
    "    'netural': 'neutral',\n",
    "    'netutral': 'neutral',\n",
    "    'neural': 'neutral',\n",
    "    'neutrall': 'neutral',\n",
    "    'neugral': 'neutral',\n",
    "    'neutrla': 'neutral',\n",
    "    'nutral': 'neutral',\n",
    "    'neutra l': 'neutral',\n",
    "    'neutal': 'neutral',\n",
    "\n",
    "    # negative \n",
    "    'nedative': 'negative',\n",
    "    'negtaive': 'negative',\n",
    "    'negayive': 'negative',\n",
    "}\n",
    "\n",
    "label_map.update({\n",
    "    'positive': 'positive',\n",
    "    'neutral': 'neutral',\n",
    "    'negative': 'negative'\n",
    "})\n",
    "\n",
    "# apply the dictionary\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].map(label_map)\n",
    "\n",
    "# check the result\n",
    "print(\"After cleaning\")\n",
    "print(crowd_train_data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030311-3cfc-4ee8-951f-f7022866f961",
   "metadata": {},
   "source": [
    "**Agreement checking after cleaning**  \n",
    "We evaluated the agreement between the crowd-sourced labels and gold standard labels again across the data after cleaning the labels. The overall accuracy was 65.49% and the Cohen's Kappa was 0.45, indicating 'average' agreement, according to the standard in Richard Johansson's slides.  \n",
    "This suggests that there's reasonable alignment, while some inconsistency still remains. It's likely due to subjective interpretation of semtimemt tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33bd2d9-9b48-4bcd-b86d-a5c6265c309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.45\n",
      "Accuracy: 65.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b00965-7608-4243-b732-89d24a55cd5d",
   "metadata": {},
   "source": [
    "**Analysis of the annotation distribution**  \n",
    "We compared the annotation distribution across crowdsourced and gold data. Both showed a similar overall distribution structure, with the majority of labels being 'neutral', followed by 'positive' then 'negative'. However, there's a noticeable difference between 'negative', indicating the sensitivity or interpretation for negative tweets between two groups varies. It suggests that the crowd group is more sensitive to negative sentiment or subjective interpretation of negative tweets, which may contribute to the Cohen's Kappa of 0.45 and the general accuracy of 65.49%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5323cc53-93a5-4876-8f78-97450d5b2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Crowd  Gold  Crowd (%)  Gold (%)\n",
      "neutral    5079  5364      47.57     50.24\n",
      "positive   3219  3652      30.15     34.21\n",
      "negative   2378  1660      22.27     15.55\n"
     ]
    }
   ],
   "source": [
    "# Get label counts from each dataset\n",
    "crowd_counts = crowd_train_data['sentiment'].value_counts()\n",
    "gold_counts = gold_train_data['sentiment'].value_counts()\n",
    "\n",
    "# Access values \n",
    "crowd_counts_dict = {\n",
    "    'neutral': crowd_counts.get('neutral', 0),\n",
    "    'positive': crowd_counts.get('positive', 0),\n",
    "    'negative': crowd_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "gold_counts_dict = {\n",
    "    'neutral': gold_counts.get('neutral', 0),\n",
    "    'positive': gold_counts.get('positive', 0),\n",
    "    'negative': gold_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "# Create Series\n",
    "crowd_series = pd.Series(crowd_counts_dict, name='Crowd')\n",
    "gold_series = pd.Series(gold_counts_dict, name='Gold')\n",
    "\n",
    "# Combine and analyze\n",
    "comparison_df = pd.concat([crowd_series, gold_series], axis=1)\n",
    "\n",
    "# Add percentage comparision\n",
    "total = comparison_df.sum()\n",
    "comparison_df['Crowd (%)'] = (comparison_df['Crowd'] / total['Crowd'] * 100).round(2)\n",
    "comparison_df['Gold (%)'] = (comparison_df['Gold'] / total['Gold'] * 100).round(2)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729ee71-a65d-482b-a00a-d54d9496fbfb",
   "metadata": {},
   "source": [
    "## 3. Implementation of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce01539-3375-43fd-a55d-59e35c6c1da4",
   "metadata": {},
   "source": [
    "Clean tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868c513f-f98f-4ff7-8d2a-cfb6f445f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    # 3. Replace user mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # 4. Remove hashtags but keep the tag text\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # 5. Remove RT (retweet marker)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    # 6. Remove emojis or translate them to text\n",
    "    text = emoji.demojize(text)  # \":smile:\" etc\n",
    "    # 7. Remove non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # 8. Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a57466-6085-479a-a17f-ec1d793080a4",
   "metadata": {},
   "source": [
    "Pipeline of Clean+TfidfVectorizer+linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "820aa2f8-0d8e-4e64-92ea-2e5a3b99595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.548689138576779\n"
     ]
    }
   ],
   "source": [
    "# the actual classification algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "# for converting training and test datasets into matrices\n",
    "# TfidfVectorizer does this specifically for documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# for splitting the dataset into training and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = crowd_train_data['text']\n",
    "Y = crowd_train_data['sentiment']\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline( TfidfVectorizer(preprocessor=clean_tweet,), LinearSVC(dual='auto') )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X_train, Y_train)\n",
    "acc = accuracy_score(Y_eval, clf.predict(X_eval))\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf6cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410d7af5-7e99-41ef-9d8c-6731ea5bda3c",
   "metadata": {},
   "source": [
    "We used hyperparameter search and cross-validation to select a set of parameters that work better on unknown data in order to maximise the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54687",
   "metadata": {},
   "source": [
    "We also tried using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0606d16e-c034-465e-b9e6-83e10439f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6441947565543071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(preprocessor=clean_tweet,), LogisticRegression(\n",
    "    penalty='l2',            # L2 regularization\n",
    "    C=1.0,                   # Regularization strength (default)\n",
    "    solver='liblinear',      # Solver suitable for smaller datasets\n",
    "    max_iter=100,            # Iterations for convergence\n",
    "    class_weight='balanced', # Handle imbalanced classes\n",
    "    fit_intercept=True,      # Include intercept\n",
    "))\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X_train, Y_train)\n",
    "acc = accuracy_score(Y_eval, clf.predict(X_eval))\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c8aca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Benchmarking models...\n",
      "\n",
      "MultinomialNB            : Mean Accuracy = 0.5506 ¬± 0.0058\n",
      "LogisticRegression       : Mean Accuracy = 0.5658 ¬± 0.0127\n",
      "LinearSVC                : Mean Accuracy = 0.5303 ¬± 0.0138\n",
      "RidgeClassifier          : Mean Accuracy = 0.5554 ¬± 0.0109\n",
      "\n",
      "üìä Classification Report (RidgeClassifier on hold-out set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.34      0.40       476\n",
      "     neutral       0.58      0.68      0.62      1016\n",
      "    positive       0.54      0.52      0.53       644\n",
      "\n",
      "    accuracy                           0.55      2136\n",
      "   macro avg       0.53      0.51      0.52      2136\n",
      "weighted avg       0.54      0.55      0.54      2136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, class_weight='balanced', solver='liblinear'),\n",
    "    \"LinearSVC\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "}\n",
    "\n",
    "# TF-IDF settings\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor=clean_tweet,         # Apply tweet cleaning function\n",
    "    ngram_range=(1, 2),               # Unigrams + bigrams\n",
    "    stop_words='english',             # Use English stop words\n",
    "    max_features=10000,               # Limit to top 10,000 features\n",
    "    sublinear_tf=True,                # Use log scaling for term frequency\n",
    "    min_df=3                          # Ignore terms that appear in fewer than 3 documents\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"üîç Benchmarking models...\\n\")\n",
    "for name, model in models.items():\n",
    "    pipeline = make_pipeline(tfidf, model)\n",
    "    scores = cross_val_score(pipeline, X, Y, cv=cv, scoring='accuracy')\n",
    "    print(f\"{name:<25}: Mean Accuracy = {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "\n",
    "# Final evaluation on hold-out set\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "final_model = make_pipeline(tfidf, RidgeClassifier())\n",
    "final_model.fit(X_train, Y_train)\n",
    "preds = final_model.predict(X_eval)\n",
    "\n",
    "print(\"\\nüìä Classification Report (RidgeClassifier on hold-out set):\")\n",
    "print(classification_report(Y_eval, preds))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b361009-f79c-43d6-90be-63f8a4fd6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.95884597\n",
      "Iteration 2, loss = 0.66361254\n",
      "Iteration 3, loss = 0.38862284\n",
      "Iteration 4, loss = 0.18520055\n",
      "Iteration 5, loss = 0.06855323\n",
      "Iteration 6, loss = 0.02281874\n",
      "Iteration 7, loss = 0.01015891\n",
      "Iteration 8, loss = 0.00597680\n",
      "Iteration 9, loss = 0.00387828\n",
      "Iteration 10, loss = 0.00323096\n",
      "Iteration 11, loss = 0.00301844\n",
      "Iteration 12, loss = 0.00220541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data (adjust this for your dataset)\n",
    "# Assuming crowd_train_data is your dataset with columns 'text' and 'sentiment'\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "# Encode the labels (since you have 3 classes)\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y_encoded, test_size=0.2, random_state=0)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_eval_tfidf = tfidf.transform(X_eval).toarray()\n",
    "\n",
    "# MLP Neural Network Model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),  # Two hidden layers with 512 and 256 neurons\n",
    "    activation='relu',              # ReLU activation function\n",
    "    solver='adam',                  # Adam optimizer\n",
    "    max_iter=100,                   # Number of iterations\n",
    "    random_state=0,                 # Random seed for reproducibility\n",
    "    verbose=True                    # Output progress during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred = mlp.predict(X_eval_tfidf)\n",
    "\n",
    "# Accuracy score\n",
    "acc = accuracy_score(Y_eval, Y_pred)\n",
    "print(f\"MLPClassifier Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d19da-668d-47ea-b1de-b5e3db6a777f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
