{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725e508-a50f-4703-bca4-5b3dc6c9469f",
   "metadata": {},
   "source": [
    "PA3\n",
    "Author: Zhuangzhuang Gong, Richard Hua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cdfe2-8f4d-4675-91f3-50556c334522",
   "metadata": {},
   "source": [
    "## 1. Reflection on the annotation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1895300-3d8c-4df0-9748-33ca439dac75",
   "metadata": {},
   "source": [
    "**Challenges**  \n",
    "One of the key challenges is that we found it hard to understand the tweets with the culture references and context outside of the tweet itself.  \n",
    "In this case, we had to make judgement calls,\n",
    "which made us more aware of the subjectivity involved in annotation. Another takeaway is dealing\n",
    "with mixed-sentiment tweets. For example, some tweets consist of both positive and negative emotions, making it hard to label them definitively.\n",
    "\n",
    "**Ideals of improvement**  \n",
    "To improve the process of annotation, refine and expand the guidelines of annotation by providing clearer annotation instructions, especially for\n",
    "mixed-sentiment tweets. This will reduce the subjectivity in the annotation.\n",
    "Another way to improve is to introduce InterAnnotator Agreement by having multiple annotators and comparing their results. This not only improve the quality of annotation but also give the insight how subjective the sentiment label is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120244-b511-4b33-aff2-b8d35816747a",
   "metadata": {},
   "source": [
    "## 2. Exploring the crowdsourced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201243-36ac-4aa9-bf86-e353fe3a80cb",
   "metadata": {},
   "source": [
    "**Read and observe the data**  \n",
    "As we can see, the sentiment annotations are not expected. There are many inconsistent label formats, such as 'neutral' and 'Neutral' or typos like 'Nutral'. Therefore, we need to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b76b17c-fdd0-4130-8e2e-5ab127e1f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentiment                                               text\n",
      "0      Positive  There's so much misconception on Islam rn so s...\n",
      "1      Positive  @Mr_Rondeau You should try Iron Maiden at abou...\n",
      "2      Negative  Going to #FantasticFour tomorrow. Half expecti...\n",
      "3       Neutral  @cfelan hey hey, just checkng to see if you or...\n",
      "4      Positive  does anyone just get drunk and watch twilight ...\n",
      "...         ...                                                ...\n",
      "10671  Positive  Glad to hear there may be a bigger more public...\n",
      "10672  Positive  Great stand by the Wolves on 3rd and long. Cur...\n",
      "10673   Neutral  Ayyye I just purchased my Ed Sheeran tickets f...\n",
      "10674  Negative  The anti-semitism, the misogyny, and the suppo...\n",
      "10675  Positive  And yet, I have yet to see the whole series of...\n",
      "\n",
      "[10676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crowd_train_data = pd.read_csv('crowdsourced_train.csv',sep='\\t')\n",
    "gold_train_data = pd.read_csv('gold_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv('test.csv', sep='\\t')\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(crowd_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de0c6-ab70-4f1f-9000-2750f79331dc",
   "metadata": {},
   "source": [
    "**Check the agreement before cleaning the data**  \n",
    "If we check the inner annotation agreement now by comparing the sentiment labels from the crowdsourced annotator and the gold annotator across 10,675 tweets. The overall accuracy is only 35.63%, and Cohen's Kappa is only 0.19, which indicates a low level of agreement.\n",
    "This suggests there's either inconsistency or subjective interpretation of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8592c8d4-f5b8-4107-a862-868c915f0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.19\n",
      "Accuracy: 35.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02d7ae-0dc5-45f5-ab08-dc7dbaa492d1",
   "metadata": {},
   "source": [
    "**Clean and re-classify labels**  \n",
    "We easily fixed some inconsistent label formats by converting all labels to lowercase and stripping the blank spaces.  \n",
    "Furthermore, we found several spelling variations and formatting issues in the label column (e.g., 'nuetral', 'positve', 'nedative', etc.). These were normalised using a mapping dictionary, and all labels were successfully consolidated into the three standard sentiment categories: positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95691e3-f3e7-4030-8b39-71986cc963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning\n",
      "sentiment\n",
      "neutral           5046\n",
      "positive          3213\n",
      "negative          2375\n",
      "netural             20\n",
      "nuetral              3\n",
      "postive              2\n",
      "postitive            1\n",
      "neutal               1\n",
      "npositive            1\n",
      "neutra l             1\n",
      "positie              1\n",
      "negayive             1\n",
      "nutral               1\n",
      "neugral              1\n",
      "negtaive             1\n",
      "neutrla              1\n",
      "neutrall             1\n",
      "neural               1\n",
      "netutral             1\n",
      "_x0008_neutral       1\n",
      "nedative             1\n",
      "neutral?             1\n",
      "positve              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert labels to lowercase\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].str.strip().str.lower()\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(\"before cleaning\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a8d299-c7f4-4e7a-82bd-9a1b1d50b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning\n",
      "sentiment\n",
      "neutral     5079\n",
      "positive    3219\n",
      "negative    2378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "label_map = {\n",
    "    # positive \n",
    "    'positve': 'positive',\n",
    "    'postive': 'positive',\n",
    "    'postitive': 'positive',\n",
    "    'positie': 'positive',\n",
    "    'npositive': 'positive',\n",
    "\n",
    "    # neutral \n",
    "    'neutral?': 'neutral',\n",
    "    'nuetral': 'neutral',\n",
    "    '_x0008_neutral': 'neutral',\n",
    "    'netural': 'neutral',\n",
    "    'netutral': 'neutral',\n",
    "    'neural': 'neutral',\n",
    "    'neutrall': 'neutral',\n",
    "    'neugral': 'neutral',\n",
    "    'neutrla': 'neutral',\n",
    "    'nutral': 'neutral',\n",
    "    'neutra l': 'neutral',\n",
    "    'neutal': 'neutral',\n",
    "\n",
    "    # negative \n",
    "    'nedative': 'negative',\n",
    "    'negtaive': 'negative',\n",
    "    'negayive': 'negative',\n",
    "}\n",
    "\n",
    "label_map.update({\n",
    "    'positive': 'positive',\n",
    "    'neutral': 'neutral',\n",
    "    'negative': 'negative'\n",
    "})\n",
    "\n",
    "# apply the dictionary\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].map(label_map)\n",
    "\n",
    "# check the result\n",
    "print(\"After cleaning\")\n",
    "print(crowd_train_data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030311-3cfc-4ee8-951f-f7022866f961",
   "metadata": {},
   "source": [
    "**Agreement checking after cleaning**  \n",
    "We evaluated the agreement between the crowd-sourced labels and gold standard labels again across the data after cleaning the labels. The overall accuracy was 65.49% and the Cohen's Kappa was 0.45, indicating 'average' agreement, according to the standard in Richard Johansson's slides.  \n",
    "This suggests that there's reasonable alignment, while some inconsistency still remains. It's likely due to subjective interpretation of semtimemt tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33bd2d9-9b48-4bcd-b86d-a5c6265c309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.45\n",
      "Accuracy: 65.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b00965-7608-4243-b732-89d24a55cd5d",
   "metadata": {},
   "source": [
    "**Analysis of the annotation distribution**  \n",
    "We compared the annotation distribution across crowdsourced and gold data. Both showed a similar overall distribution structure, with the majority of labels being 'neutral', followed by 'positive' then 'negative'. However, there's a noticeable difference between 'negative', indicating the sensitivity or interpretation for negative tweets between two groups varies. It suggests that the crowd group is more sensitive to negative sentiment or subjective interpretation of negative tweets, which may contribute to the Cohen's Kappa of 0.45 and the general accuracy of 65.49%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5323cc53-93a5-4876-8f78-97450d5b2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Crowd  Gold  Crowd (%)  Gold (%)\n",
      "neutral    5079  5364      47.57     50.24\n",
      "positive   3219  3652      30.15     34.21\n",
      "negative   2378  1660      22.27     15.55\n"
     ]
    }
   ],
   "source": [
    "# Get label counts from each dataset\n",
    "crowd_counts = crowd_train_data['sentiment'].value_counts()\n",
    "gold_counts = gold_train_data['sentiment'].value_counts()\n",
    "\n",
    "# Access values \n",
    "crowd_counts_dict = {\n",
    "    'neutral': crowd_counts.get('neutral', 0),\n",
    "    'positive': crowd_counts.get('positive', 0),\n",
    "    'negative': crowd_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "gold_counts_dict = {\n",
    "    'neutral': gold_counts.get('neutral', 0),\n",
    "    'positive': gold_counts.get('positive', 0),\n",
    "    'negative': gold_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "# Create Series\n",
    "crowd_series = pd.Series(crowd_counts_dict, name='Crowd')\n",
    "gold_series = pd.Series(gold_counts_dict, name='Gold')\n",
    "\n",
    "# Combine and analyze\n",
    "comparison_df = pd.concat([crowd_series, gold_series], axis=1)\n",
    "\n",
    "# Add percentage comparision\n",
    "total = comparison_df.sum()\n",
    "comparison_df['Crowd (%)'] = (comparison_df['Crowd'] / total['Crowd'] * 100).round(2)\n",
    "comparison_df['Gold (%)'] = (comparison_df['Gold'] / total['Gold'] * 100).round(2)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729ee71-a65d-482b-a00a-d54d9496fbfb",
   "metadata": {},
   "source": [
    "## 3. Implementation of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce01539-3375-43fd-a55d-59e35c6c1da4",
   "metadata": {},
   "source": [
    "Clean tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868c513f-f98f-4ff7-8d2a-cfb6f445f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    # 3. Replace user mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # 4. Remove hashtags but keep the tag text\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # 5. Remove RT (retweet marker)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    # 6. Remove emojis or translate them to text\n",
    "    text = emoji.demojize(text)  # \":smile:\" etc\n",
    "    # 7. Remove non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # 8. Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a57466-6085-479a-a17f-ec1d793080a4",
   "metadata": {},
   "source": [
    "Pipeline of Clean+TfidfVectorizer+linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "820aa2f8-0d8e-4e64-92ea-2e5a3b99595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.548689138576779\n"
     ]
    }
   ],
   "source": [
    "# the actual classification algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "# for converting training and test datasets into matrices\n",
    "# TfidfVectorizer does this specifically for documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# for splitting the dataset into training and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = crowd_train_data['text']\n",
    "Y = crowd_train_data['sentiment']\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline( TfidfVectorizer(preprocessor=clean_tweet,), LinearSVC(dual='auto') )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X_train, Y_train)\n",
    "acc = accuracy_score(Y_eval, clf.predict(X_eval))\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf6cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410d7af5-7e99-41ef-9d8c-6731ea5bda3c",
   "metadata": {},
   "source": [
    "We used hyperparameter search and cross-validation to select a set of parameters that work better on unknown data in order to maximise the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54687",
   "metadata": {},
   "source": [
    "We also tried using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0606d16e-c034-465e-b9e6-83e10439f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6441947565543071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(preprocessor=clean_tweet,), LogisticRegression(\n",
    "    penalty='l2',            # L2 regularization\n",
    "    C=1.0,                   # Regularization strength (default)\n",
    "    solver='liblinear',      # Solver suitable for smaller datasets\n",
    "    max_iter=100,            # Iterations for convergence\n",
    "    class_weight='balanced', # Handle imbalanced classes\n",
    "    fit_intercept=True,      # Include intercept\n",
    "))\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X_train, Y_train)\n",
    "acc = accuracy_score(Y_eval, clf.predict(X_eval))\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c8aca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Benchmarking models...\n",
      "\n",
      "MultinomialNB            : Mean Accuracy = 0.6132 ± 0.0062\n",
      "LogisticRegression       : Mean Accuracy = 0.6318 ± 0.0062\n",
      "LinearSVC                : Mean Accuracy = 0.6035 ± 0.0104\n",
      "RidgeClassifier          : Mean Accuracy = 0.6272 ± 0.0112\n",
      "\n",
      "📊 Classification Report (RidgeClassifier on hold-out set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.32      0.42       332\n",
      "     neutral       0.63      0.76      0.69      1073\n",
      "    positive       0.64      0.58      0.61       731\n",
      "\n",
      "    accuracy                           0.63      2136\n",
      "   macro avg       0.62      0.55      0.57      2136\n",
      "weighted avg       0.63      0.63      0.62      2136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, class_weight='balanced', solver='liblinear'),\n",
    "    \"LinearSVC\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "}\n",
    "\n",
    "# TF-IDF settings\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor=clean_tweet,         # Apply tweet cleaning function\n",
    "    ngram_range=(1, 2),               # Unigrams + bigrams\n",
    "    stop_words='english',             # Use English stop words\n",
    "    max_features=10000,               # Limit to top 10,000 features\n",
    "    sublinear_tf=True,                # Use log scaling for term frequency\n",
    "    min_df=3                          # Ignore terms that appear in fewer than 3 documents\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"🔍 Benchmarking models...\\n\")\n",
    "for name, model in models.items():\n",
    "    pipeline = make_pipeline(tfidf, model)\n",
    "    scores = cross_val_score(pipeline, X, Y, cv=cv, scoring='accuracy')\n",
    "    print(f\"{name:<25}: Mean Accuracy = {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Final evaluation on hold-out set\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "final_model = make_pipeline(tfidf, RidgeClassifier())\n",
    "final_model.fit(X_train, Y_train)\n",
    "preds = final_model.predict(X_eval)\n",
    "\n",
    "print(\"\\n📊 Classification Report (RidgeClassifier on hold-out set):\")\n",
    "print(classification_report(Y_eval, preds))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b361009-f79c-43d6-90be-63f8a4fd6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.95884597\n",
      "Iteration 2, loss = 0.66361254\n",
      "Iteration 3, loss = 0.38862284\n",
      "Iteration 4, loss = 0.18520055\n",
      "Iteration 5, loss = 0.06855323\n",
      "Iteration 6, loss = 0.02281874\n",
      "Iteration 7, loss = 0.01015891\n",
      "Iteration 8, loss = 0.00597680\n",
      "Iteration 9, loss = 0.00387828\n",
      "Iteration 10, loss = 0.00323096\n",
      "Iteration 11, loss = 0.00301844\n",
      "Iteration 12, loss = 0.00220541\n",
      "Iteration 13, loss = 0.00274713\n",
      "Iteration 14, loss = 0.00211303\n",
      "Iteration 15, loss = 0.00208056\n",
      "Iteration 16, loss = 0.00183821\n",
      "Iteration 17, loss = 0.00168452\n",
      "Iteration 18, loss = 0.00182496\n",
      "Iteration 19, loss = 0.00206072\n",
      "Iteration 20, loss = 0.00130422\n",
      "Iteration 21, loss = 0.00212225\n",
      "Iteration 22, loss = 0.00159790\n",
      "Iteration 23, loss = 0.00154578\n",
      "Iteration 24, loss = 0.00150607\n",
      "Iteration 25, loss = 0.00240101\n",
      "Iteration 26, loss = 0.00207064\n",
      "Iteration 27, loss = 0.00164088\n",
      "Iteration 28, loss = 0.00197591\n",
      "Iteration 29, loss = 0.00188707\n",
      "Iteration 30, loss = 0.00135071\n",
      "Iteration 31, loss = 0.00140986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLPClassifier Accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data (adjust this for your dataset)\n",
    "# Assuming crowd_train_data is your dataset with columns 'text' and 'sentiment'\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "# Encode the labels (since you have 3 classes)\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y_encoded, test_size=0.2, random_state=0)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_eval_tfidf = tfidf.transform(X_eval).toarray()\n",
    "\n",
    "# MLP Neural Network Model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),  # Two hidden layers with 512 and 256 neurons\n",
    "    activation='relu',              # ReLU activation function\n",
    "    solver='adam',                  # Adam optimizer\n",
    "    max_iter=100,                   # Number of iterations\n",
    "    random_state=0,                 # Random seed for reproducibility\n",
    "    verbose=True                    # Output progress during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred = mlp.predict(X_eval_tfidf)\n",
    "\n",
    "# Accuracy score\n",
    "acc = accuracy_score(Y_eval, Y_pred)\n",
    "print(f\"MLPClassifier Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "843d19da-668d-47ea-b1de-b5e3db6a777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03187873\n",
      "Iteration 2, loss = 1.00150204\n",
      "Iteration 3, loss = 0.99970938\n",
      "Iteration 4, loss = 0.99863038\n",
      "Iteration 5, loss = 0.99715840\n",
      "Iteration 6, loss = 0.99576138\n",
      "Iteration 7, loss = 0.99414437\n",
      "Iteration 8, loss = 0.99244218\n",
      "Iteration 9, loss = 0.99067205\n",
      "Iteration 10, loss = 0.98866992\n",
      "Iteration 11, loss = 0.98677097\n",
      "Iteration 12, loss = 0.98467793\n",
      "Iteration 13, loss = 0.98235667\n",
      "Iteration 14, loss = 0.98001638\n",
      "Iteration 15, loss = 0.97738927\n",
      "Iteration 16, loss = 0.97475543\n",
      "Iteration 17, loss = 0.97199800\n",
      "Iteration 18, loss = 0.96892930\n",
      "Iteration 19, loss = 0.96581807\n",
      "Iteration 20, loss = 0.96250372\n",
      "Iteration 21, loss = 0.95896755\n",
      "Iteration 22, loss = 0.95583468\n",
      "Iteration 23, loss = 0.95152028\n",
      "Iteration 24, loss = 0.94742007\n",
      "Iteration 25, loss = 0.94306210\n",
      "Iteration 26, loss = 0.93877794\n",
      "Iteration 27, loss = 0.93418454\n",
      "Iteration 28, loss = 0.92923885\n",
      "Iteration 29, loss = 0.92428720\n",
      "Iteration 30, loss = 0.91947063\n",
      "Iteration 31, loss = 0.91403816\n",
      "Iteration 32, loss = 0.90857330\n",
      "Iteration 33, loss = 0.90334517\n",
      "Iteration 34, loss = 0.89734152\n",
      "Iteration 35, loss = 0.89156312\n",
      "Iteration 36, loss = 0.88629969\n",
      "Iteration 37, loss = 0.88007434\n",
      "Iteration 38, loss = 0.87387218\n",
      "Iteration 39, loss = 0.86827848\n",
      "Iteration 40, loss = 0.86198009\n",
      "Iteration 41, loss = 0.85573487\n",
      "Iteration 42, loss = 0.84983741\n",
      "Iteration 43, loss = 0.84355640\n",
      "Iteration 44, loss = 0.83752849\n",
      "Iteration 45, loss = 0.83117321\n",
      "Iteration 46, loss = 0.82531207\n",
      "Iteration 47, loss = 0.81870668\n",
      "Iteration 48, loss = 0.81259898\n",
      "Iteration 49, loss = 0.80642040\n",
      "Iteration 50, loss = 0.80023475\n",
      "Iteration 51, loss = 0.79409517\n",
      "Iteration 52, loss = 0.78768978\n",
      "Iteration 53, loss = 0.78190261\n",
      "Iteration 54, loss = 0.77593785\n",
      "Iteration 55, loss = 0.76916354\n",
      "Iteration 56, loss = 0.76311263\n",
      "Iteration 57, loss = 0.75700759\n",
      "Iteration 58, loss = 0.75094184\n",
      "Iteration 59, loss = 0.74491803\n",
      "Iteration 60, loss = 0.73858834\n",
      "Iteration 61, loss = 0.73230172\n",
      "Iteration 62, loss = 0.72701670\n",
      "Iteration 63, loss = 0.72116888\n",
      "Iteration 64, loss = 0.71439466\n",
      "Iteration 65, loss = 0.70848325\n",
      "Iteration 66, loss = 0.70283856\n",
      "Iteration 67, loss = 0.69756585\n",
      "Iteration 68, loss = 0.69153254\n",
      "Iteration 69, loss = 0.68501434\n",
      "Iteration 70, loss = 0.67916766\n",
      "Iteration 71, loss = 0.67362506\n",
      "Iteration 72, loss = 0.66741703\n",
      "Iteration 73, loss = 0.66164461\n",
      "Iteration 74, loss = 0.65644475\n",
      "Iteration 75, loss = 0.65044285\n",
      "Iteration 76, loss = 0.64561984\n",
      "Iteration 77, loss = 0.63961575\n",
      "Iteration 78, loss = 0.63378857\n",
      "Iteration 79, loss = 0.62779785\n",
      "Iteration 80, loss = 0.62320147\n",
      "Iteration 81, loss = 0.61721374\n",
      "Iteration 82, loss = 0.61122311\n",
      "Iteration 83, loss = 0.60592609\n",
      "Iteration 84, loss = 0.60053961\n",
      "Iteration 85, loss = 0.59481316\n",
      "Iteration 86, loss = 0.58952871\n",
      "Iteration 87, loss = 0.58459521\n",
      "Iteration 88, loss = 0.57949887\n",
      "Iteration 89, loss = 0.57398731\n",
      "Iteration 90, loss = 0.56920828\n",
      "Iteration 91, loss = 0.56442935\n",
      "Iteration 92, loss = 0.55909981\n",
      "Iteration 93, loss = 0.55306074\n",
      "Iteration 94, loss = 0.54855810\n",
      "Iteration 95, loss = 0.54286549\n",
      "Iteration 96, loss = 0.53798129\n",
      "Iteration 97, loss = 0.53267643\n",
      "Iteration 98, loss = 0.52794622\n",
      "Iteration 99, loss = 0.52329568\n",
      "Iteration 100, loss = 0.51799073\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time= 1.3min\n",
      "Iteration 1, loss = 5.87233166\n",
      "Iteration 2, loss = 2.83527757\n",
      "Iteration 3, loss = 1.87962951\n",
      "Iteration 4, loss = 1.41573742\n",
      "Iteration 5, loss = 1.19373627\n",
      "Iteration 6, loss = 1.09109734\n",
      "Iteration 7, loss = 1.04565902\n",
      "Iteration 8, loss = 1.02609920\n",
      "Iteration 9, loss = 1.01748902\n",
      "Iteration 10, loss = 1.01360724\n",
      "Iteration 11, loss = 1.01161375\n",
      "Iteration 12, loss = 1.01034322\n",
      "Iteration 13, loss = 1.01003231\n",
      "Iteration 14, loss = 1.00927984\n",
      "Iteration 15, loss = 1.00902793\n",
      "Iteration 16, loss = 1.00846070\n",
      "Iteration 17, loss = 1.00848221\n",
      "Iteration 18, loss = 1.00812401\n",
      "Iteration 19, loss = 1.00795822\n",
      "Iteration 20, loss = 1.00750218\n",
      "Iteration 21, loss = 1.00698524\n",
      "Iteration 22, loss = 1.00678860\n",
      "Iteration 23, loss = 1.00770265\n",
      "Iteration 24, loss = 1.00631136\n",
      "Iteration 25, loss = 1.00662186\n",
      "Iteration 26, loss = 1.00599410\n",
      "Iteration 27, loss = 1.00607856\n",
      "Iteration 28, loss = 1.00571991\n",
      "Iteration 29, loss = 1.00614910\n",
      "Iteration 30, loss = 1.00564409\n",
      "Iteration 31, loss = 1.00550949\n",
      "Iteration 32, loss = 1.00570167\n",
      "Iteration 33, loss = 1.00573227\n",
      "Iteration 34, loss = 1.00481036\n",
      "Iteration 35, loss = 1.00489316\n",
      "Iteration 36, loss = 1.00484243\n",
      "Iteration 37, loss = 1.00478087\n",
      "Iteration 38, loss = 1.00436668\n",
      "Iteration 39, loss = 1.00429173\n",
      "Iteration 40, loss = 1.00498548\n",
      "Iteration 41, loss = 1.00405431\n",
      "Iteration 42, loss = 1.00470466\n",
      "Iteration 43, loss = 1.00409347\n",
      "Iteration 44, loss = 1.00388914\n",
      "Iteration 45, loss = 1.00413166\n",
      "Iteration 46, loss = 1.00404074\n",
      "Iteration 47, loss = 1.00376798\n",
      "Iteration 48, loss = 1.00347691\n",
      "Iteration 49, loss = 1.00416789\n",
      "Iteration 50, loss = 1.00362123\n",
      "Iteration 51, loss = 1.00429759\n",
      "Iteration 52, loss = 1.00381068\n",
      "Iteration 53, loss = 1.00360913\n",
      "Iteration 54, loss = 1.00386832\n",
      "Iteration 55, loss = 1.00384682\n",
      "Iteration 56, loss = 1.00365375\n",
      "Iteration 57, loss = 1.00334984\n",
      "Iteration 58, loss = 1.00373280\n",
      "Iteration 59, loss = 1.00318753\n",
      "Iteration 60, loss = 1.00318146\n",
      "Iteration 61, loss = 1.00319976\n",
      "Iteration 62, loss = 1.00362884\n",
      "Iteration 63, loss = 1.00366200\n",
      "Iteration 64, loss = 1.00295072\n",
      "Iteration 65, loss = 1.00301164\n",
      "Iteration 66, loss = 1.00324102\n",
      "Iteration 67, loss = 1.00322029\n",
      "Iteration 68, loss = 1.00298285\n",
      "Iteration 69, loss = 1.00372979\n",
      "Iteration 70, loss = 1.00291702\n",
      "Iteration 71, loss = 1.00321564\n",
      "Iteration 72, loss = 1.00293948\n",
      "Iteration 73, loss = 1.00334769\n",
      "Iteration 74, loss = 1.00284895\n",
      "Iteration 75, loss = 1.00302138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=adam; total time=  55.6s\n",
      "Iteration 1, loss = 3.51536256\n",
      "Iteration 2, loss = 1.28659468\n",
      "Iteration 3, loss = 1.10070074\n",
      "Iteration 4, loss = 1.07386272\n",
      "Iteration 5, loss = 1.09231528\n",
      "Iteration 6, loss = 1.13330066\n",
      "Iteration 7, loss = 1.08471509\n",
      "Iteration 8, loss = 1.10190168\n",
      "Iteration 9, loss = 1.11284783\n",
      "Iteration 10, loss = 1.23640363\n",
      "Iteration 11, loss = 1.07808852\n",
      "Iteration 12, loss = 1.46595737\n",
      "Iteration 13, loss = 2.04700999\n",
      "Iteration 14, loss = 1.51357816\n",
      "Iteration 15, loss = 1.13744974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  16.7s\n",
      "Iteration 1, loss = 1.54356213\n",
      "Iteration 2, loss = 1.13198785\n",
      "Iteration 3, loss = 1.03065470\n",
      "Iteration 4, loss = 1.01784216\n",
      "Iteration 5, loss = 1.01188802\n",
      "Iteration 6, loss = 1.01895564\n",
      "Iteration 7, loss = 1.01153964\n",
      "Iteration 8, loss = 1.01460677\n",
      "Iteration 9, loss = 1.01510728\n",
      "Iteration 10, loss = 1.01372191\n",
      "Iteration 11, loss = 1.01475709\n",
      "Iteration 12, loss = 1.01262516\n",
      "Iteration 13, loss = 1.01435073\n",
      "Iteration 14, loss = 1.00841226\n",
      "Iteration 15, loss = 1.00905313\n",
      "Iteration 16, loss = 1.00810421\n",
      "Iteration 17, loss = 1.01160214\n",
      "Iteration 18, loss = 1.00858264\n",
      "Iteration 19, loss = 1.01083179\n",
      "Iteration 20, loss = 1.00796372\n",
      "Iteration 21, loss = 1.00730619\n",
      "Iteration 22, loss = 1.01018717\n",
      "Iteration 23, loss = 1.01082731\n",
      "Iteration 24, loss = 1.00664539\n",
      "Iteration 25, loss = 1.00625879\n",
      "Iteration 26, loss = 1.00601975\n",
      "Iteration 27, loss = 1.00945758\n",
      "Iteration 28, loss = 1.00833462\n",
      "Iteration 29, loss = 1.00686655\n",
      "Iteration 30, loss = 1.00817264\n",
      "Iteration 31, loss = 1.00915495\n",
      "Iteration 32, loss = 1.00640902\n",
      "Iteration 33, loss = 1.00629587\n",
      "Iteration 34, loss = 1.00585370\n",
      "Iteration 35, loss = 1.00881997\n",
      "Iteration 36, loss = 1.00748791\n",
      "Iteration 37, loss = 1.00710043\n",
      "Iteration 38, loss = 1.00680084\n",
      "Iteration 39, loss = 1.00542152\n",
      "Iteration 40, loss = 1.00563029\n",
      "Iteration 41, loss = 1.00666520\n",
      "Iteration 42, loss = 1.00530670\n",
      "Iteration 43, loss = 1.00584043Iteration 1, loss = nan\n",
      "Iteration 2, loss = nan\n",
      "Iteration 3, loss = nan\n",
      "Iteration 4, loss = nan\n",
      "Iteration 5, loss = nan\n",
      "Iteration 6, loss = nan\n",
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  52.3s\n",
      "Iteration 1, loss = 2.39234579\n",
      "Iteration 2, loss = 1.04327491\n",
      "Iteration 3, loss = 1.01743922\n",
      "Iteration 4, loss = 1.01272126\n",
      "Iteration 5, loss = 1.01387280\n",
      "Iteration 6, loss = 1.01029830\n",
      "Iteration 7, loss = 1.00862148\n",
      "Iteration 8, loss = 1.01192202\n",
      "Iteration 9, loss = 1.01466210\n",
      "Iteration 10, loss = 1.01316799\n",
      "Iteration 11, loss = 1.01220792\n",
      "Iteration 12, loss = 1.01247117\n",
      "Iteration 13, loss = 1.01819428\n",
      "Iteration 14, loss = 1.00802267\n",
      "Iteration 15, loss = 1.00788448\n",
      "Iteration 16, loss = 1.00996988\n",
      "Iteration 17, loss = 1.00824351\n",
      "Iteration 18, loss = 1.00654011\n",
      "Iteration 19, loss = 1.00674908\n",
      "Iteration 20, loss = 1.01079717\n",
      "Iteration 21, loss = 1.00703215\n",
      "Iteration 22, loss = 1.00814511\n",
      "Iteration 23, loss = 1.00863210\n",
      "Iteration 24, loss = 1.00732444\n",
      "Iteration 25, loss = 1.00694031\n",
      "Iteration 26, loss = 1.01360225\n",
      "Iteration 27, loss = 1.00961286\n",
      "Iteration 28, loss = 1.00712569\n",
      "Iteration 29, loss = 1.01168261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  16.0s\n",
      "Iteration 1, loss = 320.84951069\n",
      "Iteration 2, loss = 19.93269278\n",
      "Iteration 3, loss = 2.27048506\n",
      "Iteration 4, loss = 1.09692261\n",
      "Iteration 5, loss = 1.01496971\n",
      "Iteration 6, loss = 1.00633602\n",
      "Iteration 7, loss = 1.00669673\n",
      "Iteration 8, loss = 1.00820510\n",
      "Iteration 9, loss = 1.00789115\n",
      "Iteration 10, loss = 1.00937502\n",
      "Iteration 11, loss = 1.01168023\n",
      "Iteration 12, loss = 1.01811244\n",
      "Iteration 13, loss = 1.01262749\n",
      "Iteration 14, loss = 1.01228340\n",
      "Iteration 15, loss = 1.00973133\n",
      "Iteration 16, loss = 1.01220754\n",
      "Iteration 17, loss = 1.01109006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=  19.0s\n",
      "Iteration 1, loss = 1.03741543\n",
      "Iteration 2, loss = 1.00377029\n",
      "Iteration 3, loss = 0.99283270\n",
      "Iteration 4, loss = 0.93441139\n",
      "Iteration 5, loss = 0.83312426\n",
      "Iteration 6, loss = 0.73605018\n",
      "Iteration 7, loss = 0.66131824\n",
      "Iteration 8, loss = 0.58562723\n",
      "Iteration 9, loss = 0.50521759\n",
      "Iteration 10, loss = 0.43520064\n",
      "Iteration 11, loss = 0.36100920\n",
      "Iteration 12, loss = 0.31722150\n",
      "Iteration 13, loss = 0.27786720\n",
      "Iteration 14, loss = 0.25213517\n",
      "Iteration 15, loss = 0.23830159\n",
      "Iteration 16, loss = 0.21767449\n",
      "Iteration 17, loss = 0.20449854\n",
      "Iteration 18, loss = 0.19428390\n",
      "Iteration 19, loss = 0.19924769\n",
      "Iteration 20, loss = 0.19891668\n",
      "Iteration 21, loss = 0.19914869\n",
      "Iteration 22, loss = 0.20096352\n",
      "Iteration 23, loss = 0.20366989\n",
      "Iteration 24, loss = 0.21329091\n",
      "Iteration 25, loss = 0.25320024\n",
      "Iteration 26, loss = 0.23978726\n",
      "Iteration 27, loss = 0.21221937\n",
      "Iteration 28, loss = 0.18428569\n",
      "Iteration 29, loss = 0.16245607\n",
      "Iteration 30, loss = 0.14785463\n",
      "Iteration 31, loss = 0.13358799\n",
      "Iteration 32, loss = 0.12289850\n",
      "Iteration 33, loss = 0.11700643\n",
      "Iteration 34, loss = 0.11302545\n",
      "Iteration 35, loss = 0.11759492\n",
      "Iteration 36, loss = 0.12446341\n",
      "Iteration 37, loss = 0.16411411\n",
      "Iteration 38, loss = 0.24707851\n",
      "Iteration 39, loss = 0.32328070\n",
      "Iteration 40, loss = 0.33528985\n",
      "Iteration 41, loss = 0.29819020\n",
      "Iteration 42, loss = 0.26310539\n",
      "Iteration 43, loss = 0.21486513\n",
      "Iteration 44, loss = 0.17587687\n",
      "Iteration 45, loss = 0.14949565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=adam; total time=  33.2s\n",
      "Iteration 1, loss = 3.65332784\n",
      "Iteration 2, loss = 1.95075317\n",
      "Iteration 3, loss = 1.62150471\n",
      "Iteration 4, loss = 2.21246902\n",
      "Iteration 5, loss = 2.31437232\n",
      "Iteration 6, loss = 1.51203871\n",
      "Iteration 7, loss = 1.56288541\n",
      "Iteration 8, loss = 1.21075996\n",
      "Iteration 9, loss = 1.11106740\n",
      "Iteration 10, loss = 1.26633547\n",
      "Iteration 11, loss = 1.73970807\n",
      "Iteration 12, loss = 1.47909694\n",
      "Iteration 13, loss = 1.67969367\n",
      "Iteration 14, loss = 1.85958950\n",
      "Iteration 15, loss = 1.31127286\n",
      "Iteration 16, loss = 1.63810765\n",
      "Iteration 17, loss = 1.18920970\n",
      "Iteration 18, loss = 1.34948665\n",
      "Iteration 19, loss = 1.72264082\n",
      "Iteration 20, loss = 1.64386589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  22.7s\n",
      "Iteration 1, loss = 19.73834329\n",
      "Iteration 2, loss = 1.39706448\n",
      "Iteration 3, loss = 1.03022639\n",
      "Iteration 4, loss = 1.00589161\n",
      "Iteration 5, loss = 1.00350513\n",
      "Iteration 6, loss = 1.00288571\n",
      "Iteration 7, loss = 1.00287144\n",
      "Iteration 8, loss = 1.00240002\n",
      "Iteration 9, loss = 1.00239959\n",
      "Iteration 10, loss = 1.00270243\n",
      "Iteration 11, loss = 1.00243349\n",
      "Iteration 12, loss = 1.00247738\n",
      "Iteration 13, loss = 1.00258595\n",
      "Iteration 14, loss = 1.00235788\n",
      "Iteration 15, loss = 1.00230597\n",
      "Iteration 16, loss = 1.00255255\n",
      "Iteration 17, loss = 1.00239744\n",
      "Iteration 18, loss = 1.00245350\n",
      "Iteration 19, loss = 1.00252957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time=  10.7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03343804\n",
      "Iteration 2, loss = 1.00153643\n",
      "Iteration 3, loss = 1.00025507\n",
      "Iteration 4, loss = 0.99909833\n",
      "Iteration 5, loss = 0.99806253\n",
      "Iteration 6, loss = 0.99675503\n",
      "Iteration 7, loss = 0.99557218\n",
      "Iteration 8, loss = 0.99401990\n",
      "Iteration 9, loss = 0.99254956\n",
      "Iteration 10, loss = 0.99060208\n",
      "Iteration 11, loss = 0.98864816\n",
      "Iteration 12, loss = 0.98662688\n",
      "Iteration 13, loss = 0.98449493\n",
      "Iteration 14, loss = 0.98249385\n",
      "Iteration 15, loss = 0.97997143\n",
      "Iteration 16, loss = 0.97712490\n",
      "Iteration 17, loss = 0.97462638\n",
      "Iteration 18, loss = 0.97163067\n",
      "Iteration 19, loss = 0.96838777\n",
      "Iteration 20, loss = 0.96497817\n",
      "Iteration 21, loss = 0.96158381\n",
      "Iteration 22, loss = 0.95799890\n",
      "Iteration 23, loss = 0.95423863\n",
      "Iteration 24, loss = 0.95032632\n",
      "Iteration 25, loss = 0.94602453\n",
      "Iteration 26, loss = 0.94150766\n",
      "Iteration 27, loss = 0.93745187\n",
      "Iteration 28, loss = 0.93284859\n",
      "Iteration 29, loss = 0.92764980\n",
      "Iteration 30, loss = 0.92218826\n",
      "Iteration 31, loss = 0.91692882\n",
      "Iteration 32, loss = 0.91151880\n",
      "Iteration 33, loss = 0.90608761\n",
      "Iteration 34, loss = 0.90087143\n",
      "Iteration 35, loss = 0.89477880\n",
      "Iteration 36, loss = 0.88881111\n",
      "Iteration 37, loss = 0.88288751\n",
      "Iteration 38, loss = 0.87741418\n",
      "Iteration 39, loss = 0.87130288\n",
      "Iteration 40, loss = 0.86512380\n",
      "Iteration 41, loss = 0.85902121\n",
      "Iteration 42, loss = 0.85312282\n",
      "Iteration 43, loss = 0.84777599\n",
      "Iteration 44, loss = 0.84133548\n",
      "Iteration 45, loss = 0.83532569\n",
      "Iteration 46, loss = 0.82948445\n",
      "Iteration 47, loss = 0.82308369\n",
      "Iteration 48, loss = 0.81702493\n",
      "Iteration 49, loss = 0.81102489\n",
      "Iteration 50, loss = 0.80566962\n",
      "Iteration 51, loss = 0.79982965\n",
      "Iteration 52, loss = 0.79353450\n",
      "Iteration 53, loss = 0.78771705\n",
      "Iteration 54, loss = 0.78149060\n",
      "Iteration 55, loss = 0.77620577\n",
      "Iteration 56, loss = 0.76974323\n",
      "Iteration 57, loss = 0.76378643\n",
      "Iteration 58, loss = 0.75809670\n",
      "Iteration 59, loss = 0.75200459\n",
      "Iteration 60, loss = 0.74634766\n",
      "Iteration 61, loss = 0.74065579\n",
      "Iteration 62, loss = 0.73457116\n",
      "Iteration 63, loss = 0.72929061\n",
      "Iteration 64, loss = 0.72315999\n",
      "Iteration 65, loss = 0.71777328\n",
      "Iteration 66, loss = 0.71237705\n",
      "Iteration 67, loss = 0.70631152\n",
      "Iteration 68, loss = 0.70014718\n",
      "Iteration 69, loss = 0.69495732\n",
      "Iteration 70, loss = 0.68936614\n",
      "Iteration 71, loss = 0.68361137\n",
      "Iteration 72, loss = 0.67760196\n",
      "Iteration 73, loss = 0.67255846\n",
      "Iteration 74, loss = 0.66682024\n",
      "Iteration 75, loss = 0.66139507\n",
      "Iteration 76, loss = 0.65583590\n",
      "Iteration 77, loss = 0.64992134\n",
      "Iteration 78, loss = 0.64484300\n",
      "Iteration 79, loss = 0.63913852\n",
      "Iteration 80, loss = 0.63452897\n",
      "Iteration 81, loss = 0.62851524\n",
      "Iteration 82, loss = 0.62337070\n",
      "Iteration 83, loss = 0.61836630\n",
      "Iteration 84, loss = 0.61292242\n",
      "Iteration 85, loss = 0.60757926\n",
      "Iteration 86, loss = 0.60181459\n",
      "Iteration 87, loss = 0.59667911\n",
      "Iteration 88, loss = 0.59128006\n",
      "Iteration 89, loss = 0.58678428\n",
      "Iteration 90, loss = 0.58119094\n",
      "Iteration 91, loss = 0.57632038\n",
      "Iteration 92, loss = 0.57085319\n",
      "Iteration 93, loss = 0.56555346\n",
      "Iteration 94, loss = 0.56088616\n",
      "Iteration 95, loss = 0.55576517\n",
      "Iteration 96, loss = 0.55082737\n",
      "Iteration 97, loss = 0.54571562\n",
      "Iteration 98, loss = 0.54098248\n",
      "Iteration 99, loss = 0.53600271\n",
      "Iteration 100, loss = 0.53149007\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time= 1.3min\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  27.1s\n",
      "Iteration 1, loss = 331.75471607\n",
      "Iteration 2, loss = 7.75544097\n",
      "Iteration 3, loss = 1.41138234\n",
      "Iteration 4, loss = 1.03834756\n",
      "Iteration 5, loss = 1.01841503\n",
      "Iteration 6, loss = 1.01584023\n",
      "Iteration 7, loss = 1.01397164\n",
      "Iteration 8, loss = 1.01235725\n",
      "Iteration 9, loss = 1.01101273\n",
      "Iteration 10, loss = 1.00985332\n",
      "Iteration 11, loss = 1.00886905\n",
      "Iteration 12, loss = 1.00802126\n",
      "Iteration 13, loss = 1.00728121\n",
      "Iteration 14, loss = 1.00666316\n",
      "Iteration 15, loss = 1.00611194\n",
      "Iteration 16, loss = 1.00562979\n",
      "Iteration 17, loss = 1.00522532\n",
      "Iteration 18, loss = 1.00487202\n",
      "Iteration 19, loss = 1.00456589\n",
      "Iteration 20, loss = 1.00429308\n",
      "Iteration 21, loss = 1.00405167\n",
      "Iteration 22, loss = 1.00385293\n",
      "Iteration 23, loss = 1.00365724\n",
      "Iteration 24, loss = 1.00351093\n",
      "Iteration 25, loss = 1.00335169\n",
      "Iteration 26, loss = 1.00321859\n",
      "Iteration 27, loss = 1.00310850\n",
      "Iteration 28, loss = 1.00301514\n",
      "Iteration 29, loss = 1.00292003\n",
      "Iteration 30, loss = 1.00284105\n",
      "Iteration 31, loss = 1.00276128\n",
      "Iteration 32, loss = 1.00270686\n",
      "Iteration 33, loss = 1.00265665\n",
      "Iteration 34, loss = 1.00261240\n",
      "Iteration 35, loss = 1.00257449\n",
      "Iteration 36, loss = 1.00253554\n",
      "Iteration 37, loss = 1.00250544\n",
      "Iteration 38, loss = 1.00247578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time=  29.1s\n",
      "Iteration 1, loss = 3.10184202\n",
      "Iteration 2, loss = 1.12039338\n",
      "Iteration 3, loss = 1.03139151\n",
      "Iteration 4, loss = 1.08547952\n",
      "Iteration 5, loss = 1.13648341\n",
      "Iteration 6, loss = 1.06741885\n",
      "Iteration 7, loss = 1.05112376\n",
      "Iteration 8, loss = 1.05079708\n",
      "Iteration 9, loss = 1.17400713\n",
      "Iteration 10, loss = 1.10800541\n",
      "Iteration 11, loss = 1.11762445\n",
      "Iteration 12, loss = 1.21988306\n",
      "Iteration 13, loss = 1.09316175\n",
      "Iteration 14, loss = 1.07484053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  16.1s\n",
      "Iteration 1, loss = 1.71808674\n",
      "Iteration 2, loss = 1.03884798\n",
      "Iteration 3, loss = 1.03330657\n",
      "Iteration 4, loss = 1.01380150\n",
      "Iteration 5, loss = 1.01672870\n",
      "Iteration 6, loss = 1.02772392\n",
      "Iteration 7, loss = 1.01789673\n",
      "Iteration 8, loss = 1.01232452\n",
      "Iteration 9, loss = 1.01073149\n",
      "Iteration 10, loss = 1.00810080\n",
      "Iteration 11, loss = 1.00932943\n",
      "Iteration 12, loss = 1.00874425\n",
      "Iteration 13, loss = 1.00709643\n",
      "Iteration 14, loss = 1.00651279\n",
      "Iteration 15, loss = 1.00944426\n",
      "Iteration 16, loss = 1.01333244\n",
      "Iteration 17, loss = 1.00813210\n",
      "Iteration 18, loss = 1.00986497\n",
      "Iteration 19, loss = 1.00867829\n",
      "Iteration 20, loss = 1.00879952\n",
      "Iteration 21, loss = 1.00650824\n",
      "Iteration 22, loss = 1.00932274\n",
      "Iteration 23, loss = 1.00885677\n",
      "Iteration 24, loss = 1.00633476\n",
      "Iteration 25, loss = 1.00736125\n",
      "Iteration 26, loss = 1.00474257\n",
      "Iteration 27, loss = 1.00922598\n",
      "Iteration 28, loss = 1.00693600\n",
      "Iteration 29, loss = 1.00609857\n",
      "Iteration 30, loss = 1.00533016\n",
      "Iteration 31, loss = 1.00657682\n",
      "Iteration 32, loss = 1.00820053\n",
      "Iteration 33, loss = 1.00852108\n",
      "Iteration 34, loss = 1.00525706\n",
      "Iteration 35, loss = 1.00698551\n",
      "Iteration 36, loss = 1.00768562\n",
      "Iteration 37, loss = 1.00691599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  19.4s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=   7.7s\n",
      "Iteration 1, loss = 1.41763840\n",
      "Iteration 2, loss = 1.07020756\n",
      "Iteration 3, loss = 1.08838082\n",
      "Iteration 4, loss = 1.17618538\n",
      "Iteration 5, loss = 1.24534410\n",
      "Iteration 6, loss = 1.28588065\n",
      "Iteration 7, loss = 1.25812426\n",
      "Iteration 8, loss = 1.27100391\n",
      "Iteration 9, loss = 1.29565236\n",
      "Iteration 10, loss = 1.30248738\n",
      "Iteration 11, loss = 1.24830724\n",
      "Iteration 12, loss = 1.22868161\n",
      "Iteration 13, loss = 1.22037897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.1, mlpclassifier__hidden_layer_sizes=(50,), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=   8.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = nan\n",
      "Iteration 2, loss = nan\n",
      "Iteration 3, loss = nan\n",
      "Iteration 4, loss = nan\n",
      "Iteration 5, loss = nan\n",
      "Iteration 6, loss = nan\n",
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  53.4s\n",
      "Iteration 1, loss = 2.45525876\n",
      "Iteration 2, loss = 1.03323564\n",
      "Iteration 3, loss = 1.02437027\n",
      "Iteration 4, loss = 1.01214340\n",
      "Iteration 5, loss = 1.00765759\n",
      "Iteration 6, loss = 1.01389706\n",
      "Iteration 7, loss = 1.00921585\n",
      "Iteration 8, loss = 1.01222805\n",
      "Iteration 9, loss = 1.01267862\n",
      "Iteration 10, loss = 1.01145831\n",
      "Iteration 11, loss = 1.01266272\n",
      "Iteration 12, loss = 1.01193272\n",
      "Iteration 13, loss = 1.01431517\n",
      "Iteration 14, loss = 1.00746971\n",
      "Iteration 15, loss = 1.00943788\n",
      "Iteration 16, loss = 1.00833337\n",
      "Iteration 17, loss = 1.01165937\n",
      "Iteration 18, loss = 1.00776587\n",
      "Iteration 19, loss = 1.01083917\n",
      "Iteration 20, loss = 1.00796740\n",
      "Iteration 21, loss = 1.00725164\n",
      "Iteration 22, loss = 1.00952691\n",
      "Iteration 23, loss = 1.00980466\n",
      "Iteration 24, loss = 1.00662044\n",
      "Iteration 25, loss = 1.00608658\n",
      "Iteration 26, loss = 1.00613859\n",
      "Iteration 27, loss = 1.00972070\n",
      "Iteration 28, loss = 1.00897232\n",
      "Iteration 29, loss = 1.00695045\n",
      "Iteration 30, loss = 1.00889982\n",
      "Iteration 31, loss = 1.00699068\n",
      "Iteration 32, loss = 1.00636221\n",
      "Iteration 33, loss = 1.00550531\n",
      "Iteration 34, loss = 1.00591577\n",
      "Iteration 35, loss = 1.00926629\n",
      "Iteration 36, loss = 1.00789160\n",
      "Iteration 37, loss = 1.00713022\n",
      "Iteration 38, loss = 1.00671812\n",
      "Iteration 39, loss = 1.00596958\n",
      "Iteration 40, loss = 1.00619868\n",
      "Iteration 41, loss = 1.00741260\n",
      "Iteration 42, loss = 1.00561156\n",
      "Iteration 43, loss = 1.00577383\n",
      "Iteration 44, loss = 1.00845544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  23.9s\n",
      "Iteration 1, loss = 6.15000854\n",
      "Iteration 2, loss = 3.00653599\n",
      "Iteration 3, loss = 2.01023536\n",
      "Iteration 4, loss = 1.51313633\n",
      "Iteration 5, loss = 1.26419405\n",
      "Iteration 6, loss = 1.14173303\n",
      "Iteration 7, loss = 1.08137608\n",
      "Iteration 8, loss = 1.05097274\n",
      "Iteration 9, loss = 1.03499435\n",
      "Iteration 10, loss = 1.02644552\n",
      "Iteration 11, loss = 1.02121412\n",
      "Iteration 12, loss = 1.01819861\n",
      "Iteration 13, loss = 1.01619910\n",
      "Iteration 14, loss = 1.01451544\n",
      "Iteration 15, loss = 1.01351903\n",
      "Iteration 16, loss = 1.01304134\n",
      "Iteration 17, loss = 1.01225012\n",
      "Iteration 18, loss = 1.01199745\n",
      "Iteration 19, loss = 1.01153390\n",
      "Iteration 20, loss = 1.01121142\n",
      "Iteration 21, loss = 1.01111590\n",
      "Iteration 22, loss = 1.01087866\n",
      "Iteration 23, loss = 1.01050946\n",
      "Iteration 24, loss = 1.01000465\n",
      "Iteration 25, loss = 1.00963504\n",
      "Iteration 26, loss = 1.00956072\n",
      "Iteration 27, loss = 1.00953215\n",
      "Iteration 28, loss = 1.00940796\n",
      "Iteration 29, loss = 1.00979148\n",
      "Iteration 30, loss = 1.00829836\n",
      "Iteration 31, loss = 1.00845922\n",
      "Iteration 32, loss = 1.00884343\n",
      "Iteration 33, loss = 1.00795573\n",
      "Iteration 34, loss = 1.00762085\n",
      "Iteration 35, loss = 1.00756080\n",
      "Iteration 36, loss = 1.00725752\n",
      "Iteration 37, loss = 1.00712532\n",
      "Iteration 38, loss = 1.00730304\n",
      "Iteration 39, loss = 1.00685061\n",
      "Iteration 40, loss = 1.00653908\n",
      "Iteration 41, loss = 1.00665170\n",
      "Iteration 42, loss = 1.00594160\n",
      "Iteration 43, loss = 1.00645143\n",
      "Iteration 44, loss = 1.00634464\n",
      "Iteration 45, loss = 1.00566750\n",
      "Iteration 46, loss = 1.00554062\n",
      "Iteration 47, loss = 1.00554469\n",
      "Iteration 48, loss = 1.00501262\n",
      "Iteration 49, loss = 1.00530194\n",
      "Iteration 50, loss = 1.00542096\n",
      "Iteration 51, loss = 1.00510630\n",
      "Iteration 52, loss = 1.00488535\n",
      "Iteration 53, loss = 1.00474807\n",
      "Iteration 54, loss = 1.00471599\n",
      "Iteration 55, loss = 1.00468256\n",
      "Iteration 56, loss = 1.00507999\n",
      "Iteration 57, loss = 1.00461837\n",
      "Iteration 58, loss = 1.00406815\n",
      "Iteration 59, loss = 1.00479244\n",
      "Iteration 60, loss = 1.00487227\n",
      "Iteration 61, loss = 1.00383403\n",
      "Iteration 62, loss = 1.00382847\n",
      "Iteration 63, loss = 1.00363982\n",
      "Iteration 64, loss = 1.00382665\n",
      "Iteration 65, loss = 1.00435026\n",
      "Iteration 66, loss = 1.00418666\n",
      "Iteration 67, loss = 1.00376551\n",
      "Iteration 68, loss = 1.00375225\n",
      "Iteration 69, loss = 1.00370579\n",
      "Iteration 70, loss = 1.00330283\n",
      "Iteration 71, loss = 1.00364762\n",
      "Iteration 72, loss = 1.00342454\n",
      "Iteration 73, loss = 1.00330651\n",
      "Iteration 74, loss = 1.00352073\n",
      "Iteration 75, loss = 1.00350852\n",
      "Iteration 76, loss = 1.00332093\n",
      "Iteration 77, loss = 1.00315622\n",
      "Iteration 78, loss = 1.00351656\n",
      "Iteration 79, loss = 1.00352991\n",
      "Iteration 80, loss = 1.00371881\n",
      "Iteration 81, loss = 1.00309202\n",
      "Iteration 82, loss = 1.00328746\n",
      "Iteration 83, loss = 1.00316699\n",
      "Iteration 84, loss = 1.00385401\n",
      "Iteration 85, loss = 1.00365265\n",
      "Iteration 86, loss = 1.00314546\n",
      "Iteration 87, loss = 1.00260986\n",
      "Iteration 88, loss = 1.00318998\n",
      "Iteration 89, loss = 1.00313437\n",
      "Iteration 90, loss = 1.00354358\n",
      "Iteration 91, loss = 1.00301027\n",
      "Iteration 92, loss = 1.00324358\n",
      "Iteration 93, loss = 1.00277090\n",
      "Iteration 94, loss = 1.00247420\n",
      "Iteration 95, loss = 1.00324435\n",
      "Iteration 96, loss = 1.00331311\n",
      "Iteration 97, loss = 1.00315629\n",
      "Iteration 98, loss = 1.00305949\n",
      "Iteration 99, loss = 1.00333271\n",
      "Iteration 100, loss = 1.00285392\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=adam; total time= 1.2min\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  26.6s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=   5.5s\n",
      "Iteration 1, loss = 1.01945908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = nan\n",
      "Iteration 2, loss = nan\n",
      "Iteration 3, loss = nan\n",
      "Iteration 4, loss = nan\n",
      "Iteration 5, loss = nan\n",
      "Iteration 6, loss = nan\n",
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  53.2s\n",
      "Iteration 1, loss = 2.53568516\n",
      "Iteration 2, loss = 1.02883974\n",
      "Iteration 3, loss = 1.01936771\n",
      "Iteration 4, loss = 1.00903126\n",
      "Iteration 5, loss = 1.00928664\n",
      "Iteration 6, loss = 1.01893107\n",
      "Iteration 7, loss = 1.01277445\n",
      "Iteration 8, loss = 1.00947342\n",
      "Iteration 9, loss = 1.00845659\n",
      "Iteration 10, loss = 1.00707072\n",
      "Iteration 11, loss = 1.00746181\n",
      "Iteration 12, loss = 1.00770481\n",
      "Iteration 13, loss = 1.00621304\n",
      "Iteration 14, loss = 1.00599354\n",
      "Iteration 15, loss = 1.00789864\n",
      "Iteration 16, loss = 1.01139813\n",
      "Iteration 17, loss = 1.00738158\n",
      "Iteration 18, loss = 1.00915198\n",
      "Iteration 19, loss = 1.00883209\n",
      "Iteration 20, loss = 1.00813557\n",
      "Iteration 21, loss = 1.00591281\n",
      "Iteration 22, loss = 1.00860210\n",
      "Iteration 23, loss = 1.00833820\n",
      "Iteration 24, loss = 1.00577714\n",
      "Iteration 25, loss = 1.00739204\n",
      "Iteration 26, loss = 1.00476830\n",
      "Iteration 27, loss = 1.00870083\n",
      "Iteration 28, loss = 1.00665036\n",
      "Iteration 29, loss = 1.00535470\n",
      "Iteration 30, loss = 1.00589146\n",
      "Iteration 31, loss = 1.00651004\n",
      "Iteration 32, loss = 1.00786436\n",
      "Iteration 33, loss = 1.00828644\n",
      "Iteration 34, loss = 1.00533567\n",
      "Iteration 35, loss = 1.00744949\n",
      "Iteration 36, loss = 1.00721299\n",
      "Iteration 37, loss = 1.00730971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  20.5s\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  26.3s\n",
      "Iteration 1, loss = 329.56538402\n",
      "Iteration 2, loss = 7.74816862\n",
      "Iteration 3, loss = 1.45016479\n",
      "Iteration 4, loss = 1.07061573\n",
      "Iteration 5, loss = 1.04480196\n",
      "Iteration 6, loss = 1.03715800\n",
      "Iteration 7, loss = 1.03122173\n",
      "Iteration 8, loss = 1.02629472\n",
      "Iteration 9, loss = 1.02222696\n",
      "Iteration 10, loss = 1.01889910\n",
      "Iteration 11, loss = 1.01611699\n",
      "Iteration 12, loss = 1.01385568\n",
      "Iteration 13, loss = 1.01196704\n",
      "Iteration 14, loss = 1.01041075\n",
      "Iteration 15, loss = 1.00914627\n",
      "Iteration 16, loss = 1.00810757\n",
      "Iteration 17, loss = 1.00722758\n",
      "Iteration 18, loss = 1.00652932\n",
      "Iteration 19, loss = 1.00585901\n",
      "Iteration 20, loss = 1.00533759\n",
      "Iteration 21, loss = 1.00488513\n",
      "Iteration 22, loss = 1.00451809\n",
      "Iteration 23, loss = 1.00419532\n",
      "Iteration 24, loss = 1.00391512\n",
      "Iteration 25, loss = 1.00368278\n",
      "Iteration 26, loss = 1.00349698\n",
      "Iteration 27, loss = 1.00331899\n",
      "Iteration 28, loss = 1.00316966\n",
      "Iteration 29, loss = 1.00304640\n",
      "Iteration 30, loss = 1.00295970\n",
      "Iteration 31, loss = 1.00286366\n",
      "Iteration 32, loss = 1.00278270\n",
      "Iteration 33, loss = 1.00270599\n",
      "Iteration 34, loss = 1.00265024\n",
      "Iteration 35, loss = 1.00258522\n",
      "Iteration 36, loss = 1.00253669\n",
      "Iteration 37, loss = 1.00249700\n",
      "Iteration 38, loss = 1.00245629\n",
      "Iteration 39, loss = 1.00242761\n",
      "Iteration 40, loss = 1.00239130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time=  31.5s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  13.0s\n",
      "Iteration 1, loss = 1.57419812\n",
      "Iteration 2, loss = 1.06293724\n",
      "Iteration 3, loss = 1.03785583\n",
      "Iteration 4, loss = 1.02607245\n",
      "Iteration 5, loss = 1.01849615\n",
      "Iteration 6, loss = 1.01505443\n",
      "Iteration 7, loss = 1.01072403\n",
      "Iteration 8, loss = 1.01456553\n",
      "Iteration 9, loss = 1.01669391\n",
      "Iteration 10, loss = 1.01528481\n",
      "Iteration 11, loss = 1.01334349\n",
      "Iteration 12, loss = 1.01529140\n",
      "Iteration 13, loss = 1.01964711\n",
      "Iteration 14, loss = 1.00890438\n",
      "Iteration 15, loss = 1.00841475\n",
      "Iteration 16, loss = 1.01019510\n",
      "Iteration 17, loss = 1.00884698\n",
      "Iteration 18, loss = 1.00717218\n",
      "Iteration 19, loss = 1.00699545\n",
      "Iteration 20, loss = 1.01084438\n",
      "Iteration 21, loss = 1.00736956\n",
      "Iteration 22, loss = 1.00781082\n",
      "Iteration 23, loss = 1.00762855\n",
      "Iteration 24, loss = 1.00735340\n",
      "Iteration 25, loss = 1.00668570\n",
      "Iteration 26, loss = 1.01288252\n",
      "Iteration 27, loss = 1.00967332\n",
      "Iteration 28, loss = 1.00678046\n",
      "Iteration 29, loss = 1.01068416\n",
      "Iteration 30, loss = 1.00811122\n",
      "Iteration 31, loss = 1.00617302\n",
      "Iteration 32, loss = 1.00636208\n",
      "Iteration 33, loss = 1.00648066\n",
      "Iteration 34, loss = 1.00638259\n",
      "Iteration 35, loss = 1.00584103\n",
      "Iteration 36, loss = 1.00899062\n",
      "Iteration 37, loss = 1.00852970\n",
      "Iteration 38, loss = 1.00809881\n",
      "Iteration 39, loss = 1.00701464\n",
      "Iteration 40, loss = 1.00864480\n",
      "Iteration 41, loss = 1.00501246\n",
      "Iteration 42, loss = 1.00588341\n",
      "Iteration 43, loss = 1.00491070\n",
      "Iteration 44, loss = 1.00783657\n",
      "Iteration 45, loss = 1.00609940\n",
      "Iteration 46, loss = 1.00647323\n",
      "Iteration 47, loss = 1.00778661\n",
      "Iteration 48, loss = 1.00571189\n",
      "Iteration 49, loss = 1.00606509\n",
      "Iteration 50, loss = 1.00631686\n",
      "Iteration 51, loss = 1.00530609\n",
      "Iteration 52, loss = 1.00610460\n",
      "Iteration 53, loss = 1.00601165\n",
      "Iteration 54, loss = 1.00611773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  28.9s\n",
      "Iteration 1, loss = 1.35665611\n",
      "Iteration 2, loss = 1.16407673[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  13.7s\n",
      "Iteration 1, loss = 1.07946175\n",
      "Iteration 2, loss = 1.05364059\n",
      "Iteration 3, loss = 1.03621519\n",
      "Iteration 4, loss = 1.02554908\n",
      "Iteration 5, loss = 1.01926159\n",
      "Iteration 6, loss = 1.01525603\n",
      "Iteration 7, loss = 1.01285078\n",
      "Iteration 8, loss = 1.01129405\n",
      "Iteration 9, loss = 1.01026900\n",
      "Iteration 10, loss = 1.00960441\n",
      "Iteration 11, loss = 1.00908398\n",
      "Iteration 12, loss = 1.00876641\n",
      "Iteration 13, loss = 1.00852192\n",
      "Iteration 14, loss = 1.00831392\n",
      "Iteration 15, loss = 1.00813698\n",
      "Iteration 16, loss = 1.00800654\n",
      "Iteration 17, loss = 1.00791000\n",
      "Iteration 18, loss = 1.00784443\n",
      "Iteration 19, loss = 1.00767005\n",
      "Iteration 20, loss = 1.00757366\n",
      "Iteration 21, loss = 1.00748136\n",
      "Iteration 22, loss = 1.00742993\n",
      "Iteration 23, loss = 1.00731712\n",
      "Iteration 24, loss = 1.00720873\n",
      "Iteration 25, loss = 1.00710891\n",
      "Iteration 26, loss = 1.00703687\n",
      "Iteration 27, loss = 1.00692661\n",
      "Iteration 28, loss = 1.00682433\n",
      "Iteration 29, loss = 1.00674876\n",
      "Iteration 30, loss = 1.00668638\n",
      "Iteration 31, loss = 1.00657239\n",
      "Iteration 32, loss = 1.00647109\n",
      "Iteration 33, loss = 1.00638022\n",
      "Iteration 34, loss = 1.00628160\n",
      "Iteration 35, loss = 1.00617474\n",
      "Iteration 36, loss = 1.00606840\n",
      "Iteration 37, loss = 1.00596808\n",
      "Iteration 38, loss = 1.00586932\n",
      "Iteration 39, loss = 1.00577741\n",
      "Iteration 40, loss = 1.00566453\n",
      "Iteration 41, loss = 1.00557234\n",
      "Iteration 42, loss = 1.00545642\n",
      "Iteration 43, loss = 1.00538824\n",
      "Iteration 44, loss = 1.00524905\n",
      "Iteration 45, loss = 1.00513737\n",
      "Iteration 46, loss = 1.00505361\n",
      "Iteration 47, loss = 1.00491720\n",
      "Iteration 48, loss = 1.00482081\n",
      "Iteration 49, loss = 1.00468423\n",
      "Iteration 50, loss = 1.00456941\n",
      "Iteration 51, loss = 1.00444005\n",
      "Iteration 52, loss = 1.00434244\n",
      "Iteration 53, loss = 1.00421753\n",
      "Iteration 54, loss = 1.00410596\n",
      "Iteration 55, loss = 1.00396383\n",
      "Iteration 56, loss = 1.00385189\n",
      "Iteration 57, loss = 1.00373018\n",
      "Iteration 58, loss = 1.00357996\n",
      "Iteration 59, loss = 1.00347998\n",
      "Iteration 60, loss = 1.00331744\n",
      "Iteration 61, loss = 1.00319569\n",
      "Iteration 62, loss = 1.00305975\n",
      "Iteration 63, loss = 1.00293519\n",
      "Iteration 64, loss = 1.00278950\n",
      "Iteration 65, loss = 1.00266076\n",
      "Iteration 66, loss = 1.00251002\n",
      "Iteration 67, loss = 1.00243073\n",
      "Iteration 68, loss = 1.00224663\n",
      "Iteration 69, loss = 1.00213419\n",
      "Iteration 70, loss = 1.00194871\n",
      "Iteration 71, loss = 1.00180215\n",
      "Iteration 72, loss = 1.00166939\n",
      "Iteration 73, loss = 1.00154488\n",
      "Iteration 74, loss = 1.00136012\n",
      "Iteration 75, loss = 1.00120561\n",
      "Iteration 76, loss = 1.00104871\n",
      "Iteration 77, loss = 1.00090274\n",
      "Iteration 78, loss = 1.00075452\n",
      "Iteration 79, loss = 1.00059674\n",
      "Iteration 80, loss = 1.00040153\n",
      "Iteration 81, loss = 1.00024382\n",
      "Iteration 82, loss = 1.00011317\n",
      "Iteration 83, loss = 0.99991639\n",
      "Iteration 84, loss = 0.99975772\n",
      "Iteration 85, loss = 0.99957055\n",
      "Iteration 86, loss = 0.99937459\n",
      "Iteration 87, loss = 0.99920676\n",
      "Iteration 88, loss = 0.99907328\n",
      "Iteration 89, loss = 0.99893532\n",
      "Iteration 90, loss = 0.99868042\n",
      "Iteration 91, loss = 0.99848452\n",
      "Iteration 92, loss = 0.99828900\n",
      "Iteration 93, loss = 0.99812071\n",
      "Iteration 94, loss = 0.99791172\n",
      "Iteration 95, loss = 0.99775036\n",
      "Iteration 96, loss = 0.99751954\n",
      "Iteration 97, loss = 0.99731279\n",
      "Iteration 98, loss = 0.99715239\n",
      "Iteration 99, loss = 0.99691147\n",
      "Iteration 100, loss = 0.99671061\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time= 1.3min\n",
      "Iteration 1, loss = 1.02666283\n",
      "Iteration 2, loss = 1.00139350\n",
      "Iteration 3, loss = 0.98960970\n",
      "Iteration 4, loss = 0.92492924\n",
      "Iteration 5, loss = 0.83767314\n",
      "Iteration 6, loss = 0.74752552\n",
      "Iteration 7, loss = 0.65659528\n",
      "Iteration 8, loss = 0.58203059\n",
      "Iteration 9, loss = 0.51303340\n",
      "Iteration 10, loss = 0.45794419\n",
      "Iteration 11, loss = 0.41405966\n",
      "Iteration 12, loss = 0.38649318\n",
      "Iteration 13, loss = 0.35989616\n",
      "Iteration 14, loss = 0.33393416\n",
      "Iteration 15, loss = 0.31321801\n",
      "Iteration 16, loss = 0.28131016\n",
      "Iteration 17, loss = 0.26042292\n",
      "Iteration 18, loss = 0.24149787\n",
      "Iteration 19, loss = 0.22899658\n",
      "Iteration 20, loss = 0.21815679\n",
      "Iteration 21, loss = 0.21786721\n",
      "Iteration 22, loss = 0.21238547\n",
      "Iteration 23, loss = 0.21935882\n",
      "Iteration 24, loss = 0.22081401\n",
      "Iteration 25, loss = 0.21664821\n",
      "Iteration 26, loss = 0.21600272\n",
      "Iteration 27, loss = 0.22197738\n",
      "Iteration 28, loss = 0.22650483\n",
      "Iteration 29, loss = 0.23739321\n",
      "Iteration 30, loss = 0.23608458\n",
      "Iteration 31, loss = 0.24423842\n",
      "Iteration 32, loss = 0.24999889\n",
      "Iteration 33, loss = 0.24991988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=adam; total time=  24.8s\n",
      "Iteration 1, loss = 3.93381418\n",
      "Iteration 2, loss = 2.73181674\n",
      "Iteration 3, loss = 1.78879418\n",
      "Iteration 4, loss = 1.30897896\n",
      "Iteration 5, loss = 1.83166040\n",
      "Iteration 6, loss = 3.97154377\n",
      "Iteration 7, loss = 2.80603466\n",
      "Iteration 8, loss = 1.82325749\n",
      "Iteration 9, loss = 2.16310722\n",
      "Iteration 10, loss = 1.83959645\n",
      "Iteration 11, loss = 1.23523582\n",
      "Iteration 12, loss = 1.35372054\n",
      "Iteration 13, loss = 1.34671185\n",
      "Iteration 14, loss = 1.93058832\n",
      "Iteration 15, loss = 1.68553304\n",
      "Iteration 16, loss = 2.50111033\n",
      "Iteration 17, loss = 2.26334840\n",
      "Iteration 18, loss = 1.91787727\n",
      "Iteration 19, loss = 1.91427632\n",
      "Iteration 20, loss = 1.84027335\n",
      "Iteration 21, loss = 3.99477848\n",
      "Iteration 22, loss = 4.35076994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  25.2s\n",
      "Iteration 1, loss = 19.79145792\n",
      "Iteration 2, loss = 1.40593951\n",
      "Iteration 3, loss = 1.03251848\n",
      "Iteration 4, loss = 1.00635550\n",
      "Iteration 5, loss = 1.00366816\n",
      "Iteration 6, loss = 1.00275343\n",
      "Iteration 7, loss = 1.00256068\n",
      "Iteration 8, loss = 1.00254588\n",
      "Iteration 9, loss = 1.00242518\n",
      "Iteration 10, loss = 1.00236429\n",
      "Iteration 11, loss = 1.00239785\n",
      "Iteration 12, loss = 1.00230317\n",
      "Iteration 13, loss = 1.00244206\n",
      "Iteration 14, loss = 1.00227655\n",
      "Iteration 15, loss = 1.00236639\n",
      "Iteration 16, loss = 1.00235013\n",
      "Iteration 17, loss = 1.00229390\n",
      "Iteration 18, loss = 1.00235225\n",
      "Iteration 19, loss = 1.00234009\n",
      "Iteration 20, loss = 1.00234223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time=  11.6s\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  28.3s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=   5.5s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=lbfgs; total time=  26.7s\n",
      "Iteration 1, loss = 1.08352254\n",
      "Iteration 2, loss = 0.35013725\n",
      "Iteration 3, loss = 0.15371721\n",
      "Iteration 4, loss = 0.07963353\n",
      "Iteration 5, loss = 0.04461706\n",
      "Iteration 6, loss = 0.02983303\n",
      "Iteration 7, loss = 0.02540921\n",
      "Iteration 8, loss = 0.02075901\n",
      "Iteration 9, loss = 0.02039412\n",
      "Iteration 10, loss = 0.01621101\n",
      "Iteration 11, loss = 0.01738838\n",
      "Iteration 12, loss = 0.02045310\n",
      "Iteration 13, loss = 0.01758314\n",
      "Iteration 14, loss = 0.03012476\n",
      "Iteration 15, loss = 0.05630550\n",
      "Iteration 16, loss = 0.09077540\n",
      "Iteration 17, loss = 0.20399527\n",
      "Iteration 18, loss = 0.29358063\n",
      "Iteration 19, loss = 0.27597768\n",
      "Iteration 20, loss = 0.19647474\n",
      "Iteration 21, loss = 0.15281469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  13.6s\n",
      "Iteration 1, loss = 103.35560510\n",
      "Iteration 2, loss = 11.84056128\n",
      "Iteration 3, loss = 1.70939675\n",
      "Iteration 4, loss = 1.09612858\n",
      "Iteration 5, loss = 1.03354751\n",
      "Iteration 6, loss = 1.03807966\n",
      "Iteration 7, loss = 1.03237556\n",
      "Iteration 8, loss = 1.03553319\n",
      "Iteration 9, loss = 1.02515690\n",
      "Iteration 10, loss = 1.01550334\n",
      "Iteration 11, loss = 1.03491640\n",
      "Iteration 12, loss = 1.03425536\n",
      "Iteration 13, loss = 1.02037636\n",
      "Iteration 14, loss = 1.02430870\n",
      "Iteration 15, loss = 1.02333628\n",
      "Iteration 16, loss = 1.02216130\n",
      "Iteration 17, loss = 1.00926619\n",
      "Iteration 18, loss = 1.01118938\n",
      "Iteration 19, loss = 1.01133031\n",
      "Iteration 20, loss = 1.00750769\n",
      "Iteration 21, loss = 1.01746634\n",
      "Iteration 22, loss = 1.01523086\n",
      "Iteration 23, loss = 1.01575404\n",
      "Iteration 24, loss = 1.01835297\n",
      "Iteration 25, loss = 1.02674570\n",
      "Iteration 26, loss = 1.02212228\n",
      "Iteration 27, loss = 1.01896224\n",
      "Iteration 28, loss = 1.00572854\n",
      "Iteration 29, loss = 1.01470454\n",
      "Iteration 30, loss = 1.01254505\n",
      "Iteration 31, loss = 1.01609718\n",
      "Iteration 32, loss = 1.03331097\n",
      "Iteration 33, loss = 1.02850156\n",
      "Iteration 34, loss = 1.02019757\n",
      "Iteration 35, loss = 1.01771478\n",
      "Iteration 36, loss = 1.01627014\n",
      "Iteration 37, loss = 1.01334925\n",
      "Iteration 38, loss = 1.01693858\n",
      "Iteration 39, loss = 1.01801372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  42.8s\n",
      "Iteration 1, loss = 321.35477515\n",
      "Iteration 2, loss = 20.10022517\n",
      "Iteration 3, loss = 2.29317182\n",
      "Iteration 4, loss = 1.09462404\n",
      "Iteration 5, loss = 1.01280250\n",
      "Iteration 6, loss = 1.00581796\n",
      "Iteration 7, loss = 1.00809704\n",
      "Iteration 8, loss = 1.00617024\n",
      "Iteration 9, loss = 1.00853960\n",
      "Iteration 10, loss = 1.01177322\n",
      "Iteration 11, loss = 1.01733902\n",
      "Iteration 12, loss = 1.01591484\n",
      "Iteration 13, loss = 1.01118155\n",
      "Iteration 14, loss = 1.01237497\n",
      "Iteration 15, loss = 1.00770394\n",
      "Iteration 16, loss = 1.01462640\n",
      "Iteration 17, loss = 1.01181645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=  22.1s\n",
      "Iteration 1, loss = 5.93931835\n",
      "Iteration 2, loss = 2.89394750\n",
      "Iteration 3, loss = 1.93287391\n",
      "Iteration 4, loss = 1.45820286\n",
      "Iteration 5, loss = 1.22437335\n",
      "Iteration 6, loss = 1.11186582\n",
      "Iteration 7, loss = 1.05928833\n",
      "Iteration 8, loss = 1.03337378\n",
      "Iteration 9, loss = 1.02120759\n",
      "Iteration 10, loss = 1.01568848\n",
      "Iteration 11, loss = 1.01182866\n",
      "Iteration 12, loss = 1.00981132\n",
      "Iteration 13, loss = 1.00919750\n",
      "Iteration 14, loss = 1.00791433\n",
      "Iteration 15, loss = 1.00734050\n",
      "Iteration 16, loss = 1.00759909\n",
      "Iteration 17, loss = 1.00706668\n",
      "Iteration 18, loss = 1.00679758\n",
      "Iteration 19, loss = 1.00660321\n",
      "Iteration 20, loss = 1.00652056\n",
      "Iteration 21, loss = 1.00618652\n",
      "Iteration 22, loss = 1.00630247\n",
      "Iteration 23, loss = 1.00616118\n",
      "Iteration 24, loss = 1.00639817\n",
      "Iteration 25, loss = 1.00545195\n",
      "Iteration 26, loss = 1.00538657\n",
      "Iteration 27, loss = 1.00534278\n",
      "Iteration 28, loss = 1.00514038\n",
      "Iteration 29, loss = 1.00485150\n",
      "Iteration 30, loss = 1.00502363\n",
      "Iteration 31, loss = 1.00495596\n",
      "Iteration 32, loss = 1.00442314\n",
      "Iteration 33, loss = 1.00464298\n",
      "Iteration 34, loss = 1.00421433\n",
      "Iteration 35, loss = 1.00446085\n",
      "Iteration 36, loss = 1.00473090\n",
      "Iteration 37, loss = 1.00422839\n",
      "Iteration 38, loss = 1.00382810\n",
      "Iteration 39, loss = 1.00391046\n",
      "Iteration 40, loss = 1.00433179\n",
      "Iteration 41, loss = 1.00413340\n",
      "Iteration 42, loss = 1.00382590\n",
      "Iteration 43, loss = 1.00357429\n",
      "Iteration 44, loss = 1.00370992\n",
      "Iteration 45, loss = 1.00371448\n",
      "Iteration 46, loss = 1.00359989\n",
      "Iteration 47, loss = 1.00367387\n",
      "Iteration 48, loss = 1.00358137\n",
      "Iteration 49, loss = 1.00368544\n",
      "Iteration 50, loss = 1.00364312\n",
      "Iteration 51, loss = 1.00362886\n",
      "Iteration 52, loss = 1.00344685\n",
      "Iteration 53, loss = 1.00308757\n",
      "Iteration 54, loss = 1.00321916\n",
      "Iteration 55, loss = 1.00335762\n",
      "Iteration 56, loss = 1.00445116\n",
      "Iteration 57, loss = 1.00337896\n",
      "Iteration 58, loss = 1.00302674\n",
      "Iteration 59, loss = 1.00361676\n",
      "Iteration 60, loss = 1.00331435\n",
      "Iteration 61, loss = 1.00303859\n",
      "Iteration 62, loss = 1.00285856\n",
      "Iteration 63, loss = 1.00268534\n",
      "Iteration 64, loss = 1.00395482\n",
      "Iteration 65, loss = 1.00333638\n",
      "Iteration 66, loss = 1.00353202\n",
      "Iteration 67, loss = 1.00379072\n",
      "Iteration 68, loss = 1.00308184\n",
      "Iteration 69, loss = 1.00331942\n",
      "Iteration 70, loss = 1.00290346\n",
      "Iteration 71, loss = 1.00363721\n",
      "Iteration 72, loss = 1.00325270\n",
      "Iteration 73, loss = 1.00302196\n",
      "Iteration 74, loss = 1.00257421\n",
      "Iteration 75, loss = 1.00283797\n",
      "Iteration 76, loss = 1.00304684\n",
      "Iteration 77, loss = 1.00358237\n",
      "Iteration 78, loss = 1.00290935\n",
      "Iteration 79, loss = 1.00379498\n",
      "Iteration 80, loss = 1.00295283\n",
      "Iteration 81, loss = 1.00261420\n",
      "Iteration 82, loss = 1.00280006\n",
      "Iteration 83, loss = 1.00261411\n",
      "Iteration 84, loss = 1.00321787\n",
      "Iteration 85, loss = 1.00264580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 19.99154271\n",
      "Iteration 2, loss = 1.42474167\n",
      "Iteration 3, loss = 1.03917025\n",
      "Iteration 4, loss = 1.00927124\n",
      "Iteration 5, loss = 1.00476101\n",
      "Iteration 6, loss = 1.00353102\n",
      "Iteration 7, loss = 1.00285745\n",
      "Iteration 8, loss = 1.00252646\n",
      "Iteration 9, loss = 1.00235382\n",
      "Iteration 10, loss = 1.00243792\n",
      "Iteration 11, loss = 1.00233361\n",
      "Iteration 12, loss = 1.00242973\n",
      "Iteration 13, loss = 1.00239478\n",
      "Iteration 14, loss = 1.00222340\n",
      "Iteration 15, loss = 1.00221224\n",
      "Iteration 16, loss = 1.00227727\n",
      "Iteration 17, loss = 1.00216863\n",
      "Iteration 18, loss = 1.00227990\n",
      "Iteration 19, loss = 1.00223672\n",
      "Iteration 20, loss = 1.00219481\n",
      "Iteration 21, loss = 1.00232653\n",
      "Iteration 22, loss = 1.00241864\n",
      "Iteration 23, loss = 1.00226103\n",
      "Iteration 24, loss = 1.00225332\n",
      "Iteration 25, loss = 1.00224264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time=  14.0s\n",
      "Iteration 1, loss = 1.09169568\n",
      "Iteration 2, loss = 1.06336684\n",
      "Iteration 3, loss = 1.04375958\n",
      "Iteration 4, loss = 1.03162773\n",
      "Iteration 5, loss = 1.02375652\n",
      "Iteration 6, loss = 1.01877375\n",
      "Iteration 7, loss = 1.01534715\n",
      "Iteration 8, loss = 1.01298436\n",
      "Iteration 9, loss = 1.01130768\n",
      "Iteration 10, loss = 1.01010564\n",
      "Iteration 11, loss = 1.00919403\n",
      "Iteration 12, loss = 1.00851828\n",
      "Iteration 13, loss = 1.00797211\n",
      "Iteration 14, loss = 1.00749821\n",
      "Iteration 15, loss = 1.00713736\n",
      "Iteration 16, loss = 1.00676551\n",
      "Iteration 17, loss = 1.00645825\n",
      "Iteration 18, loss = 1.00615824\n",
      "Iteration 19, loss = 1.00589251\n",
      "Iteration 20, loss = 1.00563237\n",
      "Iteration 21, loss = 1.00533469\n",
      "Iteration 22, loss = 1.00509406\n",
      "Iteration 23, loss = 1.00485140\n",
      "Iteration 24, loss = 1.00457837\n",
      "Iteration 25, loss = 1.00433811\n",
      "Iteration 26, loss = 1.00408333\n",
      "Iteration 27, loss = 1.00383086\n",
      "Iteration 28, loss = 1.00358106\n",
      "Iteration 29, loss = 1.00334711\n",
      "Iteration 30, loss = 1.00312040\n",
      "Iteration 31, loss = 1.00286956\n",
      "Iteration 32, loss = 1.00259854\n",
      "Iteration 33, loss = 1.00236122\n",
      "Iteration 34, loss = 1.00210715\n",
      "Iteration 35, loss = 1.00187391\n",
      "Iteration 36, loss = 1.00165020\n",
      "Iteration 37, loss = 1.00138141\n",
      "Iteration 38, loss = 1.00111689\n",
      "Iteration 39, loss = 1.00086378\n",
      "Iteration 40, loss = 1.00061317\n",
      "Iteration 41, loss = 1.00036051\n",
      "Iteration 42, loss = 1.00012635\n",
      "Iteration 43, loss = 0.99987086\n",
      "Iteration 44, loss = 0.99961529\n",
      "Iteration 45, loss = 0.99940627\n",
      "Iteration 46, loss = 0.99911664\n",
      "Iteration 47, loss = 0.99886946\n",
      "Iteration 48, loss = 0.99861978\n",
      "Iteration 49, loss = 0.99835430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  12.8s\n",
      "Iteration 1, loss = 105.15470095\n",
      "Iteration 2, loss = 14.01171930\n",
      "Iteration 3, loss = 2.72639247\n",
      "Iteration 4, loss = 1.29846753\n",
      "Iteration 5, loss = 1.08667599\n",
      "Iteration 6, loss = 1.03291004\n",
      "Iteration 7, loss = 1.02473901\n",
      "Iteration 8, loss = 1.02556946\n",
      "Iteration 9, loss = 1.04337725\n",
      "Iteration 10, loss = 1.04569356\n",
      "Iteration 11, loss = 1.03460690\n",
      "Iteration 12, loss = 1.03291078\n",
      "Iteration 13, loss = 1.03854902\n",
      "Iteration 14, loss = 1.02657348\n",
      "Iteration 15, loss = 1.02637190\n",
      "Iteration 16, loss = 1.08884425\n",
      "Iteration 17, loss = 1.04077846\n",
      "Iteration 18, loss = 1.02588682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  20.5s\n",
      "Iteration 1, loss = 1.05661911\n",
      "Iteration 2, loss = 1.03959441\n",
      "Iteration 3, loss = 1.02796019\n",
      "Iteration 4, loss = 1.02075823\n",
      "Iteration 5, loss = 1.01626505\n",
      "Iteration 6, loss = 1.01359509\n",
      "Iteration 7, loss = 1.01178352\n",
      "Iteration 8, loss = 1.01065487\n",
      "Iteration 9, loss = 1.00989136\n",
      "Iteration 10, loss = 1.00940397\n",
      "Iteration 11, loss = 1.00908925\n",
      "Iteration 12, loss = 1.00881364\n",
      "Iteration 13, loss = 1.00865506\n",
      "Iteration 14, loss = 1.00855242\n",
      "Iteration 15, loss = 1.00843297\n",
      "Iteration 16, loss = 1.00831087\n",
      "Iteration 17, loss = 1.00824362\n",
      "Iteration 18, loss = 1.00819960\n",
      "Iteration 19, loss = 1.00808711\n",
      "Iteration 20, loss = 1.00803212\n",
      "Iteration 21, loss = 1.00794414\n",
      "Iteration 22, loss = 1.00786898\n",
      "Iteration 23, loss = 1.00781694\n",
      "Iteration 24, loss = 1.00775896\n",
      "Iteration 25, loss = 1.00766799\n",
      "Iteration 26, loss = 1.00758843\n",
      "Iteration 27, loss = 1.00752539\n",
      "Iteration 28, loss = 1.00745244\n",
      "Iteration 29, loss = 1.00736803\n",
      "Iteration 30, loss = 1.00731202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time=  28.8s\n",
      "Iteration 1, loss = 321.97885230\n",
      "Iteration 2, loss = 20.15130433\n",
      "Iteration 3, loss = 2.29789207\n",
      "Iteration 4, loss = 1.09419039\n",
      "Iteration 5, loss = 1.01278997\n",
      "Iteration 6, loss = 1.00689387\n",
      "Iteration 7, loss = 1.00858335\n",
      "Iteration 8, loss = 1.01218760\n",
      "Iteration 9, loss = 1.01336012\n",
      "Iteration 10, loss = 1.01430419\n",
      "Iteration 11, loss = 1.00570806\n",
      "Iteration 12, loss = 1.01226768\n",
      "Iteration 13, loss = 1.01167046\n",
      "Iteration 14, loss = 1.01390135\n",
      "Iteration 15, loss = 1.02315916\n",
      "Iteration 16, loss = 1.06580019\n",
      "Iteration 17, loss = 1.03751501\n",
      "Iteration 18, loss = 1.01917711\n",
      "Iteration 19, loss = 1.01339222\n",
      "Iteration 20, loss = 1.00987886\n",
      "Iteration 21, loss = 1.01214468\n",
      "Iteration 22, loss = 1.01568432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=  24.0s\n",
      "Iteration 1, loss = 1.01585218\n",
      "Iteration 2, loss = 1.00335297\n",
      "Iteration 3, loss = 0.99178994\n",
      "Iteration 4, loss = 0.93458530\n",
      "Iteration 5, loss = 0.83334520\n",
      "Iteration 6, loss = 0.71592207\n",
      "Iteration 7, loss = 0.58884428\n",
      "Iteration 8, loss = 0.46980258\n",
      "Iteration 9, loss = 0.38132288\n",
      "Iteration 10, loss = 0.31837301\n",
      "Iteration 11, loss = 0.28971283\n",
      "Iteration 12, loss = 0.26061263\n",
      "Iteration 13, loss = 0.24579260\n",
      "Iteration 14, loss = 0.24233188\n",
      "Iteration 15, loss = 0.22966850\n",
      "Iteration 16, loss = 0.21506343\n",
      "Iteration 17, loss = 0.19665044\n",
      "Iteration 18, loss = 0.18937433\n",
      "Iteration 19, loss = 0.19082935\n",
      "Iteration 20, loss = 0.19561435\n",
      "Iteration 21, loss = 0.19573021\n",
      "Iteration 22, loss = 0.20242503\n",
      "Iteration 23, loss = 0.21058947\n",
      "Iteration 24, loss = 0.21342042\n",
      "Iteration 25, loss = 0.23620907\n",
      "Iteration 26, loss = 0.23913674\n",
      "Iteration 27, loss = 0.22188105\n",
      "Iteration 28, loss = 0.20025015\n",
      "Iteration 29, loss = 0.18465464\n",
      "Iteration 30, loss = 0.17109393\n",
      "Iteration 31, loss = 0.16604093\n",
      "Iteration 32, loss = 0.15961250\n",
      "Iteration 33, loss = 0.16034455\n",
      "Iteration 34, loss = 0.16135036\n",
      "Iteration 35, loss = 0.17189051\n",
      "Iteration 36, loss = 0.17959176\n",
      "Iteration 37, loss = 0.19234158\n",
      "Iteration 38, loss = 0.21286309\n",
      "Iteration 39, loss = 0.22018047\n",
      "Iteration 40, loss = 0.22897257\n",
      "Iteration 41, loss = 0.22267481\n",
      "Iteration 42, loss = 0.21478948\n",
      "Iteration 43, loss = 0.20978154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=adam; total time=  31.3s\n",
      "Iteration 1, loss = 5.16322306\n",
      "Iteration 2, loss = 1.76349390\n",
      "Iteration 3, loss = 2.49990318\n",
      "Iteration 4, loss = 1.90579552\n",
      "Iteration 5, loss = 1.36743618\n",
      "Iteration 6, loss = 1.42553406\n",
      "Iteration 7, loss = 2.61894009\n",
      "Iteration 8, loss = 1.82412041\n",
      "Iteration 9, loss = 2.20628797\n",
      "Iteration 10, loss = 2.09156829\n",
      "Iteration 11, loss = 1.70036354\n",
      "Iteration 12, loss = 2.66293196\n",
      "Iteration 13, loss = 1.66046630\n",
      "Iteration 14, loss = 1.36775566\n",
      "Iteration 15, loss = 1.97841558\n",
      "Iteration 16, loss = 3.32698178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  18.8s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  13.9s\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=  13.3s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=   9.2s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=   7.4s\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(50,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=   5.1s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=lbfgs; total time=  28.5s\n",
      "Iteration 1, loss = 1.02921234\n",
      "Iteration 2, loss = 0.30606220\n",
      "Iteration 3, loss = 0.14414473\n",
      "Iteration 4, loss = 0.07550288\n",
      "Iteration 5, loss = 0.04933437\n",
      "Iteration 6, loss = 0.03790959\n",
      "Iteration 7, loss = 0.02771779\n",
      "Iteration 8, loss = 0.02814753\n",
      "Iteration 9, loss = 0.03626833\n",
      "Iteration 10, loss = 0.04679147\n",
      "Iteration 11, loss = 0.08167293\n",
      "Iteration 12, loss = 0.12034311\n",
      "Iteration 13, loss = 0.12957404\n",
      "Iteration 14, loss = 0.16972449\n",
      "Iteration 15, loss = 0.15938726\n",
      "Iteration 16, loss = 0.13388837\n",
      "Iteration 17, loss = 0.12403753\n",
      "Iteration 18, loss = 0.09341192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=  23.2s\n",
      "Iteration 1, loss = 19.17925640\n",
      "Iteration 2, loss = 2.38543848\n",
      "Iteration 3, loss = 1.12111124\n",
      "Iteration 4, loss = 1.01447212\n",
      "Iteration 5, loss = 1.00794751\n",
      "Iteration 6, loss = 1.00881201\n",
      "Iteration 7, loss = 1.00448972\n",
      "Iteration 8, loss = 1.00442340\n",
      "Iteration 9, loss = 1.00763434\n",
      "Iteration 10, loss = 1.00646698\n",
      "Iteration 11, loss = 1.00369876\n",
      "Iteration 12, loss = 1.00668049\n",
      "Iteration 13, loss = 1.00651218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08755536\n",
      "Iteration 2, loss = 1.00580331\n",
      "Iteration 3, loss = 1.00137144\n",
      "Iteration 4, loss = 1.00050756\n",
      "Iteration 5, loss = 0.99944495\n",
      "Iteration 6, loss = 0.99854208\n",
      "Iteration 7, loss = 0.99756028\n",
      "Iteration 8, loss = 0.99637276\n",
      "Iteration 9, loss = 0.99520591\n",
      "Iteration 10, loss = 0.99374251\n",
      "Iteration 11, loss = 0.99226553\n",
      "Iteration 12, loss = 0.99082876\n",
      "Iteration 13, loss = 0.98920649\n",
      "Iteration 14, loss = 0.98746905\n",
      "Iteration 15, loss = 0.98575891\n",
      "Iteration 16, loss = 0.98394912\n",
      "Iteration 17, loss = 0.98178039\n",
      "Iteration 18, loss = 0.97948122\n",
      "Iteration 19, loss = 0.97739390\n",
      "Iteration 20, loss = 0.97495949\n",
      "Iteration 21, loss = 0.97203773\n",
      "Iteration 22, loss = 0.96967165\n",
      "Iteration 23, loss = 0.96655043\n",
      "Iteration 24, loss = 0.96349153\n",
      "Iteration 25, loss = 0.96018084\n",
      "Iteration 26, loss = 0.95680952\n",
      "Iteration 27, loss = 0.95322052\n",
      "Iteration 28, loss = 0.94941610\n",
      "Iteration 29, loss = 0.94568971\n",
      "Iteration 30, loss = 0.94146460\n",
      "Iteration 31, loss = 0.93717791\n",
      "Iteration 32, loss = 0.93276651\n",
      "Iteration 33, loss = 0.92790485\n",
      "Iteration 34, loss = 0.92324629\n",
      "Iteration 35, loss = 0.91829152\n",
      "Iteration 36, loss = 0.91305657\n",
      "Iteration 37, loss = 0.90785559\n",
      "Iteration 38, loss = 0.90274270\n",
      "Iteration 39, loss = 0.89694589\n",
      "Iteration 40, loss = 0.89176215\n",
      "Iteration 41, loss = 0.88600092\n",
      "Iteration 42, loss = 0.87977053\n",
      "Iteration 43, loss = 0.87409906\n",
      "Iteration 44, loss = 0.86812999\n",
      "Iteration 45, loss = 0.86214301\n",
      "Iteration 46, loss = 0.85596016\n",
      "Iteration 47, loss = 0.84948638\n",
      "Iteration 48, loss = 0.84415475\n",
      "Iteration 49, loss = 0.83806280\n",
      "Iteration 50, loss = 0.83190108\n",
      "Iteration 51, loss = 0.82514509\n",
      "Iteration 52, loss = 0.81887259\n",
      "Iteration 53, loss = 0.81269873\n",
      "Iteration 54, loss = 0.80635470\n",
      "Iteration 55, loss = 0.80014196\n",
      "Iteration 56, loss = 0.79380603\n",
      "Iteration 57, loss = 0.78799007\n",
      "Iteration 58, loss = 0.78120886\n",
      "Iteration 59, loss = 0.77600343\n",
      "Iteration 60, loss = 0.76933898\n",
      "Iteration 61, loss = 0.76301419\n",
      "Iteration 62, loss = 0.75783166\n",
      "Iteration 63, loss = 0.75093754\n",
      "Iteration 64, loss = 0.74538950\n",
      "Iteration 65, loss = 0.73917967\n",
      "Iteration 66, loss = 0.73227961\n",
      "Iteration 67, loss = 0.72626873\n",
      "Iteration 68, loss = 0.72009776\n",
      "Iteration 69, loss = 0.71454416\n",
      "Iteration 70, loss = 0.70811701\n",
      "Iteration 71, loss = 0.70227348\n",
      "Iteration 72, loss = 0.69681169\n",
      "Iteration 73, loss = 0.68954543\n",
      "Iteration 74, loss = 0.68434200\n",
      "Iteration 75, loss = 0.67862791\n",
      "Iteration 76, loss = 0.67345476\n",
      "Iteration 77, loss = 0.66724717\n",
      "Iteration 78, loss = 0.66075658\n",
      "Iteration 79, loss = 0.65546298\n",
      "Iteration 80, loss = 0.64963290\n",
      "Iteration 81, loss = 0.64378330\n",
      "Iteration 82, loss = 0.63787779\n",
      "Iteration 83, loss = 0.63255182\n",
      "Iteration 84, loss = 0.62660124\n",
      "Iteration 85, loss = 0.62072691\n",
      "Iteration 86, loss = 0.61579027\n",
      "Iteration 87, loss = 0.61094874\n",
      "Iteration 88, loss = 0.60501780\n",
      "Iteration 89, loss = 0.59992664\n",
      "Iteration 90, loss = 0.59378510\n",
      "Iteration 91, loss = 0.58776621\n",
      "Iteration 92, loss = 0.58329705\n",
      "Iteration 93, loss = 0.57695121\n",
      "Iteration 94, loss = 0.57261432\n",
      "Iteration 95, loss = 0.56689345\n",
      "Iteration 96, loss = 0.56111148\n",
      "Iteration 97, loss = 0.55641949\n",
      "Iteration 98, loss = 0.55119678\n",
      "Iteration 99, loss = 0.54585847\n",
      "Iteration 100, loss = 0.54095057\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=sgd; total time= 1.2min\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  27.3s\n",
      "Iteration 1, loss = 328.13609366\n",
      "Iteration 2, loss = 7.69540605\n",
      "Iteration 3, loss = 1.42977369\n",
      "Iteration 4, loss = 1.05643938\n",
      "Iteration 5, loss = 1.03428111\n",
      "Iteration 6, loss = 1.02968313\n",
      "Iteration 7, loss = 1.02593190\n",
      "Iteration 8, loss = 1.02279315\n",
      "Iteration 9, loss = 1.02007503\n",
      "Iteration 10, loss = 1.01776899\n",
      "Iteration 11, loss = 1.01578833\n",
      "Iteration 12, loss = 1.01401145\n",
      "Iteration 13, loss = 1.01252868\n",
      "Iteration 14, loss = 1.01125656\n",
      "Iteration 15, loss = 1.01009680\n",
      "Iteration 16, loss = 1.00911101\n",
      "Iteration 17, loss = 1.00827017\n",
      "Iteration 18, loss = 1.00755286\n",
      "Iteration 19, loss = 1.00687985\n",
      "Iteration 20, loss = 1.00632598\n",
      "Iteration 21, loss = 1.00580574\n",
      "Iteration 22, loss = 1.00537999\n",
      "Iteration 23, loss = 1.00498989\n",
      "Iteration 24, loss = 1.00466117\n",
      "Iteration 25, loss = 1.00435911\n",
      "Iteration 26, loss = 1.00410037\n",
      "Iteration 27, loss = 1.00387028\n",
      "Iteration 28, loss = 1.00367207\n",
      "Iteration 29, loss = 1.00348694\n",
      "Iteration 30, loss = 1.00332905\n",
      "Iteration 31, loss = 1.00320271\n",
      "Iteration 32, loss = 1.00307464\n",
      "Iteration 33, loss = 1.00296243\n",
      "Iteration 34, loss = 1.00286880\n",
      "Iteration 35, loss = 1.00278660\n",
      "Iteration 36, loss = 1.00270735\n",
      "Iteration 37, loss = 1.00263819\n",
      "Iteration 38, loss = 1.00257112\n",
      "Iteration 39, loss = 1.00253523\n",
      "Iteration 40, loss = 1.00247590\n",
      "Iteration 41, loss = 1.00243216\n",
      "Iteration 42, loss = 1.00240459\n",
      "Iteration 43, loss = 1.00235834\n",
      "Iteration 44, loss = 1.00234067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=1000.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time=  34.7s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=lbfgs; total time=  12.3s\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=   9.5s\n",
      "Iteration 1, loss = 1.13153299\n",
      "Iteration 2, loss = 1.08383337\n",
      "Iteration 3, loss = 1.05244985\n",
      "Iteration 4, loss = 1.03448625\n",
      "Iteration 5, loss = 1.02412708\n",
      "Iteration 6, loss = 1.01805825\n",
      "Iteration 7, loss = 1.01434757\n",
      "Iteration 8, loss = 1.01222627\n",
      "Iteration 9, loss = 1.01077780\n",
      "Iteration 10, loss = 1.00983490\n",
      "Iteration 11, loss = 1.00914792\n",
      "Iteration 12, loss = 1.00862152\n",
      "Iteration 13, loss = 1.00824834\n",
      "Iteration 14, loss = 1.00793337\n",
      "Iteration 15, loss = 1.00765687\n",
      "Iteration 16, loss = 1.00742252\n",
      "Iteration 17, loss = 1.00717704\n",
      "Iteration 18, loss = 1.00696638\n",
      "Iteration 19, loss = 1.00674908\n",
      "Iteration 20, loss = 1.00653772\n",
      "Iteration 21, loss = 1.00634347\n",
      "Iteration 22, loss = 1.00614447\n",
      "Iteration 23, loss = 1.00599948\n",
      "Iteration 24, loss = 1.00577131\n",
      "Iteration 25, loss = 1.00558404\n",
      "Iteration 26, loss = 1.00539300\n",
      "Iteration 27, loss = 1.00518654\n",
      "Iteration 28, loss = 1.00497591\n",
      "Iteration 29, loss = 1.00483226\n",
      "Iteration 30, loss = 1.00459178\n",
      "Iteration 31, loss = 1.00443765\n",
      "Iteration 32, loss = 1.00420896\n",
      "Iteration 33, loss = 1.00405588\n",
      "Iteration 34, loss = 1.00383157\n",
      "Iteration 35, loss = 1.00363765\n",
      "Iteration 36, loss = 1.00344246\n",
      "Iteration 37, loss = 1.00325365\n",
      "Iteration 38, loss = 1.00305780\n",
      "Iteration 39, loss = 1.00289975\n",
      "Iteration 40, loss = 1.00268009\n",
      "Iteration 41, loss = 1.00246917\n",
      "Iteration 42, loss = 1.00228864\n",
      "Iteration 43, loss = 1.00209317\n",
      "Iteration 44, loss = 1.00191379\n",
      "Iteration 45, loss = 1.00171577\n",
      "Iteration 46, loss = 1.00149750\n",
      "Iteration 47, loss = 1.00130980\n",
      "Iteration 48, loss = 1.00111306\n",
      "Iteration 49, loss = 1.00091785\n",
      "Iteration 50, loss = 1.00070115\n",
      "Iteration 51, loss = 1.00051048\n",
      "Iteration 52, loss = 1.00030206\n",
      "Iteration 53, loss = 1.00010529\n",
      "Iteration 54, loss = 0.99990311\n",
      "Iteration 55, loss = 0.99969267\n",
      "Iteration 56, loss = 0.99950100\n",
      "Iteration 57, loss = 0.99929798\n",
      "Iteration 58, loss = 0.99910747\n",
      "Iteration 59, loss = 0.99891300\n",
      "Iteration 60, loss = 0.99867994\n",
      "Iteration 61, loss = 0.99848816\n",
      "Iteration 62, loss = 0.99828018\n",
      "Iteration 63, loss = 0.99806790\n",
      "Iteration 64, loss = 0.99789840\n",
      "Iteration 65, loss = 0.99765977\n",
      "Iteration 66, loss = 0.99746201\n",
      "Iteration 67, loss = 0.99726548\n",
      "Iteration 68, loss = 0.99703377\n",
      "Iteration 69, loss = 0.99684082\n",
      "Iteration 70, loss = 0.99662825\n",
      "Iteration 71, loss = 0.99642959\n",
      "Iteration 72, loss = 0.99620126\n",
      "Iteration 73, loss = 0.99597173\n",
      "Iteration 74, loss = 0.99577117\n",
      "Iteration 75, loss = 0.99556670\n",
      "Iteration 76, loss = 0.99534607\n",
      "Iteration 77, loss = 0.99512656\n",
      "Iteration 78, loss = 0.99491242\n",
      "Iteration 79, loss = 0.99468954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 44, loss = 1.00779123\n",
      "Iteration 45, loss = 1.00637239\n",
      "Iteration 46, loss = 1.00728227\n",
      "Iteration 47, loss = 1.00909187\n",
      "Iteration 48, loss = 1.00583962\n",
      "Iteration 49, loss = 1.00624225\n",
      "Iteration 50, loss = 1.00523353\n",
      "Iteration 51, loss = 1.00756898\n",
      "Iteration 52, loss = 1.00799968\n",
      "Iteration 53, loss = 1.00631587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=10.0, mlpclassifier__hidden_layer_sizes=(50, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=sgd; total time=  27.9s\n",
      "Iteration 1, loss = 1.49141160\n",
      "Iteration 2, loss = 1.06745851\n",
      "Iteration 3, loss = 1.08648895\n",
      "Iteration 4, loss = 1.15920405\n",
      "Iteration 5, loss = 1.23630790\n",
      "Iteration 6, loss = 1.25596990\n",
      "Iteration 7, loss = 1.26054826\n",
      "Iteration 8, loss = 1.27477331\n",
      "Iteration 9, loss = 1.24310118\n",
      "Iteration 10, loss = 1.19452975\n",
      "Iteration 11, loss = 1.22483573\n",
      "Iteration 12, loss = 1.22901810\n",
      "Iteration 13, loss = 1.21952768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.1, mlpclassifier__hidden_layer_sizes=(50,), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=   8.3s\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=lbfgs; total time=  28.1s\n",
      "Iteration 1, loss = 1.09328453\n",
      "Iteration 2, loss = 0.31497068\n",
      "Iteration 3, loss = 0.11073210\n",
      "Iteration 4, loss = 0.05661259\n",
      "Iteration 5, loss = 0.03741422\n",
      "Iteration 6, loss = 0.02710782\n",
      "Iteration 7, loss = 0.01852660\n",
      "Iteration 8, loss = 0.01523001\n",
      "Iteration 9, loss = 0.01324368\n",
      "Iteration 10, loss = 0.01170229\n",
      "Iteration 11, loss = 0.01109905\n",
      "Iteration 12, loss = 0.01155773\n",
      "Iteration 13, loss = 0.01488247\n",
      "Iteration 14, loss = 0.01197445\n",
      "Iteration 15, loss = 0.01706931\n",
      "Iteration 16, loss = 0.02398493\n",
      "Iteration 17, loss = 0.05987884\n",
      "Iteration 18, loss = 0.21366858\n",
      "Iteration 19, loss = 0.44712662\n",
      "Iteration 20, loss = 0.49237154\n",
      "Iteration 21, loss = 0.25606412\n",
      "Iteration 22, loss = 0.14636213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=1e-05, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.1, mlpclassifier__solver=adam; total time=  22.8s\n",
      "Iteration 1, loss = 1.02800733\n",
      "Iteration 2, loss = 0.81969969\n",
      "Iteration 3, loss = 0.76637157\n",
      "Iteration 4, loss = 0.73155324\n",
      "Iteration 5, loss = 0.68871473\n",
      "Iteration 6, loss = 0.65573022\n",
      "Iteration 7, loss = 0.61501490\n",
      "Iteration 8, loss = 0.59321526\n",
      "Iteration 9, loss = 0.58068640\n",
      "Iteration 10, loss = 0.55300888\n",
      "Iteration 11, loss = 0.52570248\n",
      "Iteration 12, loss = 0.51041481\n",
      "Iteration 13, loss = 0.48755196\n",
      "Iteration 14, loss = 0.47510515\n",
      "Iteration 15, loss = 0.46449458\n",
      "Iteration 16, loss = 0.44027941\n",
      "Iteration 17, loss = 0.43490083\n",
      "Iteration 18, loss = 0.43171094\n",
      "Iteration 19, loss = 0.42099495\n",
      "Iteration 20, loss = 0.40009597\n",
      "Iteration 21, loss = 0.39950183\n",
      "Iteration 22, loss = 0.39638846\n",
      "Iteration 23, loss = 0.39516787\n",
      "Iteration 24, loss = 0.38958970\n",
      "Iteration 25, loss = 0.38174511\n",
      "Iteration 26, loss = 0.37042086\n",
      "Iteration 27, loss = 0.35335741\n",
      "Iteration 28, loss = 0.34943166\n",
      "Iteration 29, loss = 0.34634530\n",
      "Iteration 30, loss = 0.35085184\n",
      "Iteration 31, loss = 0.35317482\n",
      "Iteration 32, loss = 0.34920198\n",
      "Iteration 33, loss = 0.33730442\n",
      "Iteration 34, loss = 0.33393559\n",
      "Iteration 35, loss = 0.33406710\n",
      "Iteration 36, loss = 0.33002563\n",
      "Iteration 37, loss = 0.32670267\n",
      "Iteration 38, loss = 0.32294088\n",
      "Iteration 39, loss = 0.32227803\n",
      "Iteration 40, loss = 0.32166024\n",
      "Iteration 41, loss = 0.32148222\n",
      "Iteration 42, loss = 0.32521111\n",
      "Iteration 43, loss = 0.32349727\n",
      "Iteration 44, loss = 0.32490957\n",
      "Iteration 45, loss = 0.32168992\n",
      "Iteration 46, loss = 0.31896125\n",
      "Iteration 47, loss = 0.31878513\n",
      "Iteration 48, loss = 0.31199067\n",
      "Iteration 49, loss = 0.30712531\n",
      "Iteration 50, loss = 0.30917928\n",
      "Iteration 51, loss = 0.30466199\n",
      "Iteration 52, loss = 0.30518918\n",
      "Iteration 53, loss = 0.30763551\n",
      "Iteration 54, loss = 0.30699190\n",
      "Iteration 55, loss = 0.30554241\n",
      "Iteration 56, loss = 0.30561472\n",
      "Iteration 57, loss = 0.30901724\n",
      "Iteration 58, loss = 0.30951014\n",
      "Iteration 59, loss = 0.30676693\n",
      "Iteration 60, loss = 0.30682584\n",
      "Iteration 61, loss = 0.30235495\n",
      "Iteration 62, loss = 0.29849981\n",
      "Iteration 63, loss = 0.29819529\n",
      "Iteration 64, loss = 0.29979278\n",
      "Iteration 65, loss = 0.30276355\n",
      "Iteration 66, loss = 0.30456159\n",
      "Iteration 67, loss = 0.29915705\n",
      "Iteration 68, loss = 0.29951577\n",
      "Iteration 69, loss = 0.29457250\n",
      "Iteration 70, loss = 0.28959000\n",
      "Iteration 71, loss = 0.29178656\n",
      "Iteration 72, loss = 0.29120777\n",
      "Iteration 73, loss = 0.28857606\n",
      "Iteration 74, loss = 0.28912407\n",
      "Iteration 75, loss = 0.28997572\n",
      "Iteration 76, loss = 0.29278386\n",
      "Iteration 77, loss = 0.29455667\n",
      "Iteration 78, loss = 0.29042498\n",
      "Iteration 79, loss = 0.28843529\n",
      "Iteration 80, loss = 0.28898900\n",
      "Iteration 81, loss = 0.28690332\n",
      "Iteration 82, loss = 0.28934382\n",
      "Iteration 83, loss = 0.28539661\n",
      "Iteration 84, loss = 0.28559682\n",
      "Iteration 85, loss = 0.28278008\n",
      "Iteration 86, loss = 0.28541015\n",
      "Iteration 87, loss = 0.28233558\n",
      "Iteration 88, loss = 0.28620223\n",
      "Iteration 89, loss = 0.28601004\n",
      "Iteration 90, loss = 0.28483559\n",
      "Iteration 91, loss = 0.28395344\n",
      "Iteration 92, loss = 0.28313445\n",
      "Iteration 93, loss = 0.28351457\n",
      "Iteration 94, loss = 0.28270849\n",
      "Iteration 95, loss = 0.28068731\n",
      "Iteration 96, loss = 0.27941677\n",
      "Iteration 97, loss = 0.28070680\n",
      "Iteration 98, loss = 0.28004201\n",
      "Iteration 99, loss = 0.27969817\n",
      "Iteration 100, loss = 0.28001414\n",
      "[CV] END mlpclassifier__activation=tanh, mlpclassifier__alpha=0.1, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.01, mlpclassifier__solver=adam; total time= 1.8min\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100,), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=lbfgs; total time=  30.5s\n",
      "Iteration 1, loss = 1.01130354\n",
      "Iteration 2, loss = 0.97954160\n",
      "Iteration 3, loss = 0.95290853\n",
      "Iteration 4, loss = 0.91074326\n",
      "Iteration 5, loss = 0.86012107\n",
      "Iteration 6, loss = 0.80703428\n",
      "Iteration 7, loss = 0.75434380\n",
      "Iteration 8, loss = 0.70178838\n",
      "Iteration 9, loss = 0.65433518\n",
      "Iteration 10, loss = 0.60403554\n",
      "Iteration 11, loss = 0.56411866\n",
      "Iteration 12, loss = 0.51834423\n",
      "Iteration 13, loss = 0.47839973\n",
      "Iteration 14, loss = 0.44456331\n",
      "Iteration 15, loss = 0.44139254\n",
      "Iteration 16, loss = 0.39995201\n",
      "Iteration 17, loss = 0.38885499\n",
      "Iteration 18, loss = 0.45616741\n",
      "Iteration 19, loss = 0.31163104\n",
      "Iteration 20, loss = 0.32388624\n",
      "Iteration 21, loss = 0.31357611\n",
      "Iteration 22, loss = 0.26911926\n",
      "Iteration 23, loss = 0.21625886\n",
      "Iteration 24, loss = 0.21177708\n",
      "Iteration 25, loss = 0.18246967\n",
      "Iteration 26, loss = 0.47403869\n",
      "Iteration 27, loss = 0.18703356\n",
      "Iteration 28, loss = 0.15376089\n",
      "Iteration 29, loss = 0.14258567\n",
      "Iteration 30, loss = 0.12419980\n",
      "Iteration 31, loss = 0.12701840\n",
      "Iteration 32, loss = 0.13478104\n",
      "Iteration 33, loss = 0.10392330\n",
      "Iteration 34, loss = 0.09360669\n",
      "Iteration 35, loss = 0.10973835\n",
      "Iteration 36, loss = 0.08316897\n",
      "Iteration 37, loss = 0.07726059\n",
      "Iteration 38, loss = 0.07813285\n",
      "Iteration 39, loss = 0.07149246\n",
      "Iteration 40, loss = 0.06446196\n",
      "Iteration 41, loss = 0.06002046\n",
      "Iteration 42, loss = 0.05846126\n",
      "Iteration 43, loss = 0.05469538\n",
      "Iteration 44, loss = 0.05148517\n",
      "Iteration 45, loss = 0.05004261\n",
      "Iteration 46, loss = 0.04842122\n",
      "Iteration 47, loss = 0.04512614\n",
      "Iteration 48, loss = 0.04309847\n",
      "Iteration 49, loss = 0.04186835\n",
      "Iteration 50, loss = 0.03930831\n",
      "Iteration 51, loss = 0.03868181\n",
      "Iteration 52, loss = 0.03624706\n",
      "Iteration 53, loss = 0.03575694\n",
      "Iteration 54, loss = 0.03394309\n",
      "Iteration 55, loss = 0.03275282\n",
      "Iteration 56, loss = 0.03141686\n",
      "Iteration 57, loss = 0.03067150\n",
      "Iteration 58, loss = 0.02991575\n",
      "Iteration 59, loss = 0.02865160\n",
      "Iteration 60, loss = 0.02770198\n",
      "Iteration 61, loss = 0.02679824\n",
      "Iteration 62, loss = 0.02594546\n",
      "Iteration 63, loss = 0.02559600\n",
      "Iteration 64, loss = 0.02478004\n",
      "Iteration 65, loss = 0.02414220\n",
      "Iteration 66, loss = 0.02378537\n",
      "Iteration 67, loss = 0.02286475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 104.43537480\n",
      "Iteration 2, loss = 13.85031087\n",
      "Iteration 3, loss = 3.21856874\n",
      "Iteration 4, loss = 1.30701410\n",
      "Iteration 5, loss = 1.05515484\n",
      "Iteration 6, loss = 1.03466108\n",
      "Iteration 7, loss = 1.02222294\n",
      "Iteration 8, loss = 1.02050546\n",
      "Iteration 9, loss = 1.03010842\n",
      "Iteration 10, loss = 1.01605241\n",
      "Iteration 11, loss = 1.02000908\n",
      "Iteration 12, loss = 1.01784408\n",
      "Iteration 13, loss = 1.01473794\n",
      "Iteration 14, loss = 1.01927262\n",
      "Iteration 15, loss = 1.01546301\n",
      "Iteration 16, loss = 1.01297706\n",
      "Iteration 17, loss = 1.01326323\n",
      "Iteration 18, loss = 1.01938941\n",
      "Iteration 19, loss = 1.01991854\n",
      "Iteration 20, loss = 1.01161342\n",
      "Iteration 21, loss = 1.01772051\n",
      "Iteration 22, loss = 1.02711169\n",
      "Iteration 23, loss = 1.01658524\n",
      "Iteration 24, loss = 1.02104460\n",
      "Iteration 25, loss = 1.01589977\n",
      "Iteration 26, loss = 1.06161259\n",
      "Iteration 27, loss = 1.04991953\n",
      "Iteration 28, loss = 1.01596111\n",
      "Iteration 29, loss = 1.01684201\n",
      "Iteration 30, loss = 1.01866079\n",
      "Iteration 31, loss = 1.03622982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=1.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  33.7s\n",
      "Iteration 1, loss = 1.05541345\n",
      "Iteration 2, loss = 1.04094800\n",
      "Iteration 3, loss = 1.03067596\n",
      "Iteration 4, loss = 1.02402021\n",
      "Iteration 5, loss = 1.01949033\n",
      "Iteration 6, loss = 1.01635192\n",
      "Iteration 7, loss = 1.01418423\n",
      "Iteration 8, loss = 1.01262450\n",
      "Iteration 9, loss = 1.01149323\n",
      "Iteration 10, loss = 1.01067768\n",
      "Iteration 11, loss = 1.01002476\n",
      "Iteration 12, loss = 1.00954997\n",
      "Iteration 13, loss = 1.00916443\n",
      "Iteration 14, loss = 1.00886633\n",
      "Iteration 15, loss = 1.00862976\n",
      "Iteration 16, loss = 1.00841179\n",
      "Iteration 17, loss = 1.00825841\n",
      "Iteration 18, loss = 1.00811606\n",
      "Iteration 19, loss = 1.00796637\n",
      "Iteration 20, loss = 1.00783983\n",
      "Iteration 21, loss = 1.00771029\n",
      "Iteration 22, loss = 1.00760180\n",
      "Iteration 23, loss = 1.00749785\n",
      "Iteration 24, loss = 1.00741983\n",
      "Iteration 25, loss = 1.00729652\n",
      "Iteration 26, loss = 1.00718647\n",
      "Iteration 27, loss = 1.00709011\n",
      "Iteration 28, loss = 1.00700082\n",
      "Iteration 29, loss = 1.00688786\n",
      "Iteration 30, loss = 1.00679167\n",
      "Iteration 31, loss = 1.00668089\n",
      "Iteration 32, loss = 1.00658915\n",
      "Iteration 33, loss = 1.00648533\n",
      "Iteration 34, loss = 1.00638401\n",
      "Iteration 35, loss = 1.00628698\n",
      "Iteration 36, loss = 1.00618115\n",
      "Iteration 37, loss = 1.00607503\n",
      "Iteration 38, loss = 1.00597019\n",
      "Iteration 39, loss = 1.00587539\n",
      "Iteration 40, loss = 1.00577415\n",
      "Iteration 41, loss = 1.00566804\n",
      "Iteration 42, loss = 1.00557047\n",
      "Iteration 43, loss = 1.00543389\n",
      "Iteration 44, loss = 1.00532977\n",
      "Iteration 45, loss = 1.00522207\n",
      "Iteration 46, loss = 1.00513887\n",
      "Iteration 47, loss = 1.00499209\n",
      "Iteration 48, loss = 1.00487852\n",
      "Iteration 49, loss = 1.00476425\n",
      "Iteration 50, loss = 1.00465255\n",
      "Iteration 51, loss = 1.00453153\n",
      "Iteration 52, loss = 1.00440813\n",
      "Iteration 53, loss = 1.00432195\n",
      "Iteration 54, loss = 1.00415620\n",
      "Iteration 55, loss = 1.00404765\n",
      "Iteration 56, loss = 1.00389449\n",
      "Iteration 57, loss = 1.00378703\n",
      "Iteration 58, loss = 1.00365808\n",
      "Iteration 59, loss = 1.00352657\n",
      "Iteration 60, loss = 1.00343172\n",
      "Iteration 61, loss = 1.00323501\n",
      "Iteration 62, loss = 1.00311010\n",
      "Iteration 63, loss = 1.00297391\n",
      "Iteration 64, loss = 1.00283475\n",
      "Iteration 65, loss = 1.00270221\n",
      "Iteration 66, loss = 1.00254733\n",
      "Iteration 67, loss = 1.00240281\n",
      "Iteration 68, loss = 1.00225886\n",
      "Iteration 69, loss = 1.00211578\n",
      "Iteration 70, loss = 1.00196894\n",
      "Iteration 71, loss = 1.00181831\n",
      "Iteration 72, loss = 1.00166362\n",
      "Iteration 73, loss = 1.00150364\n",
      "Iteration 74, loss = 1.00133519\n",
      "Iteration 75, loss = 1.00118983\n",
      "Iteration 76, loss = 1.00101382\n",
      "Iteration 77, loss = 1.00089538\n",
      "Iteration 78, loss = 1.00070300\n",
      "Iteration 79, loss = 1.00053502\n",
      "Iteration 80, loss = 1.00036654\n",
      "Iteration 81, loss = 1.00021602\n",
      "Iteration 82, loss = 1.00001741\n",
      "Iteration 83, loss = 0.99985084\n",
      "Iteration 84, loss = 0.99968721\n",
      "Iteration 85, loss = 0.99950433\n",
      "Iteration 86, loss = 0.99932367\n",
      "Iteration 87, loss = 0.99916594\n",
      "Iteration 88, loss = 0.99896959\n",
      "Iteration 89, loss = 0.99878554\n",
      "Iteration 90, loss = 0.99859479\n",
      "Iteration 91, loss = 0.99839368\n",
      "Iteration 92, loss = 0.99827367\n",
      "Iteration 93, loss = 0.99800640\n",
      "Iteration 94, loss = 0.99781320\n",
      "Iteration 95, loss = 0.99760919\n",
      "Iteration 96, loss = 0.99741126\n",
      "Iteration 97, loss = 0.99721387\n",
      "Iteration 98, loss = 0.99699805\n",
      "Iteration 99, loss = 0.99678943\n",
      "Iteration 100, loss = 0.99657711\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=0.01, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.001, mlpclassifier__solver=sgd; total time= 1.6min\n",
      "Iteration 1, loss = 5.80311924\n",
      "Iteration 2, loss = 1.63173942\n",
      "Iteration 3, loss = 1.12680769\n",
      "Iteration 4, loss = 1.07328456\n",
      "Iteration 5, loss = 1.08466098\n",
      "Iteration 6, loss = 1.07453154\n",
      "Iteration 7, loss = 1.07320003\n",
      "Iteration 8, loss = 1.04298759\n",
      "Iteration 9, loss = 1.10296204\n",
      "Iteration 10, loss = 1.14410610\n",
      "Iteration 11, loss = 1.69800075\n",
      "Iteration 12, loss = 1.64947268\n",
      "Iteration 13, loss = 1.13112307\n",
      "Iteration 14, loss = 1.08551994\n",
      "Iteration 15, loss = 1.05233871\n",
      "Iteration 16, loss = 1.08478135\n",
      "Iteration 17, loss = 1.07659399\n",
      "Iteration 18, loss = 1.13799893\n",
      "Iteration 19, loss = 1.53428282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END mlpclassifier__activation=logistic, mlpclassifier__alpha=0.0001, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=adam; total time=  21.0s\n",
      "[CV] END mlpclassifier__activation=relu, mlpclassifier__alpha=100.0, mlpclassifier__hidden_layer_sizes=(100, 50), mlpclassifier__learning_rate_init=0.5, mlpclassifier__solver=lbfgs; total time=   6.8s\n",
      "Iteration 1, loss = 1.11040128\n",
      "Iteration 2, loss = 1.07210499\n",
      "Iteration 3, loss = 1.04661395\n",
      "Iteration 4, loss = 1.03192508\n",
      "Iteration 5, loss = 1.02305693\n",
      "Iteration 6, loss = 1.01787765\n",
      "Iteration 7, loss = 1.01451079\n",
      "Iteration 8, loss = 1.01242568\n",
      "Iteration 9, loss = 1.01100189\n",
      "Iteration 10, loss = 1.01004633\n",
      "Iteration 11, loss = 1.00928251\n",
      "Iteration 12, loss = 1.00875373\n",
      "Iteration 13, loss = 1.00831862\n",
      "Iteration 14, loss = 1.00795676\n",
      "Iteration 15, loss = 1.00764414\n",
      "Iteration 16, loss = 1.00730856\n",
      "Iteration 17, loss = 1.00703820\n",
      "Iteration 18, loss = 1.00676856\n",
      "Iteration 19, loss = 1.00649390\n",
      "Iteration 20, loss = 1.00626303\n",
      "Iteration 21, loss = 1.00601768\n",
      "Iteration 22, loss = 1.00576200\n",
      "Iteration 23, loss = 1.00552278\n",
      "Iteration 24, loss = 1.00528206\n",
      "Iteration 25, loss = 1.00507778\n",
      "Iteration 26, loss = 1.00484354\n",
      "Iteration 27, loss = 1.00460649\n",
      "Iteration 28, loss = 1.00434638\n",
      "Iteration 29, loss = 1.00410766\n",
      "Iteration 30, loss = 1.00385710\n",
      "Iteration 31, loss = 1.00362709\n",
      "Iteration 32, loss = 1.00339216\n",
      "Iteration 33, loss = 1.00318055\n",
      "Iteration 34, loss = 1.00290749\n",
      "Iteration 35, loss = 1.00268133\n",
      "Iteration 36, loss = 1.00245387\n",
      "Iteration 37, loss = 1.00218604\n",
      "Iteration 38, loss = 1.00194633\n",
      "Iteration 39, loss = 1.00173949\n",
      "Iteration 40, loss = 1.00150040\n",
      "Iteration 41, loss = 1.00122975\n",
      "Iteration 42, loss = 1.00103926\n",
      "Iteration 43, loss = 1.00078303\n",
      "Iteration 44, loss = 1.00052312\n",
      "Iteration 45, loss = 1.00027273\n",
      "Iteration 46, loss = 1.00002152\n",
      "Iteration 47, loss = 0.99979509\n",
      "Iteration 48, loss = 0.99954354\n",
      "Iteration 49, loss = 0.99928487\n",
      "Iteration 50, loss = 0.99906438\n",
      "Iteration 51, loss = 0.99882051\n",
      "Iteration 52, loss = 0.99858106\n",
      "Iteration 53, loss = 0.99831704\n",
      "Iteration 54, loss = 0.99807824\n",
      "Iteration 55, loss = 0.99781056\n",
      "Iteration 56, loss = 0.99757115\n",
      "Iteration 57, loss = 0.99731890\n",
      "Iteration 58, loss = 0.99708717\n",
      "Iteration 59, loss = 0.99681879\n",
      "Iteration 60, loss = 0.99658455\n",
      "Iteration 61, loss = 0.99633259\n",
      "Iteration 62, loss = 0.99605258\n",
      "Iteration 63, loss = 0.99581382\n",
      "Iteration 64, loss = 0.99555414\n",
      "Iteration 65, loss = 0.99531971\n",
      "Iteration 66, loss = 0.99505247\n",
      "Iteration 67, loss = 0.99477522\n",
      "Iteration 68, loss = 0.99452632\n",
      "Iteration 69, loss = 0.99428369\n",
      "Iteration 70, loss = 0.99401676\n",
      "Iteration 71, loss = 0.99375715\n",
      "Iteration 72, loss = 0.99351764\n",
      "Iteration 73, loss = 0.99322070\n",
      "Iteration 74, loss = 0.99296451\n",
      "Iteration 75, loss = 0.99270172\n",
      "Iteration 76, loss = 0.99246095\n",
      "Iteration 77, loss = 0.99219654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "3 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 754, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 496, in _fit\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.62857176        nan 0.59461497 0.44871815 0.50175644 0.44895232\n",
      " 0.50175644 0.6124123  0.50175644 0.58372352 0.50175644 0.47389974\n",
      " 0.44893377 0.588174   0.50175644 0.44895232 0.50175644 0.60725982\n",
      " 0.50222477 0.50175644 0.60913186 0.50175644 0.59847733 0.59086636\n",
      " 0.56709586 0.5927396  0.50175644 0.59437982 0.50175644 0.59719\n",
      " 0.5919199  0.59613642 0.59824419 0.62412247 0.59590115 0.50175644\n",
      " 0.50175644 0.50175644 0.59250654 0.6096006  0.59800991 0.50175644\n",
      " 0.50175644 0.50175644 0.5695545  0.50175644 0.60725941 0.59437982\n",
      " 0.50175644 0.50175644]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03189580\n",
      "Iteration 2, loss = 1.00122300\n",
      "Iteration 3, loss = 1.00003775\n",
      "Iteration 4, loss = 0.99855953\n",
      "Iteration 5, loss = 0.99718828\n",
      "Iteration 6, loss = 0.99535666\n",
      "Iteration 7, loss = 0.99352166\n",
      "Iteration 8, loss = 0.99157300\n",
      "Iteration 9, loss = 0.98921129\n",
      "Iteration 10, loss = 0.98671516\n",
      "Iteration 11, loss = 0.98395900\n",
      "Iteration 12, loss = 0.98102961\n",
      "Iteration 13, loss = 0.97769900\n",
      "Iteration 14, loss = 0.97419781\n",
      "Iteration 15, loss = 0.97052468\n",
      "Iteration 16, loss = 0.96634637\n",
      "Iteration 17, loss = 0.96165996\n",
      "Iteration 18, loss = 0.95700362\n",
      "Iteration 19, loss = 0.95190090\n",
      "Iteration 20, loss = 0.94682235\n",
      "Iteration 21, loss = 0.94085935\n",
      "Iteration 22, loss = 0.93467344\n",
      "Iteration 23, loss = 0.92827880\n",
      "Iteration 24, loss = 0.92176443\n",
      "Iteration 25, loss = 0.91484039\n",
      "Iteration 26, loss = 0.90763823\n",
      "Iteration 27, loss = 0.90042082\n",
      "Iteration 28, loss = 0.89292594\n",
      "Iteration 29, loss = 0.88591063\n",
      "Iteration 30, loss = 0.87817981\n",
      "Iteration 31, loss = 0.87036455\n",
      "Iteration 32, loss = 0.86281001\n",
      "Iteration 33, loss = 0.85499249\n",
      "Iteration 34, loss = 0.84776002\n",
      "Iteration 35, loss = 0.84049686\n",
      "Iteration 36, loss = 0.83255206\n",
      "Iteration 37, loss = 0.82533539\n",
      "Iteration 38, loss = 0.81767062\n",
      "Iteration 39, loss = 0.81086048\n",
      "Iteration 40, loss = 0.80383226\n",
      "Iteration 41, loss = 0.79619117\n",
      "Iteration 42, loss = 0.78917021\n",
      "Iteration 43, loss = 0.78223734\n",
      "Iteration 44, loss = 0.77481213\n",
      "Iteration 45, loss = 0.76786497\n",
      "Iteration 46, loss = 0.76086117\n",
      "Iteration 47, loss = 0.75383385\n",
      "Iteration 48, loss = 0.74776922\n",
      "Iteration 49, loss = 0.74031332\n",
      "Iteration 50, loss = 0.73384397\n",
      "Iteration 51, loss = 0.72728323\n",
      "Iteration 52, loss = 0.72108849\n",
      "Iteration 53, loss = 0.71452172\n",
      "Iteration 54, loss = 0.70787525\n",
      "Iteration 55, loss = 0.70124187\n",
      "Iteration 56, loss = 0.69523419\n",
      "Iteration 57, loss = 0.68906392\n",
      "Iteration 58, loss = 0.68216327\n",
      "Iteration 59, loss = 0.67601592\n",
      "Iteration 60, loss = 0.67063386\n",
      "Iteration 61, loss = 0.66375558\n",
      "Iteration 62, loss = 0.65787578\n",
      "Iteration 63, loss = 0.65250701\n",
      "Iteration 64, loss = 0.64663251\n",
      "Iteration 65, loss = 0.64022972\n",
      "Iteration 66, loss = 0.63477491\n",
      "Iteration 67, loss = 0.62883704\n",
      "Iteration 68, loss = 0.62318335\n",
      "Iteration 69, loss = 0.61710442\n",
      "Iteration 70, loss = 0.61105824\n",
      "Iteration 71, loss = 0.60653696\n",
      "Iteration 72, loss = 0.60051294\n",
      "Iteration 73, loss = 0.59445460\n",
      "Iteration 74, loss = 0.58843693\n",
      "Iteration 75, loss = 0.58364842\n",
      "Iteration 76, loss = 0.57811526\n",
      "Iteration 77, loss = 0.57263055\n",
      "Iteration 78, loss = 0.56733044\n",
      "Iteration 79, loss = 0.56156563\n",
      "Iteration 80, loss = 0.55623483\n",
      "Iteration 81, loss = 0.55139193\n",
      "Iteration 82, loss = 0.54698819\n",
      "Iteration 83, loss = 0.54100087\n",
      "Iteration 84, loss = 0.53659418\n",
      "Iteration 85, loss = 0.53064124\n",
      "Iteration 86, loss = 0.52614214\n",
      "Iteration 87, loss = 0.52011477\n",
      "Iteration 88, loss = 0.51528770\n",
      "Iteration 89, loss = 0.51118609\n",
      "Iteration 90, loss = 0.50571372\n",
      "Iteration 91, loss = 0.50048481\n",
      "Iteration 92, loss = 0.49588060\n",
      "Iteration 93, loss = 0.49146380\n",
      "Iteration 94, loss = 0.48641562\n",
      "Iteration 95, loss = 0.48167200\n",
      "Iteration 96, loss = 0.47670622\n",
      "Iteration 97, loss = 0.47202727\n",
      "Iteration 98, loss = 0.46764052\n",
      "Iteration 99, loss = 0.46299183\n",
      "Iteration 100, loss = 0.45833532\n",
      "Best hyperparameters found: {'mlpclassifier__solver': 'sgd', 'mlpclassifier__learning_rate_init': 0.01, 'mlpclassifier__hidden_layer_sizes': (100,), 'mlpclassifier__alpha': np.float64(0.0001), 'mlpclassifier__activation': 'relu'}\n",
      "Evaluation accuracy with best model: 0.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a pipeline with TfidfVectorizer and MLPClassifier\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=clean_tweet),\n",
    "    MLPClassifier(max_iter=100, random_state=0, verbose=True)\n",
    ")\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50)],\n",
    "    'mlpclassifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'mlpclassifier__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'mlpclassifier__alpha': np.logspace(-5, 3, 9),  # Exponentially spaced values for alpha (L2 regularization)\n",
    "    'mlpclassifier__learning_rate_init': [0.001, 0.01, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=0, scoring='accuracy'\n",
    ")\n",
    " \n",
    "# Split the dataset into training and evaluation sets\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the RandomizedSearchCV object\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters found:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the evaluation set\n",
    "best_model = random_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_eval)\n",
    "acc = accuracy_score(Y_eval, Y_pred)\n",
    "print(\"Evaluation accuracy with best model: {:.4f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b0d28-2b87-4f9d-820a-4fe8eb40a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
