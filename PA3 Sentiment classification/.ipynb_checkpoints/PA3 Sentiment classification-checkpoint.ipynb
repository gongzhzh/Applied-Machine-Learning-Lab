{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725e508-a50f-4703-bca4-5b3dc6c9469f",
   "metadata": {},
   "source": [
    "PA3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120244-b511-4b33-aff2-b8d35816747a",
   "metadata": {},
   "source": [
    "## 1. Explore the crowdsourced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201243-36ac-4aa9-bf86-e353fe3a80cb",
   "metadata": {},
   "source": [
    "1. Read and observe the data  \n",
    "As we can see, the sentiment annotations are not expected. There are inconsistent label formats such as 'neutral' and 'Neutral' or typo like 'Nutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b76b17c-fdd0-4130-8e2e-5ab127e1f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "neutral                                             2671\n",
      "Neutral                                             2342\n",
      "positive                                            1748\n",
      "Positive                                            1462\n",
      "negative                                            1396\n",
      "Negative                                             973\n",
      "Neutral                                               28\n",
      "netural                                               14\n",
      "Netural                                                6\n",
      "Nuetral                                                3\n",
      "negative                                               3\n",
      "postive                                                2\n",
      " neutral                                               2\n",
      "                neutral                                2\n",
      " negative                                              2\n",
      "positie                                                1\n",
      "negayive                                               1\n",
      "                                        positive       1\n",
      "Nutral                                                 1\n",
      "npositive                                              1\n",
      "               negative                                1\n",
      "neutal                                                 1\n",
      "    positive                                           1\n",
      "postitive                                              1\n",
      "neutra l                                               1\n",
      "Neutrall                                               1\n",
      "negtaive                                               1\n",
      "neutrla                                                1\n",
      "neugral                                                1\n",
      "Neural                                                 1\n",
      "netutral                                               1\n",
      "_x0008_neutral                                         1\n",
      "Nedative                                               1\n",
      "neutral                                                1\n",
      "positive                                               1\n",
      "neutral?                                               1\n",
      "Positve                                                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crowd_train_data = pd.read_csv('crowdsourced_train.csv',sep='\\t')\n",
    "gold_train_data = pd.read_csv('gold_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv('test.csv', sep='\\t')\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02d7ae-0dc5-45f5-ab08-dc7dbaa492d1",
   "metadata": {},
   "source": [
    "3. Clean and re-classify labels  \n",
    "We can create a dictionary to fix these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "18a8d299-c7f4-4e7a-82bd-9a1b1d50b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "neutral           5046\n",
      "positive          3213\n",
      "negative          2375\n",
      "netural             20\n",
      "nuetral              3\n",
      "postive              2\n",
      "postitive            1\n",
      "neutal               1\n",
      "npositive            1\n",
      "neutra l             1\n",
      "positie              1\n",
      "negayive             1\n",
      "nutral               1\n",
      "neugral              1\n",
      "negtaive             1\n",
      "neutrla              1\n",
      "neutrall             1\n",
      "neural               1\n",
      "netutral             1\n",
      "_x0008_neutral       1\n",
      "nedative             1\n",
      "neutral?             1\n",
      "positve              1\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "neutral     5364\n",
      "positive    3652\n",
      "negative    1660\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert labels to lowercase\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].str.strip.str.lower()\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "goldlabel_counts = gold_train_data['sentiment'].value_counts()\n",
    "print(goldlabel_counts)\n",
    "# create a dictionary\n",
    "label_map = {\n",
    "    'positve': 'positive',\n",
    "    'posiive': 'positive',\n",
    "    'negativ': 'negative',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030311-3cfc-4ee8-951f-f7022866f961",
   "metadata": {},
   "source": [
    "4. Measuring agreement  \n",
    "We compared the sentiment labels from the crowdsourced annotator and the gold annotator across 10675 tweets. \n",
    "As we can see, the simple agreement between crowdsourced data and gold data is only 35.63%, and the Cohen's Kappa is only 0.19, indicating a low level of agreement.  \n",
    "This suggests there's either inconsistency or subjective interpretation of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d33bd2d9-9b48-4bcd-b86d-a5c6265c309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.44\n",
      "Accuracy: 65.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a04a8-b711-498f-8045-e1966deef57f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
