{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725e508-a50f-4703-bca4-5b3dc6c9469f",
   "metadata": {},
   "source": [
    "PA3\n",
    "Author: Zhuangzhuang Gong, Richard Hua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cdfe2-8f4d-4675-91f3-50556c334522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Reflection on the annotation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1895300-3d8c-4df0-9748-33ca439dac75",
   "metadata": {},
   "source": [
    "**Challenges**  \n",
    "One of the key challenges is that we found it hard to understand the tweets with the culture references and context outside of the tweet itself.  \n",
    "In this case, we had to make judgement calls,\n",
    "which made us more aware of the subjectivity involved in annotation. Another takeaway is dealing\n",
    "with mixed-sentiment tweets. For example, some tweets consist of both positive and negative emotions, making it hard to label them definitively.\n",
    "\n",
    "**Ideals of improvement**  \n",
    "To improve the process of annotation, refine and expand the guidelines of annotation by providing clearer annotation instructions, especially for\n",
    "mixed-sentiment tweets. This will reduce the subjectivity in the annotation.\n",
    "Another way to improve is to introduce InterAnnotator Agreement by having multiple annotators and comparing their results. This not only improve the quality of annotation but also give the insight how subjective the sentiment label is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120244-b511-4b33-aff2-b8d35816747a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Exploring the crowdsourced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201243-36ac-4aa9-bf86-e353fe3a80cb",
   "metadata": {},
   "source": [
    "**Read and observe the data**  \n",
    "As we can see, the sentiment annotations are not expected. There are many inconsistent label formats, such as 'neutral' and 'Neutral' or typos like 'Nutral'. Therefore, we need to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b76b17c-fdd0-4130-8e2e-5ab127e1f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentiment                                               text\n",
      "0      Positive  There's so much misconception on Islam rn so s...\n",
      "1      Positive  @Mr_Rondeau You should try Iron Maiden at abou...\n",
      "2      Negative  Going to #FantasticFour tomorrow. Half expecti...\n",
      "3       Neutral  @cfelan hey hey, just checkng to see if you or...\n",
      "4      Positive  does anyone just get drunk and watch twilight ...\n",
      "...         ...                                                ...\n",
      "10671  Positive  Glad to hear there may be a bigger more public...\n",
      "10672  Positive  Great stand by the Wolves on 3rd and long. Cur...\n",
      "10673   Neutral  Ayyye I just purchased my Ed Sheeran tickets f...\n",
      "10674  Negative  The anti-semitism, the misogyny, and the suppo...\n",
      "10675  Positive  And yet, I have yet to see the whole series of...\n",
      "\n",
      "[10676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crowd_train_data = pd.read_csv('crowdsourced_train.csv',sep='\\t')\n",
    "gold_train_data = pd.read_csv('gold_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv('test.csv', sep='\\t')\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(crowd_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de0c6-ab70-4f1f-9000-2750f79331dc",
   "metadata": {},
   "source": [
    "**Check the agreement before cleaning the data**  \n",
    "If we check the inner annotation agreement now by comparing the sentiment labels from the crowdsourced annotator and the gold annotator across 10,675 tweets. The overall accuracy is only 35.63%, and Cohen's Kappa is only 0.19, which indicates a low level of agreement.\n",
    "This suggests there's either inconsistency or subjective interpretation of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8592c8d4-f5b8-4107-a862-868c915f0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.19\n",
      "Accuracy: 35.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02d7ae-0dc5-45f5-ab08-dc7dbaa492d1",
   "metadata": {},
   "source": [
    "**Clean and re-classify labels**  \n",
    "We easily fixed some inconsistent label formats by converting all labels to lowercase and stripping the blank spaces.  \n",
    "Furthermore, we found several spelling variations and formatting issues in the label column (e.g., 'nuetral', 'positve', 'nedative', etc.). These were normalised using a mapping dictionary, and all labels were successfully consolidated into the three standard sentiment categories: positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d95691e3-f3e7-4030-8b39-71986cc963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning\n",
      "sentiment\n",
      "neutral           5046\n",
      "positive          3213\n",
      "negative          2375\n",
      "netural             20\n",
      "nuetral              3\n",
      "postive              2\n",
      "postitive            1\n",
      "neutal               1\n",
      "npositive            1\n",
      "neutra l             1\n",
      "positie              1\n",
      "negayive             1\n",
      "nutral               1\n",
      "neugral              1\n",
      "negtaive             1\n",
      "neutrla              1\n",
      "neutrall             1\n",
      "neural               1\n",
      "netutral             1\n",
      "_x0008_neutral       1\n",
      "nedative             1\n",
      "neutral?             1\n",
      "positve              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert labels to lowercase\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].str.strip().str.lower()\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(\"before cleaning\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18a8d299-c7f4-4e7a-82bd-9a1b1d50b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning\n",
      "sentiment\n",
      "neutral     5079\n",
      "positive    3219\n",
      "negative    2378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "label_map = {\n",
    "    # positive \n",
    "    'positve': 'positive',\n",
    "    'postive': 'positive',\n",
    "    'postitive': 'positive',\n",
    "    'positie': 'positive',\n",
    "    'npositive': 'positive',\n",
    "\n",
    "    # neutral \n",
    "    'neutral?': 'neutral',\n",
    "    'nuetral': 'neutral',\n",
    "    '_x0008_neutral': 'neutral',\n",
    "    'netural': 'neutral',\n",
    "    'netutral': 'neutral',\n",
    "    'neural': 'neutral',\n",
    "    'neutrall': 'neutral',\n",
    "    'neugral': 'neutral',\n",
    "    'neutrla': 'neutral',\n",
    "    'nutral': 'neutral',\n",
    "    'neutra l': 'neutral',\n",
    "    'neutal': 'neutral',\n",
    "\n",
    "    # negative \n",
    "    'nedative': 'negative',\n",
    "    'negtaive': 'negative',\n",
    "    'negayive': 'negative',\n",
    "}\n",
    "\n",
    "label_map.update({\n",
    "    'positive': 'positive',\n",
    "    'neutral': 'neutral',\n",
    "    'negative': 'negative'\n",
    "})\n",
    "\n",
    "# apply the dictionary\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].map(label_map)\n",
    "\n",
    "# check the result\n",
    "print(\"After cleaning\")\n",
    "print(crowd_train_data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030311-3cfc-4ee8-951f-f7022866f961",
   "metadata": {},
   "source": [
    "**Agreement checking after cleaning**  \n",
    "We evaluated the agreement between the crowd-sourced labels and gold standard labels again across the data after cleaning the labels. The overall accuracy was 65.49% and the Cohen's Kappa was 0.45, indicating 'average' agreement, according to the standard in Richard Johansson's slides.  \n",
    "This suggests that there's reasonable alignment, while some inconsistency still remains. It's likely due to subjective interpretation of semtimemt tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d33bd2d9-9b48-4bcd-b86d-a5c6265c309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.45\n",
      "Accuracy: 65.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b00965-7608-4243-b732-89d24a55cd5d",
   "metadata": {},
   "source": [
    "**Analysis of the annotation distribution**  \n",
    "We compared the annotation distribution across crowdsourced and gold data. Both showed a similar overall distribution structure, with the majority of labels being 'neutral', followed by 'positive' then 'negative'. However, there's a noticeable difference between 'negative', indicating the sensitivity or interpretation for negative tweets between two groups varies. It suggests that the crowd group is more sensitive to negative sentiment or subjective interpretation of negative tweets, which may contribute to the Cohen's Kappa of 0.45 and the general accuracy of 65.49%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5323cc53-93a5-4876-8f78-97450d5b2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Crowd  Gold  Crowd (%)  Gold (%)\n",
      "neutral    5079  5364      47.57     50.24\n",
      "positive   3219  3652      30.15     34.21\n",
      "negative   2378  1660      22.27     15.55\n"
     ]
    }
   ],
   "source": [
    "# Get label counts from each dataset\n",
    "crowd_counts = crowd_train_data['sentiment'].value_counts()\n",
    "gold_counts = gold_train_data['sentiment'].value_counts()\n",
    "\n",
    "# Access values \n",
    "crowd_counts_dict = {\n",
    "    'neutral': crowd_counts.get('neutral', 0),\n",
    "    'positive': crowd_counts.get('positive', 0),\n",
    "    'negative': crowd_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "gold_counts_dict = {\n",
    "    'neutral': gold_counts.get('neutral', 0),\n",
    "    'positive': gold_counts.get('positive', 0),\n",
    "    'negative': gold_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "# Create Series\n",
    "crowd_series = pd.Series(crowd_counts_dict, name='Crowd')\n",
    "gold_series = pd.Series(gold_counts_dict, name='Gold')\n",
    "\n",
    "# Combine and analyze\n",
    "comparison_df = pd.concat([crowd_series, gold_series], axis=1)\n",
    "\n",
    "# Add percentage comparision\n",
    "total = comparison_df.sum()\n",
    "comparison_df['Crowd (%)'] = (comparison_df['Crowd'] / total['Crowd'] * 100).round(2)\n",
    "comparison_df['Gold (%)'] = (comparison_df['Gold'] / total['Gold'] * 100).round(2)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729ee71-a65d-482b-a00a-d54d9496fbfb",
   "metadata": {},
   "source": [
    "## 3. Implementation of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce01539-3375-43fd-a55d-59e35c6c1da4",
   "metadata": {},
   "source": [
    "**Clean tweet**  \n",
    "We cleaned the data before we train the model to reduce the effect of noisy characters and formats issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "868c513f-f98f-4ff7-8d2a-cfb6f445f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    # 3. Replace user mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # 4. Remove hashtags but keep the tag text\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # 5. Remove RT (retweet marker)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    # 6. Remove emojis or translate them to text\n",
    "    text = emoji.demojize(text)  # \":smile:\" etc\n",
    "    # 7. Remove non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # 8. Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a57466-6085-479a-a17f-ec1d793080a4",
   "metadata": {},
   "source": [
    "**TfidfVectorizer+linearSVC**  \n",
    "We tried a combination of Tfidvectorizer and LinearSVC. The combination of the two can quickly \"feed\" massive text into the model, but also achieve a good balance between accuracy and computational resources, which is one of the most common and validated approaches in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "820aa2f8-0d8e-4e64-92ea-2e5a3b99595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6173062997827661\n",
      "f1_w: 0.6164180770579569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_crowd = crowd_train_data['text'].apply(clean_tweet)\n",
    "Y_crowd = crowd_train_data['sentiment']\n",
    "\n",
    "X_test = test_data['text'].apply(clean_tweet)\n",
    "Y_test = test_data['sentiment']\n",
    "\n",
    "pipeline = Pipeline([(\"tfidf\", TfidfVectorizer(strip_accents='unicode', sublinear_tf=True)),\n",
    "                     (\"svc\", LinearSVC(dual='auto'))])\n",
    "clf = pipeline.fit(X_crowd, Y_crowd)\n",
    "acc = accuracy_score(Y_test, clf.predict(X_test))\n",
    "print(\"acc:\",acc)\n",
    "f1_w = f1_score(Y_test, clf.predict(X_test), average='weighted')\n",
    "print(\"f1_w:\",f1_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacc452",
   "metadata": {},
   "source": [
    "To ensure that the classifier has better performance and is more reliable. We used hyperparameter search and cross-validation to select a set of parameters that work better on unknown data in order to maximise the model's performance.\n",
    "\n",
    "The reason we used hyperparameter search was that it helps improve the predictive performance by finding the optimal settings. Also, it helps balance the model between overfitting and underfitting. On the other hand, cross-validation averages the performance by splitting the data, reducing the variance of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b60f7",
   "metadata": {},
   "source": [
    "Here's a brief about the grid of hyperparameters.  \n",
    "'tfidf__ngram_range' tells the vectorizer to extract all contiguous word-sequences (n-grams) whose length is between (1,1) and (1,2) and see which one performs better.  \n",
    "'tfidf__max_df' tells the vectorizer to ignore certain degrees of super-common terms when establishing a dictionary. We searched three common levels of percentage of degrees to filter the noise but also keep the essential terms.   \n",
    "'tfidf__min_df' tells the vectorizer to ignore certain degrees of rare terms when establishing a dictionary.  We searched three common levels of the amount of rare terms.  \n",
    "'tfidf__max_features' tells the vectorizer the cap vocabulary size.  We tried two different sizes.  \n",
    "'svc__C' refers to the punishing coefficient. The smaller the value, the stronger regularisation it has. We set different levels of regularisation to explore the trade-off between overfitting and underfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c7d853c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters:\n",
      " {'svc__C': 1.0, 'tfidf__max_df': 0.75, 'tfidf__max_features': 10000, 'tfidf__min_df': 10, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV score: 0.5566823740476127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5040    0.4717    0.4873      1077\n",
      "     neutral     0.5975    0.7185    0.6524      2597\n",
      "    positive     0.6734    0.5070    0.5785      1850\n",
      "\n",
      "    accuracy                         0.5996      5524\n",
      "   macro avg     0.5916    0.5657    0.5727      5524\n",
      "weighted avg     0.6047    0.5996    0.5955      5524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# set up a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    # TF–IDF params\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],    # unigrams vs unigrams+bi-grams\n",
    "    'tfidf__max_df':       [0.75, 0.85, 1.0], # ignore super-common terms\n",
    "    'tfidf__min_df':       [3, 5, 10],        # ignore rare terms\n",
    "    'tfidf__max_features':[10_000, 20_000],   # cap vocabulary size\n",
    "\n",
    "    # LinearSVC params\n",
    "    'svc__C':              [0.1, 1.0, 10.0],  # regularization strength\n",
    "}\n",
    "\n",
    "# choose a cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# wrap in GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='f1_weighted',  \n",
    "    n_jobs=-1,              # use all cores\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# fit\n",
    "grid.fit(X_crowd, Y_crowd)\n",
    "print(\"Best parameters:\\n\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "# evaluate on held-out test set\n",
    "y_crowd_pred = grid.predict(X_test)\n",
    "print(classification_report(Y_test, y_crowd_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c90ccb-9817-443f-8605-78d975bcc097",
   "metadata": {},
   "source": [
    "**Result of cross-validation**  \n",
    "There's a slight dropping down on the weighted F1 score. 5-fold CV trains each model on only 80 % of the data per fold and then averages over five splits, so it yields a more conservative estimate than a single run on 100 % of the data.\n",
    "GridSearchCV optimises hyperparameters for average CV performance, which may underperform on the one specific test split.\n",
    "\n",
    "However, in order to pursue better generalisation of the model, we chose the cross-validation optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd3baec4-d57a-4c20-a9bd-a149323db6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test weright F1: 0.5954742459828205\n"
     ]
    }
   ],
   "source": [
    "best_clf = grid.best_estimator_\n",
    "y_pred_test = best_clf.predict(X_test)\n",
    "print(\"Test weright F1:\", f1_score(Y_test, y_pred_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad5a6d-a87e-47b6-9eaf-9268dfe8701a",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "Switching from crowd-sourced to gold-standard labels boosted overall accuracy from 59.96% to 67.87% and weighted F1 from 0.5955 to 0.6746. In particular, positive-class recall jumped by 14.6 points, neutral F1 rose by 6.06 points, and negative precision increased by 17.32 points, showing that the cleaner gold labels yield much clearer decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7c938401-83e6-4de3-917e-71ebb55b3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6772    0.4968    0.5731      1077\n",
      "     neutral     0.6620    0.7724    0.7130      2597\n",
      "    positive     0.7089    0.6530    0.6798      1850\n",
      "\n",
      "    accuracy                         0.6787      5524\n",
      "   macro avg     0.6827    0.6407    0.6553      5524\n",
      "weighted avg     0.6807    0.6787    0.6746      5524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gold data\n",
    "X_gold = gold_train_data['text'].apply(clean_tweet)\n",
    "Y_gold = gold_train_data['sentiment']\n",
    "\n",
    "#split data\n",
    "# X_gold_train, X_gold_eval, Y_gold_train, Y_gold_eval = train_test_split(X_gold, Y_gold, test_size=0.2, random_state=42)\n",
    "\n",
    "grid.best_estimator_.fit(X_gold, Y_gold)\n",
    "\n",
    "# evaluate on held-out test set\n",
    "Y_gold_pred = grid.predict(X_test)\n",
    "print(classification_report(Y_test, Y_gold_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54687",
   "metadata": {},
   "source": [
    "We also tried using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0606d16e-c034-465e-b9e6-83e10439f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7098117306299783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(preprocessor=clean_tweet,), LogisticRegression(\n",
    "    penalty='l2',            # L2 regularization\n",
    "    C=1.0,                   # Regularization strength (default)\n",
    "    solver='liblinear',      # Solver suitable for smaller datasets\n",
    "    max_iter=100,            # Iterations for convergence\n",
    "    class_weight='balanced', # Handle imbalanced classes\n",
    "    fit_intercept=True,      # Include intercept\n",
    "))\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X, Y)\n",
    "acc = accuracy_score(Y_test, clf.predict(X_test))\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c8aca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking models...\n",
      "\n",
      "MultinomialNB            : Mean Accuracy = 0.6132 ± 0.0062\n",
      "LogisticRegression       : Mean Accuracy = 0.6318 ± 0.0062\n",
      "LinearSVC                : Mean Accuracy = 0.6035 ± 0.0104\n",
      "RidgeClassifier          : Mean Accuracy = 0.6272 ± 0.0112\n",
      "Classification Report (RidgeClassifier on hold-out set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.57      0.66      1077\n",
      "     neutral       0.71      0.83      0.76      2597\n",
      "    positive       0.76      0.70      0.73      1850\n",
      "\n",
      "    accuracy                           0.74      5524\n",
      "   macro avg       0.75      0.70      0.72      5524\n",
      "weighted avg       0.74      0.74      0.73      5524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, class_weight='balanced', solver='liblinear'),\n",
    "    \"LinearSVC\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "}\n",
    "\n",
    "# TF-IDF settings\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor=clean_tweet,         # Apply tweet cleaning function\n",
    "    ngram_range=(1, 2),               # Unigrams + bigrams\n",
    "    stop_words='english',             # Use English stop words\n",
    "    max_features=10000,               # Limit to top 10,000 features\n",
    "    sublinear_tf=True,                # Use log scaling for term frequency\n",
    "    min_df=3                          # Ignore terms that appear in fewer than 3 documents\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Benchmarking models...\\n\")\n",
    "for name, model in models.items():\n",
    "    pipeline = make_pipeline(tfidf, model)\n",
    "    scores = cross_val_score(pipeline, X, Y, cv=cv, scoring='accuracy')\n",
    "    print(f\"{name:<25}: Mean Accuracy = {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Final evaluation on hold-out set\n",
    "final_model = make_pipeline(tfidf, RidgeClassifier())\n",
    "final_model.fit(X, Y)\n",
    "preds = final_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (RidgeClassifier on hold-out set):\")\n",
    "print(classification_report(Y_test, preds))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b361009-f79c-43d6-90be-63f8a4fd6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data (adjust this for your dataset)\n",
    "# Assuming crowd_train_data is your dataset with columns 'text' and 'sentiment'\n",
    "X = gold_train_data['text']\n",
    "Y = gold_train_data['sentiment']\n",
    "\n",
    "# Encode the labels (since you have 3 classes)\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X).toarray()\n",
    "X_eval_tfidf = tfidf.transform(X_test).toarray()\n",
    "\n",
    "# MLP Neural Network Model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),  # Two hidden layers with 512 and 256 neurons\n",
    "    activation='relu',              # ReLU activation function\n",
    "    solver='adam',                  # Adam optimizer\n",
    "    max_iter=100,                   # Number of iterations\n",
    "    random_state=0,                 # Random seed for reproducibility\n",
    "    verbose=True                    # Output progress during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred = mlp.predict(X_eval_tfidf)\n",
    "\n",
    "# Accuracy score\n",
    "acc = accuracy_score(Y_eval, Y_pred)\n",
    "print(f\"MLPClassifier Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d19da-668d-47ea-b1de-b5e3db6a777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a pipeline with TfidfVectorizer and MLPClassifier\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=clean_tweet),\n",
    "    MLPClassifier(max_iter=100, random_state=0, verbose=True)\n",
    ")\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50)],\n",
    "    'mlpclassifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'mlpclassifier__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'mlpclassifier__alpha': np.logspace(-5, 3, 9),  # Exponentially spaced values for alpha (L2 regularization)\n",
    "    'mlpclassifier__learning_rate_init': [0.001, 0.01, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=0, scoring='accuracy'\n",
    ")\n",
    " \n",
    "# Split the dataset into training and evaluation sets\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the RandomizedSearchCV object\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters found:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the evaluation set\n",
    "best_model = random_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_eval)\n",
    "acc = accuracy_score(Y_eval, Y_pred)\n",
    "print(\"Evaluation accuracy with best model: {:.4f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b0d28-2b87-4f9d-820a-4fe8eb40a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
