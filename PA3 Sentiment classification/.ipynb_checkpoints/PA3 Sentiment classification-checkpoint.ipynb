{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725e508-a50f-4703-bca4-5b3dc6c9469f",
   "metadata": {},
   "source": [
    "PA3\n",
    "Author: Zhuangzhuang Gong, Richard Hua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cdfe2-8f4d-4675-91f3-50556c334522",
   "metadata": {},
   "source": [
    "## 1. Reflection on the annotation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1895300-3d8c-4df0-9748-33ca439dac75",
   "metadata": {},
   "source": [
    "**Challenges**  \n",
    "One of the key challenges is that we found it hard to understand the tweets with the culture references and context outside of the tweet itself.  \n",
    "In this case, we had to make judgement calls,\n",
    "which made us more aware of the subjectivity involved in annotation. Another takeaway is dealing\n",
    "with mixed-sentiment tweets. For example, some tweets consist of both positive and negative emotions, making it hard to label them definitively.\n",
    "\n",
    "**Ideals of improvement**  \n",
    "To improve the process of annotation, refine and expand the guidelines of annotation by providing clearer annotation instructions, especially for\n",
    "mixed-sentiment tweets. This will reduce the subjectivity in the annotation.\n",
    "Another way to improve is to introduce InterAnnotator Agreement by having multiple annotators and comparing their results. This not only improve the quality of annotation but also give the insight how subjective the sentiment label is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120244-b511-4b33-aff2-b8d35816747a",
   "metadata": {},
   "source": [
    "## 2. Exploring the crowdsourced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201243-36ac-4aa9-bf86-e353fe3a80cb",
   "metadata": {},
   "source": [
    "**Read and observe the data**  \n",
    "As we can see, the sentiment annotations are not expected. There are many inconsistent label formats, such as 'neutral' and 'Neutral' or typos like 'Nutral'. Therefore, we need to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b76b17c-fdd0-4130-8e2e-5ab127e1f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentiment                                               text\n",
      "0      Positive  There's so much misconception on Islam rn so s...\n",
      "1      Positive  @Mr_Rondeau You should try Iron Maiden at abou...\n",
      "2      Negative  Going to #FantasticFour tomorrow. Half expecti...\n",
      "3       Neutral  @cfelan hey hey, just checkng to see if you or...\n",
      "4      Positive  does anyone just get drunk and watch twilight ...\n",
      "...         ...                                                ...\n",
      "10671  Positive  Glad to hear there may be a bigger more public...\n",
      "10672  Positive  Great stand by the Wolves on 3rd and long. Cur...\n",
      "10673   Neutral  Ayyye I just purchased my Ed Sheeran tickets f...\n",
      "10674  Negative  The anti-semitism, the misogyny, and the suppo...\n",
      "10675  Positive  And yet, I have yet to see the whole series of...\n",
      "\n",
      "[10676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crowd_train_data = pd.read_csv('crowdsourced_train.csv',sep='\\t')\n",
    "gold_train_data = pd.read_csv('gold_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv('test.csv', sep='\\t')\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(crowd_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de0c6-ab70-4f1f-9000-2750f79331dc",
   "metadata": {},
   "source": [
    "**Check the agreement before cleaning the data**  \n",
    "If we check the inner annotation agreement now by comparing the sentiment labels from the crowdsourced annotator and the gold annotator across 10,675 tweets. The overall accuracy is only 35.63%, and Cohen's Kappa is only 0.19, which indicates a low level of agreement.\n",
    "This suggests there's either inconsistency or subjective interpretation of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8592c8d4-f5b8-4107-a862-868c915f0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.19\n",
      "Accuracy: 35.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02d7ae-0dc5-45f5-ab08-dc7dbaa492d1",
   "metadata": {},
   "source": [
    "**Clean and re-classify labels**  \n",
    "We easily fixed some inconsistent label formats by converting all labels to lowercase and stripping the blank spaces.  \n",
    "Furthermore, we found several spelling variations and formatting issues in the label column (e.g., 'nuetral', 'positve', 'nedative', etc.). These were normalised using a mapping dictionary, and all labels were successfully consolidated into the three standard sentiment categories: positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95691e3-f3e7-4030-8b39-71986cc963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning\n",
      "sentiment\n",
      "neutral           5046\n",
      "positive          3213\n",
      "negative          2375\n",
      "netural             20\n",
      "nuetral              3\n",
      "postive              2\n",
      "postitive            1\n",
      "neutal               1\n",
      "npositive            1\n",
      "neutra l             1\n",
      "positie              1\n",
      "negayive             1\n",
      "nutral               1\n",
      "neugral              1\n",
      "negtaive             1\n",
      "neutrla              1\n",
      "neutrall             1\n",
      "neural               1\n",
      "netutral             1\n",
      "_x0008_neutral       1\n",
      "nedative             1\n",
      "neutral?             1\n",
      "positve              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert labels to lowercase\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].str.strip().str.lower()\n",
    "label_counts = crowd_train_data['sentiment'].value_counts()\n",
    "print(\"before cleaning\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a8d299-c7f4-4e7a-82bd-9a1b1d50b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning\n",
      "sentiment\n",
      "neutral     5079\n",
      "positive    3219\n",
      "negative    2378\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "label_map = {\n",
    "    # positive \n",
    "    'positve': 'positive',\n",
    "    'postive': 'positive',\n",
    "    'postitive': 'positive',\n",
    "    'positie': 'positive',\n",
    "    'npositive': 'positive',\n",
    "\n",
    "    # neutral \n",
    "    'neutral?': 'neutral',\n",
    "    'nuetral': 'neutral',\n",
    "    '_x0008_neutral': 'neutral',\n",
    "    'netural': 'neutral',\n",
    "    'netutral': 'neutral',\n",
    "    'neural': 'neutral',\n",
    "    'neutrall': 'neutral',\n",
    "    'neugral': 'neutral',\n",
    "    'neutrla': 'neutral',\n",
    "    'nutral': 'neutral',\n",
    "    'neutra l': 'neutral',\n",
    "    'neutal': 'neutral',\n",
    "\n",
    "    # negative \n",
    "    'nedative': 'negative',\n",
    "    'negtaive': 'negative',\n",
    "    'negayive': 'negative',\n",
    "}\n",
    "\n",
    "label_map.update({\n",
    "    'positive': 'positive',\n",
    "    'neutral': 'neutral',\n",
    "    'negative': 'negative'\n",
    "})\n",
    "\n",
    "# apply the dictionary\n",
    "crowd_train_data['sentiment'] = crowd_train_data['sentiment'].map(label_map)\n",
    "\n",
    "# check the result\n",
    "print(\"After cleaning\")\n",
    "print(crowd_train_data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030311-3cfc-4ee8-951f-f7022866f961",
   "metadata": {},
   "source": [
    "**Agreement checking after cleaning**  \n",
    "We evaluated the agreement between the crowd-sourced labels and gold standard labels again across the data after cleaning the labels. The overall accuracy was 65.49% and the Cohen's Kappa was 0.45, indicating 'average' agreement, according to the standard in Richard Johansson's slides.  \n",
    "This suggests that there's reasonable alignment, while some inconsistency still remains. It's likely due to subjective interpretation of semtimemt tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33bd2d9-9b48-4bcd-b86d-a5c6265c309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.45\n",
      "Accuracy: 65.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "kappa = cohen_kappa_score(crowd_train_data['sentiment'], gold_train_data['sentiment'])\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b00965-7608-4243-b732-89d24a55cd5d",
   "metadata": {},
   "source": [
    "**Analysis of the annotation distribution**  \n",
    "We compared the annotation distribution across crowdsourced and gold data. Both showed a similar overall distribution structure, with the majority of labels being 'neutral', followed by 'positive' then 'negative'. However, there's a noticeable difference between 'negative', indicating the sensitivity or interpretation for negative tweets between two groups varies. It suggests that the crowd group is more sensitive to negative sentiment or subjective interpretation of negative tweets, which may contribute to the Cohen's Kappa of 0.45 and the general accuracy of 65.49%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5323cc53-93a5-4876-8f78-97450d5b2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Crowd  Gold  Crowd (%)  Gold (%)\n",
      "neutral    5079  5364      47.57     50.24\n",
      "positive   3219  3652      30.15     34.21\n",
      "negative   2378  1660      22.27     15.55\n"
     ]
    }
   ],
   "source": [
    "# Get label counts from each dataset\n",
    "crowd_counts = crowd_train_data['sentiment'].value_counts()\n",
    "gold_counts = gold_train_data['sentiment'].value_counts()\n",
    "\n",
    "# Access values \n",
    "crowd_counts_dict = {\n",
    "    'neutral': crowd_counts.get('neutral', 0),\n",
    "    'positive': crowd_counts.get('positive', 0),\n",
    "    'negative': crowd_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "gold_counts_dict = {\n",
    "    'neutral': gold_counts.get('neutral', 0),\n",
    "    'positive': gold_counts.get('positive', 0),\n",
    "    'negative': gold_counts.get('negative', 0)\n",
    "}\n",
    "\n",
    "# Create Series\n",
    "crowd_series = pd.Series(crowd_counts_dict, name='Crowd')\n",
    "gold_series = pd.Series(gold_counts_dict, name='Gold')\n",
    "\n",
    "# Combine and analyze\n",
    "comparison_df = pd.concat([crowd_series, gold_series], axis=1)\n",
    "\n",
    "# Add percentage comparision\n",
    "total = comparison_df.sum()\n",
    "comparison_df['Crowd (%)'] = (comparison_df['Crowd'] / total['Crowd'] * 100).round(2)\n",
    "comparison_df['Gold (%)'] = (comparison_df['Gold'] / total['Gold'] * 100).round(2)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729ee71-a65d-482b-a00a-d54d9496fbfb",
   "metadata": {},
   "source": [
    "## 3. Implementation of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce01539-3375-43fd-a55d-59e35c6c1da4",
   "metadata": {},
   "source": [
    "Clean tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868c513f-f98f-4ff7-8d2a-cfb6f445f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    # 3. Replace user mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # 4. Remove hashtags but keep the tag text\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # 5. Remove RT (retweet marker)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    # 6. Remove emojis or translate them to text\n",
    "    text = emoji.demojize(text)  # \":smile:\" etc\n",
    "    # 7. Remove non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # 8. Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a57466-6085-479a-a17f-ec1d793080a4",
   "metadata": {},
   "source": [
    "Pipeline of Clean+TfidfVectorizer+linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "820aa2f8-0d8e-4e64-92ea-2e5a3b99595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6324906367041199\n",
      "acc test: 0.7858435916002896\n"
     ]
    }
   ],
   "source": [
    "# the actual classification algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "# for converting training and test datasets into matrices\n",
    "# TfidfVectorizer does this specifically for documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# for splitting the dataset into training and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = crowd_train_data['text']\n",
    "Y = crowd_train_data['sentiment']\n",
    "\n",
    "X_gold = gold_train_data['text']\n",
    "Y_gold = gold_train_data['sentiment']\n",
    "\n",
    "X_test = test_data['text']\n",
    "Y_test = test_data['sentiment']\n",
    "\n",
    "# TF-IDF settings\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline( TfidfVectorizer(preprocessor=clean_tweet,), LinearSVC(dual='auto'))\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n",
    "clf = train_document_classifier(X_train, Y_train)\n",
    "acc = accuracy_score(Y_eval, clf.predict(X_eval))\n",
    "acc_test = accuracy_score(Y_test, clf.predict(X_test))\n",
    "print(\"acc:\",acc)\n",
    "print(\"acc test:\",acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d7af5-7e99-41ef-9d8c-6731ea5bda3c",
   "metadata": {},
   "source": [
    "We used hyperparameter search and cross-validation to select a set of parameters that work better on unknown data in order to maximise the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c8aca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Hyperparameter tuning and evaluation...\n",
      "\n",
      "âœ¨ Tuning MultinomialNB...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m preds_test = search.best_estimator_.predict(X_test)\n\u001b[32m     78\u001b[39m preds_acc = accuracy_score(Y_test, preds_test)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest: \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpreds_acc\u001b[49m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m best validation score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test set accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Better TF-IDF for tweets\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor=clean_tweet,           # Clean tweets\n",
    "    ngram_range=(1, 3),                  # Unigrams, bigrams, trigrams\n",
    "    stop_words='english',\n",
    "    max_features=15000,                  # Bigger vocabulary\n",
    "    sublinear_tf=True,\n",
    "    min_df=2,                            # Ignore very rare words (at least 2 docs)\n",
    "    max_df=0.9                           # Ignore very common words (>90% docs)\n",
    ")\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', solver='liblinear'),\n",
    "    \"LinearSVC\": LinearSVC(class_weight='balanced', max_iter=1500),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "}\n",
    "\n",
    "# Hyperparameter spaces\n",
    "param_spaces = {\n",
    "    \"MultinomialNB\": {\n",
    "        \"multinomialnb__alpha\": uniform(1e-3, 1)\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"logisticregression__C\": uniform(0.01, 5),\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"],\n",
    "        \"logisticregression__solver\": [\"liblinear\"]\n",
    "    },\n",
    "    \"LinearSVC\": {\n",
    "        \"linearsvc__C\": uniform(0.01, 5),\n",
    "        \"linearsvc__loss\": [\"hinge\", \"squared_hinge\"]\n",
    "    },\n",
    "    \"RidgeClassifier\": {\n",
    "        \"ridgeclassifier__alpha\": uniform(0.01, 10),\n",
    "        \"ridgeclassifier__tol\": uniform(1e-5, 1e-3)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Random search settings\n",
    "n_iter_search = 20  # you can adjust (20-50 is common)\n",
    "best_models = {}\n",
    "\n",
    "print(\"ðŸ” Hyperparameter tuning and evaluation...\\n\")\n",
    "for name, model in models.items():\n",
    "    print(f\"âœ¨ Tuning {name}...\")\n",
    "    pipeline = make_pipeline(tfidf, model)\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_spaces[name],\n",
    "        n_iter=n_iter_search,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "    )\n",
    "    search.fit(X, Y)\n",
    "    best_models[name] = search.best_estimator_\n",
    "\n",
    "    preds = search.best_estimator_.predict(X_eval)\n",
    "    acc = accuracy_score(Y_eval, preds)\n",
    "    preds_test = search.best_estimator_.predict(X_test)\n",
    "    preds_acc = accuracy_score(Y_test, preds_test)\n",
    "    print('test:')\n",
    "    print(preds_acc)\n",
    "    print(f\"âœ… {name} best validation score: {search.best_score_:.4f}\")\n",
    "    print(f\"âœ… {name} test set accuracy: {acc:.4f}\\n\")\n",
    "    print(classification_report(Y_eval, preds))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"ðŸ† Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b361009-f79c-43d6-90be-63f8a4fd6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.95884597\n",
      "Iteration 2, loss = 0.66361254\n",
      "Iteration 3, loss = 0.38862284\n",
      "Iteration 4, loss = 0.18520055\n",
      "Iteration 5, loss = 0.06855323\n",
      "Iteration 6, loss = 0.02281874\n",
      "Iteration 7, loss = 0.01015891\n",
      "Iteration 8, loss = 0.00597680\n",
      "Iteration 9, loss = 0.00387828\n",
      "Iteration 10, loss = 0.00323096\n",
      "Iteration 11, loss = 0.00301844\n",
      "Iteration 12, loss = 0.00220541\n",
      "Iteration 13, loss = 0.00274713\n",
      "Iteration 14, loss = 0.00211303\n",
      "Iteration 15, loss = 0.00208056\n",
      "Iteration 16, loss = 0.00183821\n",
      "Iteration 17, loss = 0.00168452\n",
      "Iteration 18, loss = 0.00182496\n",
      "Iteration 19, loss = 0.00206072\n",
      "Iteration 20, loss = 0.00130422\n",
      "Iteration 21, loss = 0.00212225\n",
      "Iteration 22, loss = 0.00159790\n",
      "Iteration 23, loss = 0.00154578\n",
      "Iteration 24, loss = 0.00150607\n",
      "Iteration 25, loss = 0.00240101\n",
      "Iteration 26, loss = 0.00207064\n",
      "Iteration 27, loss = 0.00164088\n",
      "Iteration 28, loss = 0.00197591\n",
      "Iteration 29, loss = 0.00188707\n",
      "Iteration 30, loss = 0.00135071\n",
      "Iteration 31, loss = 0.00140986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLPClassifier Accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load your data (adjust this for your dataset)\n",
    "# # Assuming crowd_train_data is your dataset with columns 'text' and 'sentiment'\n",
    "# X = gold_train_data['text']\n",
    "# Y = gold_train_data['sentiment']\n",
    "\n",
    "# # Encode the labels (since you have 3 classes)\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "# # Split into train and validation sets\n",
    "# X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y_encoded, test_size=0.2, random_state=0)\n",
    "\n",
    "# # TF-IDF Vectorization\n",
    "# tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "# X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "# X_eval_tfidf = tfidf.transform(X_eval).toarray()\n",
    "\n",
    "# # MLP Neural Network Model\n",
    "# mlp = MLPClassifier(\n",
    "#     hidden_layer_sizes=(512, 256),  # Two hidden layers with 512 and 256 neurons\n",
    "#     activation='relu',              # ReLU activation function\n",
    "#     solver='adam',                  # Adam optimizer\n",
    "#     max_iter=100,                   # Number of iterations\n",
    "#     random_state=0,                 # Random seed for reproducibility\n",
    "#     verbose=True                    # Output progress during training\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# mlp.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# Y_pred = mlp.predict(X_eval_tfidf)\n",
    "\n",
    "# # Accuracy score\n",
    "# acc = accuracy_score(Y_eval, Y_pred)\n",
    "# print(f\"MLPClassifier Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "843d19da-668d-47ea-b1de-b5e3db6a777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:64: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/richardhua/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Fit the RandomizedSearchCV object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest hyperparameters found:\u001b[39m\u001b[33m\"\u001b[39m, random_search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/applied-ml/venv/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a pipeline with TfidfVectorizer and MLPClassifier\n",
    "# pipeline = make_pipeline(\n",
    "#     TfidfVectorizer(preprocessor=clean_tweet),\n",
    "#     MLPClassifier(max_iter=100, random_state=0, verbose=True)\n",
    "# )\n",
    "\n",
    "# # Define the hyperparameter search space\n",
    "# param_dist = {\n",
    "#     'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50)],\n",
    "#     'mlpclassifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "#     'mlpclassifier__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "#     'mlpclassifier__alpha': np.logspace(-5, 3, 9),  # Exponentially spaced values for alpha (L2 regularization)\n",
    "#     'mlpclassifier__learning_rate_init': [0.001, 0.01, 0.1, 0.5],\n",
    "# }\n",
    "\n",
    "# # Set up RandomizedSearchCV with 3-fold cross-validation\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     pipeline, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=0, scoring='accuracy'\n",
    "# )\n",
    " \n",
    "# # Split the dataset into training and evaluation sets\n",
    "# X_train, X_eval, Y_train, Y_eval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# # Fit the RandomizedSearchCV object\n",
    "# random_search.fit(X_train, Y_train)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# print(\"Best hyperparameters found:\", random_search.best_params_)\n",
    "\n",
    "# # Evaluate the best model on the evaluation set\n",
    "# best_model = random_search.best_estimator_\n",
    "# Y_pred = best_model.predict(X_eval)\n",
    "# acc = accuracy_score(Y_eval, Y_pred)\n",
    "# print(\"Evaluation accuracy with best model: {:.4f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b0d28-2b87-4f9d-820a-4fe8eb40a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
