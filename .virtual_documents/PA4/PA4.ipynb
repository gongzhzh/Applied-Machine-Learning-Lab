








from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import Perceptron
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline

X1 = [{'city':'Gothenburg', 'month':'July'},
      {'city':'Gothenburg', 'month':'December'},
      {'city':'Paris', 'month':'July'},
      {'city':'Paris', 'month':'December'}]
Y1 = ['rain', 'rain', 'sun', 'rain']

X2 = [{'city':'Sydney', 'month':'July'},
      {'city':'Sydney', 'month':'December'},
      {'city':'Paris', 'month':'July'},
      {'city':'Paris', 'month':'December'}]
Y2 = ['rain', 'sun', 'sun', 'rain']

classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))
classifier1.fit(X1, Y1)
guesses1 = classifier1.predict(X1)
print(accuracy_score(Y1, guesses1))

classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))
#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())
classifier2.fit(X2, Y2)
guesses2 = classifier2.predict(X2)
print(accuracy_score(Y2, guesses2))





X2_inter = []
for x in X2:
    xi = x.copy()
    xi['city_month'] = f"{x['city']}_{x['month']}"
    X2_inter.append(xi)

print(X2_inter)


clf = make_pipeline(
    DictVectorizer(),
    Perceptron(max_iter=10)
)
clf.fit(X2_inter, Y2)
print("Accuracy with interaction:", accuracy_score(Y2, clf.predict(X2_inter)))





from sklearn.svm import SVC
clf_rbf = make_pipeline(
    DictVectorizer(),
    SVC(kernel='rbf', gamma='scale')   
)
clf_rbf.fit(X2, Y2)
print("RBF SVM accuracy:", accuracy_score(Y2, clf_rbf.predict(X2)))








import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from aml_perceptron import Perceptron, SparsePerceptron

# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 
def read_data(corpus_file):
    X = []
    Y = []
    with open(corpus_file, encoding='utf-8') as f:
        for line in f:
            _, y, _, x = line.split(maxsplit=3)
            X.append(x.strip())
            Y.append(y)
    return X, Y


if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)

    # Set up the preprocessing steps and the classifier.
    pipeline = make_pipeline(
        TfidfVectorizer(),
        SelectKBest(k=1000),
        Normalizer(),

        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron
        Perceptron()  
    )

    # Train the classifier.
    t0 = time.time()
    pipeline.fit(Xtrain, Ytrain)
    t1 = time.time()
    print('Training time: {:.2f} sec.'.format(t1-t0))

    # Evaluate on the test set.
    Yguess = pipeline.predict(Xtest)
    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))










from collections import Counter
import pandas as pd
import numpy as np

X, Y = read_data('data/all_sentiment_shuffled.txt')
print(Counter(Y))
Xs = pd.Series(X)
Ys = pd.Series(Y)
print("Outlier X:", Xs.isnull().sum())
print("Outlier Y:",Ys.isnull().sum())





import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from pegasos import Pegasos
from itertools import product
# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 



if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)
    
    param_space = {
    "n_iter":        [10, 20, 100, 200],
    "lambda_param":  [1e-5, 1e-4, 1e-3],
    }

    results = []
    # Iterate over all combinations of parameters.
    # We use the product function from itertools to create a cartesian
    for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
        # Set up the preprocessing steps and the classifier.
        clf = make_pipeline(
            TfidfVectorizer(),
            SelectKBest(k=1000),
            Normalizer(),
            # NB that this is our Pegasos. See the implementation in the according .py file
            Pegasos(n_iter=n_iter, lambda_param=lam)
        )

        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest)) 

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
    # Sort the results by accuracy.
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))








import time
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from itertools import product
from pegasos import LogisticRegression

# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 
def read_data(corpus_file):
    X = []
    Y = []
    with open(corpus_file, encoding='utf-8') as f:
        for line in f:
            _, y, _, x = line.split(maxsplit=3)
            X.append(x.strip())
            Y.append(y)
    return X, Y


if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)

    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)

    # ---------- parameter grid ----------
    grid_n_iter   = [10, 20, 100, 200]          # epochs
    grid_lambda   = [1e-3, 1e-4, 1e-5]     # regulariser
    results = []

    # ---------- loop ----------
    for n_iter, lam in product(grid_n_iter, grid_lambda):
        clf = make_pipeline(
            TfidfVectorizer(),
            SelectKBest(k=1000),
            Normalizer(),
            LogisticRegression(n_iter=n_iter, lambda_param=lam)
        )

        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest))

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<4} λ={lam:<.0e}  |  time {train_time:5.2f}s  |  acc {acc:.4f}")

    # ---------- summary ----------
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))









import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from pegasos import Pegasos_faster_linear
from itertools import product
# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 



if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)
    
    param_space = {
    "n_iter":        [20, 100, 200],
    "lambda_param":  [ 1e-4],
    }

    results = []
    # Iterate over all combinations of parameters.
    # We use the product function from itertools to create a cartesian
    for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
        # Set up the preprocessing steps and the classifier.
        clf = make_pipeline(
            TfidfVectorizer(),
            SelectKBest(k=1000),
            Normalizer(),
            # NB that this is our Pegasos. See the implementation in the according .py file
            Pegasos_faster_linear(n_iter=n_iter, lambda_param=lam)
        )

        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest)) 

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
    # Sort the results by accuracy.
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))





import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from pegasos import Pegasos
from itertools import product
# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 



if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)
    
    param_space = {
    "n_iter":        [100],
    "lambda_param":  [1e-4],
    }

    results = []
    # Iterate over all combinations of parameters.
    # We use the product function from itertools to create a cartesian
    for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
        # Set up the preprocessing steps and the classifier.
        clf = make_pipeline(
            TfidfVectorizer(),
            #SelectKBest(k=1000),
            Normalizer(),
            # NB that this is our Pegasos. See the implementation in the according .py file
            Pegasos(n_iter=n_iter, lambda_param=lam)
        )
    
        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest)) 

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
    # Sort the results by accuracy.
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))





# import time
# import warnings

# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.preprocessing import Normalizer
# from sklearn.pipeline import make_pipeline
# from sklearn.feature_selection import SelectKBest
# from sklearn.metrics import accuracy_score
# from sklearn.model_selection import train_test_split

# from pegasos import Pegasos
# from itertools import product
# # This function reads the corpus, returns a list of documents, and a list
# # of their corresponding polarity labels. 



# if __name__ == '__main__':
#     warnings.filterwarnings("ignore", category=FutureWarning)
    
#     # Read all the documents.
#     X, Y = read_data('data/all_sentiment_shuffled.txt')
    
#     # Split into training and test parts.
#     Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
#                                                     random_state=0)
    
#     param_space = {
#     "n_iter":        [100],
#     "lambda_param":  [1e-4],
#     }

#     results = []
#     # Iterate over all combinations of parameters.
#     # We use the product function from itertools to create a cartesian
#     for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
#         # Set up the preprocessing steps and the classifier.
#         clf = make_pipeline(
#             TfidfVectorizer(ngram_range=(1,2)),
#             #SelectKBest(k=1000),
#             Normalizer(),
#             # NB that this is our Pegasos. See the implementation in the according .py file
#             Pegasos(n_iter=n_iter, lambda_param=lam)
#         )
    
#         t0 = time.time()
#         clf.fit(Xtrain, Ytrain)
#         train_time = time.time() - t0
#         acc = accuracy_score(Ytest, clf.predict(Xtest)) 

#         results.append(
#             {"n_iter": n_iter,
#             "lambda": lam,
#             "train_time(s)": round(train_time, 2),
#             "test_acc": round(acc, 4)}
#         )
#         print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
#     # Sort the results by accuracy.
#     df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
#     print("\n=== Result Conclusion ===")
#     print(df.to_string(index=False))





import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from pegasos import Pegasos_sparse_vectors
from itertools import product
# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 



if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)
    
    param_space = {
    "n_iter":        [10, 20, 100, 200],
    "lambda_param":  [1e-4],
    }

    results = []
    # Iterate over all combinations of parameters.
    # We use the product function from itertools to create a cartesian
    for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
        # Set up the preprocessing steps and the classifier.
        clf = make_pipeline(
            TfidfVectorizer(),
            SelectKBest(k=1000),
            Normalizer(),
            # NB that this is our Pegasos. See the implementation in the according .py file
            Pegasos_sparse_vectors(n_iter=n_iter, lambda_param=lam)
        )

        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest)) 

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
    # Sort the results by accuracy.
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))





import time
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from pegasos import Pegasos_vec_scale
from itertools import product
# This function reads the corpus, returns a list of documents, and a list
# of their corresponding polarity labels. 



if __name__ == '__main__':
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # Read all the documents.
    X, Y = read_data('data/all_sentiment_shuffled.txt')
    
    # Split into training and test parts.
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,
                                                    random_state=0)
    
    param_space = {
    "n_iter":        [10, 100, 200],
    "lambda_param":  [ 1e-4],
    }

    results = []
    # Iterate over all combinations of parameters.
    # We use the product function from itertools to create a cartesian
    for n_iter, lam in product(param_space["n_iter"], param_space["lambda_param"]):
        # Set up the preprocessing steps and the classifier.
        clf = make_pipeline(
            TfidfVectorizer(),
            SelectKBest(k=1000),
            Normalizer(),
            # NB that this is our Pegasos. See the implementation in the according .py file
            Pegasos_vec_scale(n_iter=n_iter, lambda_param=lam)
        )

        t0 = time.time()
        clf.fit(Xtrain, Ytrain)
        train_time = time.time() - t0
        acc = accuracy_score(Ytest, clf.predict(Xtest)) 

        results.append(
            {"n_iter": n_iter,
            "lambda": lam,
            "train_time(s)": round(train_time, 2),
            "test_acc": round(acc, 4)}
        )
        print(f"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}")
    # Sort the results by accuracy.
    df = pd.DataFrame(results).sort_values("test_acc", ascending=False)
    print("\n=== Result Conclusion ===")
    print(df.to_string(index=False))
