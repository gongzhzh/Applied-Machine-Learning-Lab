{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860a4923",
   "metadata": {},
   "source": [
    "# PA4 Implementing linear classifiers \n",
    "By Group PA4 52 - Zhuangzhuang Gong, Richard Hua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee72eeb-ab9b-4ff8-b7ef-ab7c5fe35e05",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d52816-0bb5-4313-97d0-753106b6e10d",
   "metadata": {},
   "source": [
    "In this predictive system, the \"mystery\" here is that the second dataset encodes an XOR-style pattern, which no strictly linear classifier can perfectly fit. \n",
    "\n",
    "In the first dataset, you can easily draw a single straight-line in the four-dimensional one-hot feature space and separate the \"sun\" from the others. That's why the perceptron can achieve 100% accuracy with the first dataset.\n",
    "\n",
    "However, there is a XOR in the second dataset, which means it's not linearly separable. Since both the perceptron and linearSVC can only learn linear decision boundaries, they cannot solve this XOR pattern.\n",
    "\n",
    "We tried two solutions to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71667483-8f9e-4277-a98f-ba4a147b63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(accuracy_score(Y1, guesses1))\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(accuracy_score(Y2, guesses2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0b19c-2ac0-4374-8358-f94aa85d07d1",
   "metadata": {},
   "source": [
    "**Add an intrerction feature**   \n",
    "We added an interaction feature to introduce a new feature (i.e., city_month) so that \"Sydney_July\" becomes its own one-hot. Thus, a linear model on those will fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d7905f-2ca5-44cc-93a8-0c62affe7b2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'city': 'Sydney', 'month': 'July', 'city_month': 'Sydney_July'}, {'city': 'Sydney', 'month': 'December', 'city_month': 'Sydney_December'}, {'city': 'Paris', 'month': 'July', 'city_month': 'Paris_July'}, {'city': 'Paris', 'month': 'December', 'city_month': 'Paris_December'}]\n"
     ]
    }
   ],
   "source": [
    "X2_inter = []\n",
    "for x in X2:\n",
    "    xi = x.copy()\n",
    "    xi['city_month'] = f\"{x['city']}_{x['month']}\"\n",
    "    X2_inter.append(xi)\n",
    "\n",
    "print(X2_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d82977f-c741-4930-98e7-4c6da5e2ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with interaction: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    Perceptron(max_iter=10)\n",
    ")\n",
    "clf.fit(X2_inter, Y2)\n",
    "print(\"Accuracy with interaction:\", accuracy_score(Y2, clf.predict(X2_inter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715dfb4-2283-42da-ba3b-6a626109f918",
   "metadata": {},
   "source": [
    "**Use a non-linear mode**  \n",
    "Another aspect is to use an RBF kernel to draw a non-linear boundary in the high-dimensional feature space.\n",
    "\n",
    "RBF can project the data into a high-dimensional space; thus, it can be separated by a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48a811a-d4d8-4e5d-b447-177df05c8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_rbf = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    SVC(kernel='rbf', gamma='scale')   \n",
    ")\n",
    "clf_rbf.fit(X2, Y2)\n",
    "print(\"RBF SVM accuracy:\", accuracy_score(Y2, clf_rbf.predict(X2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b96d62-0b31-445f-bae3-06e01819a865",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516406f1-3c54-40dd-8343-e9fe333b76b0",
   "metadata": {},
   "source": [
    "### Experiment  \n",
    "We run the experiment, and the result is around the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68083152-0a3d-41fd-8de8-f9477c1d48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.20 sec.\n",
      "Accuracy: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        Perceptron()  \n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97125de7-f937-4654-9f2d-023297d5dc66",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1004ffa-4290-4dc8-8d3f-0f9f0a36cbfb",
   "metadata": {},
   "source": [
    "We first checked the consistency of the label and the outlier of the dataset. We didn't find any inconsistency or outlier issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1969bc6d-ceab-4dff-8530-bc837a9750ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'pos': 6000, 'neg': 5914})\n",
      "Outlier X: 0\n",
      "Outlier Y: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "print(Counter(Y))\n",
    "Xs = pd.Series(X)\n",
    "Ys = pd.Series(Y)\n",
    "print(\"Outlier X:\", Xs.isnull().sum())\n",
    "print(\"Outlier Y:\",Ys.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a39de7-8e51-497f-ad39-47008c76920e",
   "metadata": {},
   "source": [
    "**Key steps of the implemention of the algorithm**\n",
    "1. Label pre-process. Project the two types of labels into +1 / -1 for uniformly process.\n",
    "2. Weight Initialization. We start a whole-zero vector as the parameter.\n",
    "3. Training Loop.\n",
    "    - Repeat for a number of passes. Randomly shuffle the training indices at the start of each pass.\n",
    "    - For every sample, compute a decaying step size.\n",
    "    - Shrink the weight vector, if the current sample violates the margin, shift it sightly.\n",
    "4. Norm projection. Check the weight vector. Rescale it if it grows beyond a shredshold to ensure the model stable.\n",
    "The sanity check showed that our result is within the expectation.\n",
    "\n",
    "**Quich takeaways**\n",
    "- Sweet spot: λ = 1 × 10⁻⁴ gives the best accuracy across all runs.\n",
    "- Iterations: Raising interation from 10 to 100 gives small gains, but when it comes to 1000, only very slightly accuracy is added to the results while training time multiplies 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18530d62-9ad0-4fd4-b062-4ec11741470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10  λ=1e-05    | time   2.1s | acc 0.8233\n",
      "n_iter=10  λ=0.0001   | time   2.3s | acc 0.8359\n",
      "n_iter=10  λ=0.001    | time   2.3s | acc 0.8254\n",
      "n_iter=20  λ=1e-05    | time   4.7s | acc 0.8355\n",
      "n_iter=20  λ=0.0001   | time   4.2s | acc 0.8292\n",
      "n_iter=20  λ=0.001    | time   4.8s | acc 0.8254\n",
      "n_iter=100 λ=1e-05    | time  16.1s | acc 0.8342\n",
      "n_iter=100 λ=0.0001   | time  13.5s | acc 0.8363\n",
      "n_iter=100 λ=0.001    | time  15.3s | acc 0.8254\n",
      "n_iter=200 λ=1e-05    | time  21.9s | acc 0.8334\n",
      "n_iter=200 λ=0.0001   | time  22.5s | acc 0.8372\n",
      "n_iter=200 λ=0.001    | time  24.3s | acc 0.8267\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    200 0.00010          22.49    0.8372\n",
      "    100 0.00010          13.55    0.8363\n",
      "     10 0.00010           2.33    0.8359\n",
      "     20 0.00001           4.68    0.8355\n",
      "    100 0.00001          16.14    0.8342\n",
      "    200 0.00001          21.94    0.8334\n",
      "     20 0.00010           4.17    0.8292\n",
      "    200 0.00100          24.29    0.8267\n",
      "     10 0.00100           2.26    0.8254\n",
      "     20 0.00100           4.80    0.8254\n",
      "    100 0.00100          15.33    0.8254\n",
      "     10 0.00001           2.14    0.8233\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 20, 100, 200],\n",
    "    \"lambda_param\":  [1e-5, 1e-4, 1e-3],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088073b-a4eb-4bee-9eb3-c96ce6297823",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdedcd9",
   "metadata": {},
   "source": [
    "In thie section, we compared the algprithms, using the log loss instead of the hinge loss.\n",
    "\n",
    "**Key Takeaways**  \n",
    "- Accuracy: both peak around the same level. The accuracy gap is tiny.  \n",
    "- Training time: hinge loss updates only on “violators”, so it reaches its plateau faster. Log loss always updates, so costs ~30 % more wall-time for the same number of passes.  \n",
    "- λ-sensitivity: both deteriorate when λ is too big or too small, but log loss drops harder at λ = 0.001 whereas hinge loss stays ~0.826."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbe01df-327a-4e94-bebe-870590de741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10   λ=1e-03  |  time  3.73s  |  acc 0.8065\n",
      "n_iter=10   λ=1e-04  |  time  2.30s  |  acc 0.8321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\AML\\Applied-Machine-Learning-Lab\\PA4\\pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10   λ=1e-05  |  time  1.99s  |  acc 0.8225\n",
      "n_iter=20   λ=1e-03  |  time  3.13s  |  acc 0.8082\n",
      "n_iter=20   λ=1e-04  |  time  3.51s  |  acc 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\AML\\Applied-Machine-Learning-Lab\\PA4\\pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=20   λ=1e-05  |  time  3.00s  |  acc 0.8284\n",
      "n_iter=100  λ=1e-03  |  time 11.15s  |  acc 0.8074\n",
      "n_iter=100  λ=1e-04  |  time 12.75s  |  acc 0.8376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\AML\\Applied-Machine-Learning-Lab\\PA4\\pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=100  λ=1e-05  |  time 42.78s  |  acc 0.8313\n",
      "n_iter=200  λ=1e-03  |  time 46.08s  |  acc 0.8074\n",
      "n_iter=200  λ=1e-04  |  time 33.86s  |  acc 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code\\AML\\Applied-Machine-Learning-Lab\\PA4\\pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=200  λ=1e-05  |  time 41.57s  |  acc 0.8326\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    200 0.00010          33.86    0.8384\n",
      "    100 0.00010          12.75    0.8376\n",
      "     20 0.00010           3.51    0.8330\n",
      "    200 0.00001          41.57    0.8326\n",
      "     10 0.00010           2.30    0.8321\n",
      "    100 0.00001          42.78    0.8313\n",
      "     20 0.00001           3.00    0.8284\n",
      "     10 0.00001           1.99    0.8225\n",
      "     20 0.00100           3.13    0.8082\n",
      "    100 0.00100          11.15    0.8074\n",
      "    200 0.00100          46.08    0.8074\n",
      "     10 0.00100           3.73    0.8065\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from pegasos import LogisticRegression\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # ---------- parameter grid ----------\n",
    "    grid_n_iter   = [10, 20, 100, 200]          # epochs\n",
    "    grid_lambda   = [1e-3, 1e-4, 1e-5]     # regulariser\n",
    "    results = []\n",
    "\n",
    "    # ---------- loop ----------\n",
    "    for n_iter, lam in product(grid_n_iter, grid_lambda):\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            LogisticRegression(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest))\n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<4} λ={lam:<.0e}  |  time {train_time:5.2f}s  |  acc {acc:.4f}\")\n",
    "\n",
    "    # ---------- summary ----------\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62e089",
   "metadata": {},
   "source": [
    "## Bonus task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed70d9",
   "metadata": {},
   "source": [
    "### A. Faster linear algebra operations\n",
    "We used  ddot(x, y) to repalce x.dot(y) adn dscal to replace '*=' in our original implementation of the pegasos algorithm.\n",
    "\n",
    "We evaluated three parameter settings using iteration counts of 10, 100, and 200 to demonstrate the optimized algorithm’s performance under different conditions comparing to the original algorithm.\n",
    "\n",
    "For time consumption aspect, performance under all three parameter settings' has been improved.\n",
    "Training time sees substantial savings at higher iteraton counts. Noticeably, the time almost reduced by 46% at 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943d8ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=20  λ=0.0001   | time   2.9s | acc 0.8342\n",
      "n_iter=100 λ=0.0001   | time  11.6s | acc 0.8389\n",
      "n_iter=200 λ=0.0001   | time  19.2s | acc 0.8401\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    200  0.0001          19.17    0.8401\n",
      "    100  0.0001          11.62    0.8389\n",
      "     20  0.0001           2.92    0.8342\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos_faster_linear\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [20, 100, 200],\n",
    "    \"lambda_param\":  [ 1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos_faster_linear(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa676fa5",
   "metadata": {},
   "source": [
    "### B. Using sparse vectors\n",
    "\n",
    "**Remove the SelectKBest step**  \n",
    "\n",
    "We removed the SelectKBest from the steps to better explore the optimization. It turns out that after removing the SelectKBest step, the time comsumption increased almost 20 times. \n",
    "\n",
    "One reason behind this is that the SelectKbest can reduce the dimensionality which can shrink the scale of the computing. Another reason is that this function can filter the low weight words, saving the cost of total computing and also reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff756f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=100 λ=0.0001   | time 287.9s | acc 0.8418\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    100  0.0001         287.94    0.8418\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [100],\n",
    "    \"lambda_param\":  [1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            #SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "    \n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f73d8",
   "metadata": {},
   "source": [
    " **Include Two-word Sequences**  \n",
    "\n",
    " We then tried adding the option ngram_range=(1,2) to the TfidfVectorizer. \n",
    " \n",
    " It includes the bigram as a feature as well, which richers the feature set but also increase the dimensionality, meaning more memory and slower operations.\n",
    "\n",
    " This exploration needs too much time to run, thus we don't exhibit our result here. However, through the expriment, we understand the trade off between counts of feature and effeciecncy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25eaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from pegasos import Pegasos\n",
    "# from itertools import product\n",
    "# # This function reads the corpus, returns a list of documents, and a list\n",
    "# # of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "#     # Read all the documents.\n",
    "#     X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "#     # Split into training and test parts.\n",
    "#     Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "#                                                     random_state=0)\n",
    "    \n",
    "#     param_space = {\n",
    "#     \"n_iter\":        [100],\n",
    "#     \"lambda_param\":  [1e-4],\n",
    "#     }\n",
    "\n",
    "#     results = []\n",
    "#     # Iterate over all combinations of parameters.\n",
    "#     # We use the product function from itertools to create a cartesian\n",
    "#     for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "#         # Set up the preprocessing steps and the classifier.\n",
    "#         clf = make_pipeline(\n",
    "#             TfidfVectorizer(ngram_range=(1,2)),\n",
    "#             #SelectKBest(k=1000),\n",
    "#             Normalizer(),\n",
    "#             # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "#             Pegasos(n_iter=n_iter, lambda_param=lam)\n",
    "#         )\n",
    "    \n",
    "#         t0 = time.time()\n",
    "#         clf.fit(Xtrain, Ytrain)\n",
    "#         train_time = time.time() - t0\n",
    "#         acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "#         results.append(\n",
    "#             {\"n_iter\": n_iter,\n",
    "#             \"lambda\": lam,\n",
    "#             \"train_time(s)\": round(train_time, 2),\n",
    "#             \"test_acc\": round(acc, 4)}\n",
    "#         )\n",
    "#         print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "#     # Sort the results by accuracy.\n",
    "#     df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "#     print(\"\\n=== Result Conclusion ===\")\n",
    "#     print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947eaee1",
   "metadata": {},
   "source": [
    "**Using sparse vectors**  \n",
    "By switching to sparse vector representations, we kept the same hyperparameters (e.g. n_iter, λ) and achieved nearly identical test accuracy (≈83.9%) while greatly reducing memory pressure. \n",
    "\n",
    "However, because weight decay and norm projection still run over the full feature dimension (O(d)), training time actually rose when d reverted to its original size. This shows that sparse updates alone relieve memory constraints but that additional feature reduction or mini-batch updates are required to maintain fast training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb798d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10  λ=0.0001   | time  90.8s | acc 0.8410\n",
      "n_iter=20  λ=0.0001   | time  34.2s | acc 0.8376\n",
      "n_iter=100 λ=0.0001   | time 141.7s | acc 0.8359\n",
      "n_iter=200 λ=0.0001   | time 230.3s | acc 0.8363\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "     10  0.0001          90.83    0.8410\n",
      "     20  0.0001          34.23    0.8376\n",
      "    200  0.0001         230.31    0.8363\n",
      "    100  0.0001         141.73    0.8359\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos_sparse_vectors\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 20, 100, 200],\n",
    "    \"lambda_param\":  [1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos_sparse_vectors(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79035399",
   "metadata": {},
   "source": [
    "### C. Speeding up the scaling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9541a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10  λ=0.0001   | time   1.4s | acc 0.8338\n",
      "n_iter=100 λ=0.0001   | time   5.9s | acc 0.8368\n",
      "n_iter=200 λ=0.0001   | time  10.0s | acc 0.8368\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    100  0.0001           5.86    0.8368\n",
      "    200  0.0001           9.98    0.8368\n",
      "     10  0.0001           1.41    0.8338\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos_vec_scale\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 100, 200],\n",
    "    \"lambda_param\":  [ 1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos_vec_scale(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99efb2-a608-4a52-abee-83ff15af6201",
   "metadata": {},
   "source": [
    "We sped up the scaling operations by extending our solution from part a) of the bonus task. We minimized the number of vector scaling operations by defining a constant alpha which is scaled at each iteration instead of the vector. This makes a difference when the vector is especially long. An issue with this approach is that the constant, alpha, tends to get very, very small. In our testing, this led to arithmetic errors as alpha approached zero. To remedy this, we decided to fix a minimum value for alpha at 10^-8. This was accomplished by always taking the maximum of either alpha or 10^-8 at each iteration. After some further testing, we found that fixing a minimum value for alpah did not significantly impact the accuracy. The accuracy scores were comparable to the scores from a). In our results we can see that the run times have overall decreased around 20% from part a). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
