{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860a4923",
   "metadata": {},
   "source": [
    "# PA4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee72eeb-ab9b-4ff8-b7ef-ab7c5fe35e05",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d52816-0bb5-4313-97d0-753106b6e10d",
   "metadata": {},
   "source": [
    "In this predictive system, the \"mystery\" here is that the second dataset encodes an XOR-style pattern, which no strictly linear classifier can perfectly fit. \n",
    "\n",
    "In the first dataset, you can easily draw a single straight-line in the four-dimensional one-hot feature space and separate the \"sun\" from the others. That's why the perceptron can achieve 100% accuracy with the first dataset.\n",
    "\n",
    "However, there is a XOR in the second dataset, which means it's not linearly separable. Since both the perceptron and linearSVC can only learn linear decision boundaries, they cannot solve this XOR pattern.\n",
    "\n",
    "We tried two solutions to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71667483-8f9e-4277-a98f-ba4a147b63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(accuracy_score(Y1, guesses1))\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(accuracy_score(Y2, guesses2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdcb3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'D:/code/AML/Applied-Machine-Learning-Lab/PA4'\n",
      "/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd D:/code/AML/Applied-Machine-Learning-Lab/PA4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f75e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD       : /Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4\n",
      "sys.path0 : /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python39.zip\n",
      "aml_spec  : ModuleSpec(name='aml_perceptron', loader=<_frozen_importlib_external.SourceFileLoader object at 0x103195bb0>, origin='/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4/aml_perceptron.py')\n"
     ]
    }
   ],
   "source": [
    "import os, sys, importlib.util\n",
    "print(\"CWD       :\", os.getcwd())           # 当前工作目录\n",
    "print(\"sys.path0 :\", sys.path[0])          # Python 查找模块的首级路径\n",
    "print(\"aml_spec  :\", importlib.util.find_spec(\"aml_perceptron\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0b19c-2ac0-4374-8358-f94aa85d07d1",
   "metadata": {},
   "source": [
    "**Add an intrerction feature**   \n",
    "We added an interaction feature to introduce a new feature (i.e., city_month) so that \"Sydney_July\" becomes its own one-hot. Thus, a linear model on those will fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d7905f-2ca5-44cc-93a8-0c62affe7b2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'city': 'Sydney', 'month': 'July', 'city_month': 'Sydney_July'}, {'city': 'Sydney', 'month': 'December', 'city_month': 'Sydney_December'}, {'city': 'Paris', 'month': 'July', 'city_month': 'Paris_July'}, {'city': 'Paris', 'month': 'December', 'city_month': 'Paris_December'}]\n"
     ]
    }
   ],
   "source": [
    "X2_inter = []\n",
    "for x in X2:\n",
    "    xi = x.copy()\n",
    "    xi['city_month'] = f\"{x['city']}_{x['month']}\"\n",
    "    X2_inter.append(xi)\n",
    "\n",
    "print(X2_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d82977f-c741-4930-98e7-4c6da5e2ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with interaction: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    Perceptron(max_iter=10)\n",
    ")\n",
    "clf.fit(X2_inter, Y2)\n",
    "print(\"Accuracy with interaction:\", accuracy_score(Y2, clf.predict(X2_inter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715dfb4-2283-42da-ba3b-6a626109f918",
   "metadata": {},
   "source": [
    "**Use a non-linear mode**  \n",
    "Another aspect is to use an RBF kernel to draw a non-linear boundary in the high-dimensional feature space.\n",
    "\n",
    "RBF can project the data into a high-dimensional space; thus, it can be separated by a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48a811a-d4d8-4e5d-b447-177df05c8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_rbf = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    SVC(kernel='rbf', gamma='scale')   \n",
    ")\n",
    "clf_rbf.fit(X2, Y2)\n",
    "print(\"RBF SVM accuracy:\", accuracy_score(Y2, clf_rbf.predict(X2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b96d62-0b31-445f-bae3-06e01819a865",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516406f1-3c54-40dd-8343-e9fe333b76b0",
   "metadata": {},
   "source": [
    "### Experiment  \n",
    "We run the experiment, and the result is around the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68083152-0a3d-41fd-8de8-f9477c1d48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.62 sec.\n",
      "Accuracy: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        Perceptron()  \n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97125de7-f937-4654-9f2d-023297d5dc66",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1004ffa-4290-4dc8-8d3f-0f9f0a36cbfb",
   "metadata": {},
   "source": [
    "We first checked the consistency of the label and the outlier of the dataset. We didn't find any inconsistency or outlier issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1969bc6d-ceab-4dff-8530-bc837a9750ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'pos': 6000, 'neg': 5914})\n",
      "Outlier X: 0\n",
      "Outlier Y: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "print(Counter(Y))\n",
    "Xs = pd.Series(X)\n",
    "Ys = pd.Series(Y)\n",
    "print(\"Outlier X:\", Xs.isnull().sum())\n",
    "print(\"Outlier Y:\",Ys.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a39de7-8e51-497f-ad39-47008c76920e",
   "metadata": {},
   "source": [
    "**Key steps of the implemention of the algorithm**\n",
    "1. Label pre-process. Project the two types of labels into +1 / -1 for uniformly process.\n",
    "2. Weight Initialization. We start a whole-zero vector as the parameter.\n",
    "3. Training Loop.\n",
    "    - Repeat for a number of passes. Randomly shuffle the training indices at the start of each pass.\n",
    "    - For every sample, compute a decaying step size.\n",
    "    - Shrink the weight vector, if the current sample violates the margin, shift it sightly.\n",
    "4. Norm projection. Check the weight vector. Rescale it if it grows beyond a shredshold to ensure the model stable.\n",
    "The sanity check showed that our result is within the expectation.\n",
    "\n",
    "**Quich takeaways**\n",
    "- Sweet spot: λ = 1 × 10⁻⁴ gives the best accuracy across all runs.\n",
    "- Iterations: Raising interation from 10 to 100 gives small gains, but when it comes to 1000, only very slightly accuracy is added to the results while training time multiplies 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c175398-8376-4026-b889-292d4cf89b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10  λ=1e-05    | time   0.8s | acc 0.8196\n",
      "n_iter=10  λ=0.0001   | time   0.8s | acc 0.8376\n",
      "n_iter=10  λ=0.001    | time   0.9s | acc 0.8242\n",
      "n_iter=20  λ=1e-05    | time   1.3s | acc 0.8275\n",
      "n_iter=20  λ=0.0001   | time   1.3s | acc 0.8363\n",
      "n_iter=20  λ=0.001    | time   1.4s | acc 0.8263\n",
      "n_iter=100 λ=1e-05    | time   5.1s | acc 0.8372\n",
      "n_iter=100 λ=0.0001   | time   5.3s | acc 0.8376\n",
      "n_iter=100 λ=0.001    | time   5.7s | acc 0.8250\n",
      "n_iter=200 λ=1e-05    | time   9.7s | acc 0.8347\n",
      "n_iter=200 λ=0.0001   | time  10.2s | acc 0.8372\n",
      "n_iter=200 λ=0.001    | time  11.1s | acc 0.8254\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "     10 0.00010           0.84    0.8376\n",
      "    100 0.00010           5.28    0.8376\n",
      "    100 0.00001           5.07    0.8372\n",
      "    200 0.00010          10.15    0.8372\n",
      "     20 0.00010           1.32    0.8363\n",
      "    200 0.00001           9.73    0.8347\n",
      "     20 0.00001           1.27    0.8275\n",
      "     20 0.00100           1.41    0.8263\n",
      "    200 0.00100          11.06    0.8254\n",
      "    100 0.00100           5.70    0.8250\n",
      "     10 0.00100           0.88    0.8242\n",
      "     10 0.00001           0.81    0.8196\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 20, 100, 200],\n",
    "    \"lambda_param\":  [1e-5, 1e-4, 1e-3],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088073b-a4eb-4bee-9eb3-c96ce6297823",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdedcd9",
   "metadata": {},
   "source": [
    "In thie section, we compared the algprithms, using the log loss instead of the hinge loss.\n",
    "\n",
    "**Key Takeaways**  \n",
    "- Accuracy: both peak around the same level. The accuracy gap is tiny.  \n",
    "- Training time: hinge loss updates only on “violators”, so it reaches its plateau faster. Log loss always updates, so costs ~30 % more wall-time for the same number of passes.  \n",
    "- λ-sensitivity: both deteriorate when λ is too big or too small, but log loss drops harder at λ = 0.001 whereas hinge loss stays ~0.826."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b512bec8-1805-4ef5-881e-fe8ab6492c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10   λ=1e-03  |  time  0.84s  |  acc 0.8065\n",
      "n_iter=10   λ=1e-04  |  time  0.84s  |  acc 0.8321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4/pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10   λ=1e-05  |  time  0.86s  |  acc 0.8229\n",
      "n_iter=20   λ=1e-03  |  time  1.34s  |  acc 0.8082\n",
      "n_iter=20   λ=1e-04  |  time  1.33s  |  acc 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4/pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=20   λ=1e-05  |  time  1.35s  |  acc 0.8288\n",
      "n_iter=100  λ=1e-03  |  time  5.13s  |  acc 0.8074\n",
      "n_iter=100  λ=1e-04  |  time  5.33s  |  acc 0.8376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4/pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=100  λ=1e-05  |  time  5.19s  |  acc 0.8313\n",
      "n_iter=200  λ=1e-03  |  time 10.09s  |  acc 0.8074\n",
      "n_iter=200  λ=1e-04  |  time  9.97s  |  acc 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhua/dev/Applied-Machine-Learning-Lab/PA4/pegasos.py:146: RuntimeWarning: overflow encountered in exp\n",
      "  loss = self.lambda_param * self.w - (y * x) / (1 + np.exp(part))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=200  λ=1e-05  |  time 10.09s  |  acc 0.8326\n",
      "\n",
      "=== Result Conclusion ===\n",
      " n_iter  lambda  train_time(s)  test_acc\n",
      "    200 0.00010           9.97    0.8384\n",
      "    100 0.00010           5.33    0.8376\n",
      "     20 0.00010           1.33    0.8330\n",
      "    200 0.00001          10.09    0.8326\n",
      "     10 0.00010           0.84    0.8321\n",
      "    100 0.00001           5.19    0.8313\n",
      "     20 0.00001           1.35    0.8288\n",
      "     10 0.00001           0.86    0.8229\n",
      "     20 0.00100           1.34    0.8082\n",
      "    100 0.00100           5.13    0.8074\n",
      "    200 0.00100          10.09    0.8074\n",
      "     10 0.00100           0.84    0.8065\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from pegasos import LogisticRegression\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # ---------- parameter grid ----------\n",
    "    grid_n_iter   = [10, 20, 100, 200]          # epochs\n",
    "    grid_lambda   = [1e-3, 1e-4, 1e-5]     # regulariser\n",
    "    results = []\n",
    "\n",
    "    # ---------- loop ----------\n",
    "    for n_iter, lam in product(grid_n_iter, grid_lambda):\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            LogisticRegression(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest))\n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<4} λ={lam:<.0e}  |  time {train_time:5.2f}s  |  acc {acc:.4f}\")\n",
    "\n",
    "    # ---------- summary ----------\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62e089",
   "metadata": {},
   "source": [
    "## Bonus task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed70d9",
   "metadata": {},
   "source": [
    "### Faster linear algebra operations\n",
    "1. We used  ddot(x, y) x.dot(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d8ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter=10  λ=0.0001   | time   0.6s | acc 0.8317\n",
      "n_iter=100 λ=0.0001   | time   2.6s | acc 0.8389\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos_opt\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 100, 200],\n",
    "    \"lambda_param\":  [ 1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos_opt(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5191e-fb98-44d8-8b64-78b035cdc2f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Speeding up the scaling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa676fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pegasos import Pegasos_vec_scale\n",
    "from itertools import product\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "    \n",
    "    param_space = {\n",
    "    \"n_iter\":        [10, 100, 200],\n",
    "    \"lambda_param\":  [ 1e-4],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    # Iterate over all combinations of parameters.\n",
    "    # We use the product function from itertools to create a cartesian\n",
    "    for n_iter, lam in product(param_space[\"n_iter\"], param_space[\"lambda_param\"]):\n",
    "        # Set up the preprocessing steps and the classifier.\n",
    "        clf = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            # NB that this is our Pegasos. See the implementation in the according .py file\n",
    "            Pegasos_vec_scale(n_iter=n_iter, lambda_param=lam)\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        train_time = time.time() - t0\n",
    "        acc = accuracy_score(Ytest, clf.predict(Xtest)) \n",
    "\n",
    "        results.append(\n",
    "            {\"n_iter\": n_iter,\n",
    "            \"lambda\": lam,\n",
    "            \"train_time(s)\": round(train_time, 2),\n",
    "            \"test_acc\": round(acc, 4)}\n",
    "        )\n",
    "        print(f\"n_iter={n_iter:<3} λ={lam:<8} | time {train_time:5.1f}s | acc {acc:.4f}\")\n",
    "    # Sort the results by accuracy.\n",
    "    df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n",
    "    print(\"\\n=== Result Conclusion ===\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3ceff-0659-4c0d-842c-a906aad2bfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
